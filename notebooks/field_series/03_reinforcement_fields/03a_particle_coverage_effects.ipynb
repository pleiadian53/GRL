{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3a: Particle Coverage Effects on Policy Fields\n",
    "\n",
    "**Supplementary to Notebook 3 — Reinforcement Fields in GRL**\n",
    "\n",
    "---\n",
    "\n",
    "## Motivation\n",
    "\n",
    "In Notebook 3, we observed that the inferred policy arrows appear mostly **parallel** rather than **converging** on the goal. This notebook provides visual proof that this behavior is due to **sparse particle coverage** and demonstrates how **richer particle distributions** produce more intuitive policy fields.\n",
    "\n",
    "### What We'll Compare\n",
    "\n",
    "| Scenario | Particle Distribution | Expected Arrow Behavior |\n",
    "|----------|----------------------|------------------------|\n",
    "| **Sparse (diagonal)** | Particles only along diagonal path | Parallel arrows |\n",
    "| **Rich (radial)** | Particles from many directions | Converging arrows |\n",
    "| **True gradient** | No particles — direct goal potential | Perfect convergence |\n",
    "\n",
    "### Time\n",
    "\n",
    "~15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme(style='whitegrid', context='notebook')\n",
    "plt.rcParams['figure.figsize'] = (14, 10)\n",
    "%matplotlib inline\n",
    "\n",
    "# Goal location\n",
    "GOAL = np.array([4.0, 4.0])\n",
    "print(\"Setup complete. Goal at (4, 4)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Core Functions\n",
    "\n",
    "We'll reuse the $Q^+$ computation from Notebook 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rbf_kernel_4d(z, z_prime, lengthscale=0.8):\n",
    "    \"\"\"RBF kernel in 4D augmented space.\"\"\"\n",
    "    return np.exp(-np.sum((z - z_prime)**2) / (2 * lengthscale**2))\n",
    "\n",
    "def compute_Q_plus(x, y, vx, vy, particles, lengthscale=0.8):\n",
    "    \"\"\"Compute Q+(x, y, vx, vy) from particles.\"\"\"\n",
    "    z = np.array([x, y, vx, vy])\n",
    "    Q = 0.0\n",
    "    for p in particles:\n",
    "        z_i = np.array([p['x'], p['y'], p['vx'], p['vy']])\n",
    "        Q += p['w'] * rbf_kernel_4d(z, z_i, lengthscale)\n",
    "    return Q\n",
    "\n",
    "def infer_policy(X, Y, particles, n_angles=24, lengthscale=0.8):\n",
    "    \"\"\"Infer greedy policy at each state.\"\"\"\n",
    "    U = np.zeros_like(X)\n",
    "    V = np.zeros_like(Y)\n",
    "    \n",
    "    angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)\n",
    "    \n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            x, y = X[i, j], Y[i, j]\n",
    "            best_Q = -np.inf\n",
    "            best_vx, best_vy = 0, 0\n",
    "            \n",
    "            for theta in angles:\n",
    "                vx, vy = np.cos(theta), np.sin(theta)\n",
    "                Q = compute_Q_plus(x, y, vx, vy, particles, lengthscale)\n",
    "                if Q > best_Q:\n",
    "                    best_Q = Q\n",
    "                    best_vx, best_vy = vx, vy\n",
    "            \n",
    "            U[i, j] = best_vx\n",
    "            V[i, j] = best_vy\n",
    "    \n",
    "    return U, V\n",
    "\n",
    "def true_gradient_policy(X, Y, goal):\n",
    "    \"\"\"Compute true gradient field pointing toward goal.\"\"\"\n",
    "    U = goal[0] - X\n",
    "    V = goal[1] - Y\n",
    "    # Normalize\n",
    "    norm = np.sqrt(U**2 + V**2) + 1e-6\n",
    "    return U / norm, V / norm\n",
    "\n",
    "print(\"Core functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Three Particle Distributions\n",
    "\n",
    "We'll create three different particle configurations to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_diagonal_particles(goal, n_points=8):\n",
    "    \"\"\"\n",
    "    Sparse particles along diagonal path.\n",
    "    All particles have similar action direction (toward upper-right).\n",
    "    \"\"\"\n",
    "    particles = []\n",
    "    for i in range(n_points):\n",
    "        t = i / (n_points - 1)\n",
    "        px, py = t * goal[0], t * goal[1]\n",
    "        # Action: always toward goal (upper-right)\n",
    "        dx, dy = goal[0] - px, goal[1] - py\n",
    "        norm = np.sqrt(dx**2 + dy**2) + 1e-6\n",
    "        particles.append({\n",
    "            'x': px, 'y': py,\n",
    "            'vx': dx/norm, 'vy': dy/norm,\n",
    "            'w': 1.5\n",
    "        })\n",
    "    return particles\n",
    "\n",
    "def create_rich_radial_particles(goal, n_rings=3, points_per_ring=8):\n",
    "    \"\"\"\n",
    "    Rich particles from multiple directions.\n",
    "    Particles at various positions, all pointing toward goal.\n",
    "    \"\"\"\n",
    "    particles = []\n",
    "    radii = [1.0, 2.0, 3.0][:n_rings]\n",
    "    \n",
    "    for r in radii:\n",
    "        for i in range(points_per_ring):\n",
    "            angle = 2 * np.pi * i / points_per_ring\n",
    "            # Position: around the goal at radius r\n",
    "            px = goal[0] + r * np.cos(angle)\n",
    "            py = goal[1] + r * np.sin(angle)\n",
    "            # Skip if outside domain\n",
    "            if px < -0.5 or px > 4.5 or py < -0.5 or py > 4.5:\n",
    "                continue\n",
    "            # Action: toward goal\n",
    "            dx, dy = goal[0] - px, goal[1] - py\n",
    "            norm = np.sqrt(dx**2 + dy**2) + 1e-6\n",
    "            particles.append({\n",
    "                'x': px, 'y': py,\n",
    "                'vx': dx/norm, 'vy': dy/norm,\n",
    "                'w': 1.5\n",
    "            })\n",
    "    \n",
    "    # Also add particles along paths from corners\n",
    "    corners = [(0, 0), (0, 4), (4, 0)]\n",
    "    for cx, cy in corners:\n",
    "        for t in [0.25, 0.5, 0.75]:\n",
    "            px = cx + t * (goal[0] - cx)\n",
    "            py = cy + t * (goal[1] - cy)\n",
    "            dx, dy = goal[0] - px, goal[1] - py\n",
    "            norm = np.sqrt(dx**2 + dy**2) + 1e-6\n",
    "            particles.append({\n",
    "                'x': px, 'y': py,\n",
    "                'vx': dx/norm, 'vy': dy/norm,\n",
    "                'w': 1.5\n",
    "            })\n",
    "    \n",
    "    return particles\n",
    "\n",
    "# Create particle sets\n",
    "particles_sparse = create_sparse_diagonal_particles(GOAL)\n",
    "particles_rich = create_rich_radial_particles(GOAL)\n",
    "\n",
    "print(f\"Sparse diagonal: {len(particles_sparse)} particles\")\n",
    "print(f\"Rich radial: {len(particles_rich)} particles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Visual Comparison\n",
    "\n",
    "Now let's see the difference in policy fields!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grids\n",
    "x_policy = np.linspace(0, 4, 12)\n",
    "y_policy = np.linspace(0, 4, 12)\n",
    "X_p, Y_p = np.meshgrid(x_policy, y_policy)\n",
    "\n",
    "# Compute policies\n",
    "print(\"Computing sparse diagonal policy...\")\n",
    "U_sparse, V_sparse = infer_policy(X_p, Y_p, particles_sparse)\n",
    "\n",
    "print(\"Computing rich radial policy...\")\n",
    "U_rich, V_rich = infer_policy(X_p, Y_p, particles_rich)\n",
    "\n",
    "print(\"Computing true gradient policy...\")\n",
    "U_true, V_true = true_gradient_policy(X_p, Y_p, GOAL)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "configs = [\n",
    "    (axes[0], U_sparse, V_sparse, particles_sparse, \n",
    "     'Sparse Diagonal Particles', 'Arrows mostly parallel'),\n",
    "    (axes[1], U_rich, V_rich, particles_rich,\n",
    "     'Rich Radial Particles', 'Arrows converge on goal'),\n",
    "    (axes[2], U_true, V_true, None,\n",
    "     'True Gradient Field', 'Perfect convergence (no particles)')\n",
    "]\n",
    "\n",
    "for ax, U, V, particles, title, subtitle in configs:\n",
    "    # Policy arrows\n",
    "    ax.quiver(X_p, Y_p, U, V, color='black', scale=18, width=0.006, zorder=5)\n",
    "    \n",
    "    # Particles (if any)\n",
    "    if particles:\n",
    "        for p in particles:\n",
    "            ax.plot(p['x'], p['y'], 'bo', markersize=8, alpha=0.6, mec='darkblue', mew=1)\n",
    "            ax.arrow(p['x'], p['y'], p['vx']*0.2, p['vy']*0.2,\n",
    "                     head_width=0.08, head_length=0.04, fc='blue', ec='blue', alpha=0.5)\n",
    "    \n",
    "    # Goal\n",
    "    ax.plot(*GOAL, 'g^', markersize=20, label='Goal', zorder=10)\n",
    "    ax.plot(0, 0, 'ko', markersize=12, label='Start', zorder=10)\n",
    "    \n",
    "    ax.set_xlim(-0.5, 4.5)\n",
    "    ax.set_ylim(-0.5, 4.5)\n",
    "    ax.set_xlabel('$x$')\n",
    "    ax.set_ylabel('$y$')\n",
    "    ax.set_title(f'{title}\\n({subtitle})', fontsize=12)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.legend(loc='upper left')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Effect of Particle Coverage on Inferred Policy', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"KEY OBSERVATION:\")\n",
    "print(\"=\"*70)\n",
    "print(\"Left:   Sparse particles → parallel arrows (limited directional info)\")\n",
    "print(\"Middle: Rich particles → converging arrows (diverse directional info)\")\n",
    "print(\"Right:  True gradient → perfect convergence (geometric, not learned)\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Quantitative Analysis\n",
    "\n",
    "Let's measure how well each policy field aligns with the true gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_alignment(U_policy, V_policy, U_true, V_true):\n",
    "    \"\"\"\n",
    "    Compute average cosine similarity between policy and true gradient.\n",
    "    1.0 = perfect alignment, 0.0 = orthogonal, -1.0 = opposite\n",
    "    \"\"\"\n",
    "    dot_product = U_policy * U_true + V_policy * V_true\n",
    "    # Both are already normalized, so dot product = cosine similarity\n",
    "    return np.mean(dot_product)\n",
    "\n",
    "# Normalize sparse and rich policies for fair comparison\n",
    "norm_sparse = np.sqrt(U_sparse**2 + V_sparse**2) + 1e-6\n",
    "U_sparse_norm = U_sparse / norm_sparse\n",
    "V_sparse_norm = V_sparse / norm_sparse\n",
    "\n",
    "norm_rich = np.sqrt(U_rich**2 + V_rich**2) + 1e-6\n",
    "U_rich_norm = U_rich / norm_rich\n",
    "V_rich_norm = V_rich / norm_rich\n",
    "\n",
    "align_sparse = compute_alignment(U_sparse_norm, V_sparse_norm, U_true, V_true)\n",
    "align_rich = compute_alignment(U_rich_norm, V_rich_norm, U_true, V_true)\n",
    "align_true = compute_alignment(U_true, V_true, U_true, V_true)\n",
    "\n",
    "print(\"Alignment with True Gradient (cosine similarity):\")\n",
    "print(f\"  Sparse diagonal: {align_sparse:.3f}\")\n",
    "print(f\"  Rich radial:     {align_rich:.3f}\")\n",
    "print(f\"  True gradient:   {align_true:.3f} (baseline)\")\n",
    "print()\n",
    "print(f\"Improvement from sparse to rich: {(align_rich - align_sparse):.3f}\")\n",
    "print(f\"  ({(align_rich - align_sparse) / (1 - align_sparse) * 100:.1f}% of remaining gap closed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize alignment heatmap\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Compute local alignment\n",
    "align_sparse_local = U_sparse_norm * U_true + V_sparse_norm * V_true\n",
    "align_rich_local = U_rich_norm * U_true + V_rich_norm * V_true\n",
    "\n",
    "# Plot\n",
    "for ax, align_local, title in [\n",
    "    (axes[0], align_sparse_local, 'Sparse Diagonal'),\n",
    "    (axes[1], align_rich_local, 'Rich Radial')\n",
    "]:\n",
    "    c = ax.contourf(X_p, Y_p, align_local, levels=np.linspace(-1, 1, 21), cmap='RdYlGn')\n",
    "    ax.plot(*GOAL, 'k^', markersize=15, zorder=10)\n",
    "    ax.set_xlabel('$x$'); ax.set_ylabel('$y$')\n",
    "    ax.set_title(f'{title}\\nLocal Alignment with True Gradient')\n",
    "    ax.set_aspect('equal')\n",
    "    plt.colorbar(c, ax=ax, label='Cosine Similarity')\n",
    "\n",
    "plt.suptitle('Where Does Each Policy Align with the True Gradient?', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green = good alignment, Red = poor alignment\")\n",
    "print(\"Rich particles provide better coverage, especially away from the diagonal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Visual Proof\n",
    "\n",
    "We demonstrated that:\n",
    "\n",
    "1. **Sparse diagonal particles** → Policy arrows are mostly parallel because all particles encode similar action directions.\n",
    "\n",
    "2. **Rich radial particles** → Policy arrows converge on the goal because particles from different directions provide diverse directional information.\n",
    "\n",
    "3. **True gradient field** → Perfect convergence (but requires explicit goal knowledge, cannot encode obstacles).\n",
    "\n",
    "### Implications for GRL\n",
    "\n",
    "| Aspect | Sparse Coverage | Rich Coverage |\n",
    "|--------|-----------------|---------------|\n",
    "| **Exploration** | Limited trajectories | Diverse trajectories |\n",
    "| **Policy quality** | Approximate | Near-optimal |\n",
    "| **Generalization** | Poor in unseen regions | Good everywhere |\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "> **Policy quality in GRL is directly proportional to the richness of the particle memory.**\n",
    "\n",
    "This is why exploration and diverse experience collection are crucial in GRL — the more varied the particles, the more the learned field resembles the true optimal policy.\n",
    "\n",
    "---\n",
    "\n",
    "**See also:**\n",
    "- `03_reinforcement_fields.ipynb` — Main notebook\n",
    "- `notes/particle_vs_gradient_fields.md` — Detailed theory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
