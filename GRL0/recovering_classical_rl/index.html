
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Physics-grounded reinforcement learning with particle-based belief representations">
      
      
        <meta name="author" content="GRL Research Team">
      
      
        <link rel="canonical" href="https://pleiadian53.github.io/GRL/GRL0/recovering_classical_rl/">
      
      
        <link rel="prev" href="../implementation/">
      
      
        <link rel="next" href="../../ROADMAP/">
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Recovering Classical RL - Generalized Reinforcement Learning (GRL)</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#recovering-classical-rl-from-grl" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Generalized Reinforcement Learning (GRL)" class="md-header__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Generalized Reinforcement Learning (GRL)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Recovering Classical RL
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  
  GRL v0 (Tutorial Paper)

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../ROADMAP/" class="md-tabs__link">
        
  
  
    
  
  Research Roadmap

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../notebooks/" class="md-tabs__link">
          
  
  
  Notebooks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../CONTRIBUTING/" class="md-tabs__link">
          
  
  
  About

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Generalized Reinforcement Learning (GRL)" class="md-nav__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Generalized Reinforcement Learning (GRL)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" checked>
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    GRL v0 (Tutorial Paper)
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    GRL v0 (Tutorial Paper)
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Part I: Tutorials
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part I: Tutorials
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tutorial Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/00-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 0: Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/01-core-concepts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 1: Core Concepts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/02-rkhs-foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 2: RKHS Foundations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/03-energy-and-fitness/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3: Energy and Fitness
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/03a-least-action-principle/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3a: Least Action Principle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/04-reinforcement-field/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4: Reinforcement Field
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/04a-riesz-representer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4a: Riesz Representer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/05-particle-memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 5: Particle Memory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/06-memory-update/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6: MemoryUpdate
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/06a-advanced-memory-dynamics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6a: Advanced Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/07-rf-sarsa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7: RF-SARSA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../tutorials/07a-continuous-policy-inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7a: Continuous Policy Inference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    
  
    Quantum-Inspired Extensions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Quantum-Inspired Extensions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/01-rkhs-quantum-parallel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: RKHS-QM Parallel
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/01a-wavefunction-interpretation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Wavefunction Interpretation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/02-rkhs-basis-and-amplitudes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Basis and Amplitudes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/03-complex-rkhs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Complex RKHS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/04-action-and-state-fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    04: Action and State Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/05-concept-projections-and-measurements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    05: Concept Projections
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/06-agent-state-and-belief-evolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    06: Agent State and Belief
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/07-learning-the-field-beyond-gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    07: Learning Beyond GP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    08: Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../quantum_inspired/09-path-integrals-and-action-principles/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    09: Path Integrals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../implementation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Implementation Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    Recovering Classical RL
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    Recovering Classical RL
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#executive-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Executive Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      
        Table of Contents
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-the-grlclassical-rl-dictionary" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. The GRL→Classical RL Dictionary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-recovery-1-q-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Recovery 1: Q-Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Recovery 1: Q-Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-q-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical Q-Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-recovery-2-dqn-deep-q-network" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Recovery 2: DQN (Deep Q-Network)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Recovery 2: DQN (Deep Q-Network)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-dqn" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical DQN
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-recovery-3-reinforce-policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Recovery 3: REINFORCE (Policy Gradient)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Recovery 3: REINFORCE (Policy Gradient)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-reinforce" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical REINFORCE
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-recovery-4-actor-critic-ppo-sac" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Recovery 4: Actor-Critic (PPO, SAC)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Recovery 4: Actor-Critic (PPO, SAC)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-actor-critic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical Actor-Critic
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-recovery-5-rlhf-for-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Recovery 5: RLHF for LLMs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Recovery 5: RLHF for LLMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-rlhf-reinforcement-learning-from-human-feedback" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical RLHF (Reinforcement Learning from Human Feedback)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version_4" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages-of-grl-for-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      
        Advantages of GRL for RLHF
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-path" class="md-nav__link">
    <span class="md-ellipsis">
      
        Implementation Path
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways_4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-what-grl-adds-beyond-classical-rl" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. What GRL Adds Beyond Classical RL
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. What GRL Adds Beyond Classical RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-continuous-action-generalization" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Continuous Action Generalization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-compositional-actions" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Compositional Actions
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-uncertainty-quantification" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Uncertainty Quantification
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-energy-based-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Energy-Based Regularization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-particle-based-interpretability" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Particle-Based Interpretability
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-hierarchical-abstraction-part-ii" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Hierarchical Abstraction (Part II)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-implementation-from-grl-to-classical" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Implementation: From GRL to Classical
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Implementation: From GRL to Classical">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#code-example-q-learning-from-grl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example: Q-Learning from GRL
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example-dqn-from-grl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example: DQN from GRL
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Conclusion
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#next-steps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Next Steps
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Research Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Notebooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Notebooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Field Series
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Field Series
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Series Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/00_intro_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    00: Introduction to Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/01_classical_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: Classical Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/01a_vector_fields_and_odes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Vector Fields and ODEs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/02_functional_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Functional Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2_7" >
        
          
          <label class="md-nav__link" for="__nav_4_2_7" id="__nav_4_2_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reinforcement Fields
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reinforcement Fields
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Reinforcement Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03a: Particle Coverage Effects
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Theory: Particle vs Gradient Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    About
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    About
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CONTRIBUTING/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Contributing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LICENSE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    License
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#executive-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Executive Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#table-of-contents" class="md-nav__link">
    <span class="md-ellipsis">
      
        Table of Contents
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-the-grlclassical-rl-dictionary" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. The GRL→Classical RL Dictionary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-recovery-1-q-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Recovery 1: Q-Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Recovery 1: Q-Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-q-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical Q-Learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-recovery-2-dqn-deep-q-network" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Recovery 2: DQN (Deep Q-Network)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Recovery 2: DQN (Deep Q-Network)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-dqn" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical DQN
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-recovery-3-reinforce-policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Recovery 3: REINFORCE (Policy Gradient)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Recovery 3: REINFORCE (Policy Gradient)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-reinforce" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical REINFORCE
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways_2" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-recovery-4-actor-critic-ppo-sac" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Recovery 4: Actor-Critic (PPO, SAC)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Recovery 4: Actor-Critic (PPO, SAC)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-actor-critic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical Actor-Critic
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways_3" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-recovery-5-rlhf-for-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Recovery 5: RLHF for LLMs
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Recovery 5: RLHF for LLMs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#classical-rlhf-reinforcement-learning-from-human-feedback" class="md-nav__link">
    <span class="md-ellipsis">
      
        Classical RLHF (Reinforcement Learning from Human Feedback)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#grl-version_4" class="md-nav__link">
    <span class="md-ellipsis">
      
        GRL Version
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#advantages-of-grl-for-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      
        Advantages of GRL for RLHF
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#implementation-path" class="md-nav__link">
    <span class="md-ellipsis">
      
        Implementation Path
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#key-takeaways_4" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-what-grl-adds-beyond-classical-rl" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. What GRL Adds Beyond Classical RL
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. What GRL Adds Beyond Classical RL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-continuous-action-generalization" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Continuous Action Generalization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-compositional-actions" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Compositional Actions
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#3-uncertainty-quantification" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Uncertainty Quantification
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4-energy-based-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Energy-Based Regularization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#5-particle-based-interpretability" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Particle-Based Interpretability
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#6-hierarchical-abstraction-part-ii" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Hierarchical Abstraction (Part II)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-implementation-from-grl-to-classical" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Implementation: From GRL to Classical
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Implementation: From GRL to Classical">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#code-example-q-learning-from-grl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example: Q-Learning from GRL
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#code-example-dqn-from-grl" class="md-nav__link">
    <span class="md-ellipsis">
      
        Code Example: DQN from GRL
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      
        Conclusion
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#next-steps" class="md-nav__link">
    <span class="md-ellipsis">
      
        Next Steps
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/pleiadian53/GRL/edit/main/docs/GRL0/recovering_classical_rl.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg>
    </a>
  
  


<h1 id="recovering-classical-rl-from-grl">Recovering Classical RL from GRL<a class="headerlink" href="#recovering-classical-rl-from-grl" title="Permanent link">&para;</a></h1>
<p><strong>Purpose</strong>: Demonstrate that traditional RL algorithms are special cases of GRL<br />
<strong>Audience</strong>: Classical RL researchers, practitioners, skeptics<br />
<strong>Goal</strong>: Bridge the gap between familiar methods and the GRL framework</p>
<hr />
<h2 id="executive-summary">Executive Summary<a class="headerlink" href="#executive-summary" title="Permanent link">&para;</a></h2>
<p><strong>Key Claim</strong>: Generalized Reinforcement Learning (GRL) is not a replacement for classical RL—it's a <strong>unifying framework</strong> that recovers existing methods as special cases while enabling new capabilities.</p>
<p><strong>Why This Matters</strong>:</p>
<ul>
<li><strong>Adoption</strong>: Researchers trust frameworks that subsume what they already know</li>
<li><strong>Validation</strong>: If GRL recovers DQN/PPO/SAC, it must be correct</li>
<li><strong>Innovation</strong>: Once the connection is clear, extensions become natural</li>
</ul>
<p><strong>What You'll Learn</strong>:</p>
<ol>
<li>How <strong>Q-learning</strong> emerges from GRL with discrete actions</li>
<li>How <strong>DQN</strong> is GRL with neural network approximation</li>
<li>How <strong>Policy Gradients</strong> (REINFORCE) follow from the energy landscape</li>
<li>How <strong>Actor-Critic</strong> (PPO, SAC) naturally arise in GRL</li>
<li>How <strong>RLHF for LLMs</strong> is GRL applied to language modeling</li>
</ol>
<hr />
<h2 id="table-of-contents">Table of Contents<a class="headerlink" href="#table-of-contents" title="Permanent link">&para;</a></h2>
<ol>
<li><a href="#dictionary">The GRL→Classical RL Dictionary</a></li>
<li><a href="#q-learning">Recovery 1: Q-Learning</a></li>
<li><a href="#dqn">Recovery 2: DQN (Deep Q-Network)</a></li>
<li><a href="#reinforce">Recovery 3: REINFORCE (Policy Gradient)</a></li>
<li><a href="#actor-critic">Recovery 4: Actor-Critic (PPO, SAC)</a></li>
<li><a href="#rlhf">Recovery 5: RLHF for LLMs</a></li>
<li><a href="#beyond">What GRL Adds Beyond Classical RL</a></li>
<li><a href="#implementation">Implementation: From GRL to Classical</a></li>
</ol>
<hr />
<p><a name="dictionary"></a></p>
<h2 id="1-the-grlclassical-rl-dictionary">1. The GRL→Classical RL Dictionary<a class="headerlink" href="#1-the-grlclassical-rl-dictionary" title="Permanent link">&para;</a></h2>
<table>
<thead>
<tr>
<th>Classical RL Concept</th>
<th>GRL Equivalent</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>State</strong> <span class="arithmatex">\(s\)</span></td>
<td>State <span class="arithmatex">\(s\)</span></td>
<td>Same</td>
</tr>
<tr>
<td><strong>Discrete Action</strong> <span class="arithmatex">\(a \in \mathcal{A}\)</span></td>
<td>Fixed parametric mapping <span class="arithmatex">\(\theta_a\)</span></td>
<td>One <span class="arithmatex">\(\theta\)</span> per discrete action</td>
</tr>
<tr>
<td><strong>Continuous Action</strong> <span class="arithmatex">\(a \in \mathbb{R}^d\)</span></td>
<td>Action parameters <span class="arithmatex">\(\theta \in \mathbb{R}^d\)</span></td>
<td>Direct correspondence</td>
</tr>
<tr>
<td><strong>Q-function</strong> <span class="arithmatex">\(Q(s, a)\)</span></td>
<td>Reinforcement field <span class="arithmatex">\(Q^+(s, \theta)\)</span></td>
<td>Evaluated at discrete <span class="arithmatex">\(\theta_a\)</span></td>
</tr>
<tr>
<td><strong>Replay Buffer</strong> <span class="arithmatex">\(\mathcal{D}\)</span></td>
<td>Particle Memory <span class="arithmatex">\(\Omega\)</span></td>
<td>Particles are weighted experiences</td>
</tr>
<tr>
<td><strong>Experience</strong> <span class="arithmatex">\((s, a, r, s')\)</span></td>
<td>Particle <span class="arithmatex">\((z, w)\)</span> where <span class="arithmatex">\(z=(s,\theta)\)</span>, <span class="arithmatex">\(w=r\)</span></td>
<td>Single transition</td>
</tr>
<tr>
<td><strong>TD Target</strong> <span class="arithmatex">\(y = r + \gamma \max_{a'} Q(s', a')\)</span></td>
<td>MemoryUpdate with TD target</td>
<td>Belief transition</td>
</tr>
<tr>
<td><strong>Policy</strong> <span class="arithmatex">\(\pi(a\|s)\)</span></td>
<td>Boltzmann over <span class="arithmatex">\(Q^+(s, \cdot)\)</span></td>
<td>Temperature-controlled sampling</td>
</tr>
<tr>
<td><strong>Value Function</strong> <span class="arithmatex">\(V(s)\)</span></td>
<td><span class="arithmatex">\(\max_\theta Q^+(s, \theta)\)</span></td>
<td>Maximum over action parameters</td>
</tr>
<tr>
<td><strong>Exploration</strong> <span class="arithmatex">\(\epsilon\)</span>-greedy</td>
<td>Temperature <span class="arithmatex">\(\beta\)</span> in Boltzmann</td>
<td>Smooth instead of discrete</td>
</tr>
</tbody>
</table>
<p><strong>Key Insight</strong>: Classical RL is GRL with:</p>
<ul>
<li>
<p><strong>Discrete or fixed action spaces</strong></p>
</li>
<li>
<p><strong>Tabular or neural network approximation</strong> of the field</p>
</li>
<li><strong>Specific choices</strong> of update rules</li>
</ul>
<hr />
<p><a name="q-learning"></a></p>
<h2 id="2-recovery-1-q-learning">2. Recovery 1: Q-Learning<a class="headerlink" href="#2-recovery-1-q-learning" title="Permanent link">&para;</a></h2>
<h3 id="classical-q-learning">Classical Q-Learning<a class="headerlink" href="#classical-q-learning" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong>:</p>
<ul>
<li>State space: <span class="arithmatex">\(\mathcal{S}\)</span></li>
<li>Action space: <span class="arithmatex">\(\mathcal{A} = \{a_1, \ldots, a_K\}\)</span> (discrete, finite)</li>
<li>Q-function: <span class="arithmatex">\(Q(s, a)\)</span> for each <span class="arithmatex">\((s, a)\)</span> pair</li>
</ul>
<p><strong>Update Rule</strong>:
$<span class="arithmatex">\(Q(s, a) \leftarrow Q(s, a) + \alpha [r + \gamma \max_{a'} Q(s', a') - Q(s, a)]\)</span>$</p>
<p><strong>Policy</strong>: <span class="arithmatex">\(\epsilon\)</span>-greedy or softmax over <span class="arithmatex">\(Q(s, \cdot)\)</span></p>
<hr />
<h3 id="grl-version">GRL Version<a class="headerlink" href="#grl-version" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong>:</p>
<ul>
<li>State space: <span class="arithmatex">\(\mathcal{S}\)</span> (same)</li>
<li>Action space: <span class="arithmatex">\(\mathcal{A} = \{a_1, \ldots, a_K\}\)</span> (discrete)</li>
<li><strong>Map each discrete action to a parameter</strong>: <span class="arithmatex">\(\theta_1, \ldots, \theta_K\)</span> (fixed)</li>
<li><strong>Augmented space</strong>: <span class="arithmatex">\(\mathcal{Z} = \mathcal{S} \times \{\theta_1, \ldots, \theta_K\}\)</span></li>
<li><strong>Reinforcement field</strong>: <span class="arithmatex">\(Q^+(s, \theta_i)\)</span> evaluated only at discrete points <span class="arithmatex">\(\theta_i\)</span></li>
</ul>
<p><strong>Particle Memory</strong>:</p>
<ul>
<li>Each experience <span class="arithmatex">\((s, a_i, r, s')\)</span> creates particle <span class="arithmatex">\((z_i, w_i)\)</span> where <span class="arithmatex">\(z_i = (s, \theta_i)\)</span>, <span class="arithmatex">\(w_i = r\)</span></li>
</ul>
<p><strong>MemoryUpdate</strong>:</p>
<ul>
<li>Add particle <span class="arithmatex">\((z_t, w_t)\)</span> where <span class="arithmatex">\(z_t = (s_t, \theta_{a_t})\)</span>, <span class="arithmatex">\(w_t = r_t\)</span></li>
<li>With <strong>no kernel association</strong> (set <span class="arithmatex">\(k(z, z') = \delta(z, z')\)</span>), MemoryUpdate reduces to:
  $<span class="arithmatex">\(Q^+(s, \theta_a) \leftarrow Q^+(s, \theta_a) + \alpha [y_t - Q^+(s, \theta_a)]\)</span>$
  where <span class="arithmatex">\(y_t = r_t + \gamma \max_{a'} Q^+(s', \theta_{a'})\)</span></li>
</ul>
<p><strong>This is exactly Q-learning!</strong></p>
<hr />
<h3 id="key-takeaways">Key Takeaways<a class="headerlink" href="#key-takeaways" title="Permanent link">&para;</a></h3>
<p><strong>Q-learning is GRL with</strong>:</p>
<ol>
<li><strong>Discrete action space</strong> (finite <span class="arithmatex">\(\{\theta_i\}\)</span>)</li>
<li><strong>Delta kernel</strong> (no generalization between actions)</li>
<li><strong>Tabular representation</strong> (store <span class="arithmatex">\(Q\)</span> for each state-action pair)</li>
</ol>
<p><strong>What GRL adds</strong>:</p>
<ul>
<li>Generalization via non-trivial kernels: <span class="arithmatex">\(k((s, \theta_i), (s, \theta_j)) &gt; 0\)</span> for <span class="arithmatex">\(i \neq j\)</span></li>
<li>Continuous interpolation: <span class="arithmatex">\(Q^+(s, \theta)\)</span> defined for all <span class="arithmatex">\(\theta\)</span>, not just discrete actions</li>
<li>Weighted particles: Experience importance via <span class="arithmatex">\(w_i\)</span></li>
</ul>
<hr />
<p><a name="dqn"></a></p>
<h2 id="3-recovery-2-dqn-deep-q-network">3. Recovery 2: DQN (Deep Q-Network)<a class="headerlink" href="#3-recovery-2-dqn-deep-q-network" title="Permanent link">&para;</a></h2>
<h3 id="classical-dqn">Classical DQN<a class="headerlink" href="#classical-dqn" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong>:</p>
<ul>
<li>Q-function approximated by neural network: <span class="arithmatex">\(Q_\psi(s, a)\)</span></li>
<li>Experience replay buffer: <span class="arithmatex">\(\mathcal{D} = \{(s_i, a_i, r_i, s_i')\}\)</span></li>
<li>Target network: <span class="arithmatex">\(Q_{\psi^-}\)</span> (delayed copy)</li>
</ul>
<p><strong>Update Rule</strong>:
$<span class="arithmatex">\(\psi \leftarrow \psi - \eta \nabla_\psi \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} [(Q_\psi(s, a) - y)^2]\)</span>$
where <span class="arithmatex">\(y = r + \gamma \max_{a'} Q_{\psi^-}(s', a')\)</span></p>
<hr />
<h3 id="grl-version_1">GRL Version<a class="headerlink" href="#grl-version_1" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong>:</p>
<ul>
<li><strong>Field approximator</strong>: Neural network <span class="arithmatex">\(Q_\psi(s, \theta)\)</span> approximates reinforcement field</li>
<li><strong>Particle memory</strong>: <span class="arithmatex">\(\Omega = \{(z_i, w_i)\}\)</span> where <span class="arithmatex">\(z_i = (s_i, \theta_i)\)</span></li>
<li><strong>Kernel</strong>: Implicit kernel induced by neural network architecture</li>
</ul>
<p><strong>Update Rule</strong>:
Sample particles from <span class="arithmatex">\(\Omega\)</span>, compute TD targets:
$<span class="arithmatex">\(\psi \leftarrow \psi - \eta \nabla_\psi \mathbb{E}_{(z,w) \sim \Omega} [(Q_\psi(z) - y)^2]\)</span>$
where <span class="arithmatex">\(y = w + \gamma \max_{\theta'} Q_\psi(s', \theta')\)</span></p>
<p><strong>Target network</strong>: Optional, same as DQN</p>
<hr />
<h3 id="key-takeaways_1">Key Takeaways<a class="headerlink" href="#key-takeaways_1" title="Permanent link">&para;</a></h3>
<p><strong>DQN is GRL with</strong>:</p>
<ol>
<li><strong>Neural network approximation</strong> of the reinforcement field</li>
<li><strong>Experience replay</strong> = particle memory sampling</li>
<li><strong>Discrete actions</strong> (typically)</li>
</ol>
<p><strong>What GRL adds</strong>:</p>
<ul>
<li><strong>Explicit particle representation</strong>: Particles are not just for replay, they define the field</li>
<li><strong>Kernel interpretation</strong>: Neural network as implicit kernel</li>
<li><strong>Continuous action generalization</strong>: <span class="arithmatex">\(Q_\psi(s, \theta)\)</span> for any <span class="arithmatex">\(\theta\)</span></li>
</ul>
<hr />
<p><a name="reinforce"></a></p>
<h2 id="4-recovery-3-reinforce-policy-gradient">4. Recovery 3: REINFORCE (Policy Gradient)<a class="headerlink" href="#4-recovery-3-reinforce-policy-gradient" title="Permanent link">&para;</a></h2>
<h3 id="classical-reinforce">Classical REINFORCE<a class="headerlink" href="#classical-reinforce" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong>:</p>
<ul>
<li>Policy: <span class="arithmatex">\(\pi_\phi(a|s)\)</span> (parameterized, e.g., neural network)</li>
<li>Objective: <span class="arithmatex">\(J(\phi) = \mathbb{E}_{\tau \sim \pi_\phi} [R(\tau)]\)</span> (expected return)</li>
</ul>
<p><strong>Update Rule</strong> (score function gradient):
$<span class="arithmatex">\(\nabla_\phi J(\phi) = \mathbb{E}_{\tau \sim \pi_\phi} [\sum_t \nabla_\phi \log \pi_\phi(a_t | s_t) \cdot G_t]\)</span>$</p>
<p>where <span class="arithmatex">\(G_t = \sum_{t'=t}^T \gamma^{t'-t} r_{t'}\)</span> (return from time <span class="arithmatex">\(t\)</span>)</p>
<hr />
<h3 id="grl-version_2">GRL Version<a class="headerlink" href="#grl-version_2" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong>:</p>
<ul>
<li><strong>Reinforcement field</strong>: <span class="arithmatex">\(Q^+(s, \theta)\)</span></li>
<li><strong>Policy</strong>: Boltzmann over field
  $<span class="arithmatex">\(\pi(a | s) = \frac{\exp(\beta \, Q^+(s, \theta_a))}{\int \exp(\beta \, Q^+(s, \theta')) d\theta'}\)</span>$</li>
</ul>
<p><strong>Gradient of Expected Return</strong>:</p>
<p>The policy gradient in GRL is:
$<span class="arithmatex">\(\nabla_\theta \mathbb{E}_{\pi} [R] = \mathbb{E}_{\pi} [\nabla_\theta Q^+(s, \theta) \cdot \text{advantage}]\)</span>$</p>
<p>If we parameterize <span class="arithmatex">\(Q^+(s, \theta) = Q_\phi(s, \theta)\)</span>, then:
$<span class="arithmatex">\(\nabla_\phi J(\phi) = \mathbb{E} [\nabla_\phi Q_\phi(s, \theta) \cdot G_t]\)</span>$</p>
<p><strong>Connection to REINFORCE</strong>:</p>
<p>The score function gradient <span class="arithmatex">\(\nabla_\phi \log \pi_\phi(a|s)\)</span> in REINFORCE is equivalent to the field gradient <span class="arithmatex">\(\nabla_\phi Q_\phi(s, \theta)\)</span> when policy is Boltzmann.</p>
<p><strong>This recovers REINFORCE!</strong></p>
<hr />
<h3 id="key-takeaways_2">Key Takeaways<a class="headerlink" href="#key-takeaways_2" title="Permanent link">&para;</a></h3>
<p><strong>REINFORCE is GRL with</strong>:</p>
<ol>
<li><strong>Boltzmann policy</strong> derived from energy landscape</li>
<li><strong>Direct parameterization</strong> of the field (or policy)</li>
<li><strong>Monte Carlo returns</strong> (<span class="arithmatex">\(G_t\)</span>) as targets</li>
</ol>
<p><strong>What GRL adds</strong>:</p>
<ul>
<li><strong>Energy interpretation</strong>: <span class="arithmatex">\(Q^+ = -E\)</span> provides physics-inspired regularization</li>
<li><strong>Particle-based updates</strong>: No need for full gradient, use particle approximation</li>
<li><strong>Smooth action selection</strong>: Temperature <span class="arithmatex">\(\beta\)</span> controls exploration naturally</li>
</ul>
<hr />
<p><a name="actor-critic"></a></p>
<h2 id="5-recovery-4-actor-critic-ppo-sac">5. Recovery 4: Actor-Critic (PPO, SAC)<a class="headerlink" href="#5-recovery-4-actor-critic-ppo-sac" title="Permanent link">&para;</a></h2>
<h3 id="classical-actor-critic">Classical Actor-Critic<a class="headerlink" href="#classical-actor-critic" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong>:</p>
<ul>
<li><strong>Actor</strong>: Policy <span class="arithmatex">\(\pi_\phi(a|s)\)</span></li>
<li><strong>Critic</strong>: Value function <span class="arithmatex">\(V_\psi(s)\)</span> or <span class="arithmatex">\(Q_\psi(s, a)\)</span></li>
</ul>
<p><strong>Update</strong>:</p>
<ul>
<li>
<p><strong>Critic</strong>: TD learning
  $<span class="arithmatex">\(\psi \leftarrow \psi - \eta \nabla_\psi [Q_\psi(s, a) - (r + \gamma V_\psi(s'))]^2\)</span>$</p>
</li>
<li>
<p><strong>Actor</strong>: Policy gradient with advantage
  $<span class="arithmatex">\(\phi \leftarrow \phi + \eta \nabla_\phi \log \pi_\phi(a|s) \cdot A(s, a)\)</span>$
  where <span class="arithmatex">\(A(s, a) = Q(s, a) - V(s)\)</span> (advantage)</p>
</li>
</ul>
<p><strong>Variants</strong>:</p>
<ul>
<li><strong>PPO</strong>: Clipped objective, KL penalty</li>
<li><strong>SAC</strong>: Entropy regularization, temperature tuning</li>
</ul>
<hr />
<h3 id="grl-version_3">GRL Version<a class="headerlink" href="#grl-version_3" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong>:</p>
<ul>
<li><strong>Critic</strong>: Reinforcement field <span class="arithmatex">\(Q^+(s, \theta)\)</span> (this is the "critic")</li>
<li><strong>Actor</strong>: Policy inferred from field via Boltzmann
  $<span class="arithmatex">\(\pi(\theta | s) \propto \exp(\beta \, Q^+(s, \theta))\)</span>$</li>
</ul>
<p><strong>Update</strong>:</p>
<ul>
<li><strong>Field (Critic)</strong>: RF-SARSA (two-layer TD system)</li>
<li>Primitive layer: TD learning for discrete transitions</li>
<li>
<p>GP layer: Smooth field over augmented space</p>
</li>
<li>
<p><strong>Policy (Actor)</strong>: Derived automatically from field</p>
</li>
<li>No separate actor parameters!</li>
<li>Policy gradient = field gradient</li>
</ul>
<p><strong>Connection</strong>:</p>
<ul>
<li><span class="arithmatex">\(Q^+(s, \theta)\)</span> plays the role of both Q-function and value function</li>
<li>Boltzmann policy automatically balances exploitation (high <span class="arithmatex">\(Q^+\)</span>) and exploration (low <span class="arithmatex">\(\beta\)</span>)</li>
<li>Advantage: <span class="arithmatex">\(A(s, \theta) = Q^+(s, \theta) - \max_{\theta'} Q^+(s, \theta')\)</span></li>
</ul>
<p><strong>This recovers Actor-Critic!</strong></p>
<hr />
<h3 id="key-takeaways_3">Key Takeaways<a class="headerlink" href="#key-takeaways_3" title="Permanent link">&para;</a></h3>
<p><strong>Actor-Critic is GRL with</strong>:</p>
<ol>
<li>
<p><strong>Reinforcement field as critic</strong></p>
</li>
<li>
<p><strong>Boltzmann policy as actor</strong> (no separate parameters)</p>
</li>
<li><strong>RF-SARSA as update rule</strong></li>
</ol>
<p><strong>What GRL adds</strong>:</p>
<ul>
<li><strong>Unified representation</strong>: No need for separate actor and critic</li>
<li><strong>Automatic exploration</strong>: Temperature <span class="arithmatex">\(\beta\)</span> replaces entropy regularization</li>
<li><strong>Particle-based</strong>: Memory naturally handles off-policy data</li>
</ul>
<p><strong>Special Cases</strong>:</p>
<ul>
<li><strong>PPO</strong>: GRL with clipped field updates, on-policy sampling</li>
<li><strong>SAC</strong>: GRL with entropy term in field (equivalent to temperature)</li>
</ul>
<hr />
<p><a name="rlhf"></a></p>
<h2 id="6-recovery-5-rlhf-for-llms">6. Recovery 5: RLHF for LLMs<a class="headerlink" href="#6-recovery-5-rlhf-for-llms" title="Permanent link">&para;</a></h2>
<h3 id="classical-rlhf-reinforcement-learning-from-human-feedback">Classical RLHF (Reinforcement Learning from Human Feedback)<a class="headerlink" href="#classical-rlhf-reinforcement-learning-from-human-feedback" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong> (e.g., for ChatGPT):</p>
<ul>
<li><strong>LLM</strong>: <span class="arithmatex">\(\pi_\phi(a_t | s_t)\)</span> where <span class="arithmatex">\(s_t\)</span> = (prompt, response so far), <span class="arithmatex">\(a_t\)</span> = next token</li>
<li><strong>Reward Model</strong>: <span class="arithmatex">\(r_\theta(s, a)\)</span> learned from human preferences</li>
<li><strong>Algorithm</strong>: PPO or similar policy gradient method</li>
</ul>
<p><strong>Update</strong> (PPO):
$<span class="arithmatex">\(\mathcal{L}(\phi) = \mathbb{E}_{(s,a) \sim \pi_\phi} [\min(r_\text{clip}, r_\text{KL})]\)</span>$</p>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(r_\text{clip}\)</span> = clipped advantage</li>
<li><span class="arithmatex">\(r_\text{KL}\)</span> = KL penalty from reference policy</li>
</ul>
<hr />
<h3 id="grl-version_4">GRL Version<a class="headerlink" href="#grl-version_4" title="Permanent link">&para;</a></h3>
<p><strong>Setup</strong>:</p>
<ul>
<li><strong>State</strong>: <span class="arithmatex">\(s_t\)</span> = (prompt, partial response)</li>
<li><strong>Action</strong>: <span class="arithmatex">\(\theta_t\)</span> = token ID or logit vector (discrete or continuous parameterization)</li>
<li><strong>Augmented space</strong>: <span class="arithmatex">\((s_t, \theta_t)\)</span> in semantic embedding space</li>
<li><strong>Reinforcement field</strong>: <span class="arithmatex">\(Q^+(s_t, \theta_t)\)</span> = expected reward for generating token <span class="arithmatex">\(\theta_t\)</span> in context <span class="arithmatex">\(s_t\)</span></li>
</ul>
<p><strong>Formulation</strong>:</p>
<p><strong>Option 1: Discrete Tokens</strong> (Classical RL recovery)
- <span class="arithmatex">\(\theta_t \in \{1, \ldots, V\}\)</span> (vocabulary size <span class="arithmatex">\(V\)</span>)
- Field: <span class="arithmatex">\(Q^+(s_t, \theta_t)\)</span> for each token
- Policy: Softmax over field
  $<span class="arithmatex">\(\pi(\theta_t | s_t) = \frac{\exp(\beta \, Q^+(s_t, \theta_t))}{\sum_{\theta'} \exp(\beta \, Q^+(s_t, \theta'))}\)</span>$</p>
<p><strong>Option 2: Continuous Parameterization</strong> (GRL extension)
- <span class="arithmatex">\(\theta_t \in \mathbb{R}^d\)</span> = token embedding or logit vector
- Field: <span class="arithmatex">\(Q^+(s_t, \theta_t)\)</span> smooth over embedding space
- Policy: Sample from continuous distribution over <span class="arithmatex">\(\theta\)</span>, map to nearest token</p>
<p><strong>Update</strong>: RF-SARSA with human feedback as rewards
- Particle memory stores (prompt, response, reward) tuples
- Kernel generalizes across similar prompts/responses
- Field learns <span class="arithmatex">\(Q^+(s, \theta)\)</span> via TD learning</p>
<hr />
<h3 id="advantages-of-grl-for-rlhf">Advantages of GRL for RLHF<a class="headerlink" href="#advantages-of-grl-for-rlhf" title="Permanent link">&para;</a></h3>
<p><strong>1. Off-Policy Learning</strong></p>
<ul>
<li>Particle memory enables replay</li>
<li>Sample efficiency: learn from all past human feedback</li>
<li>Classical RLHF (PPO) is on-policy only</li>
</ul>
<p><strong>2. Smooth Generalization</strong></p>
<ul>
<li>Kernel similarity between prompts</li>
<li>Transfer value across related contexts</li>
<li>Fewer human labels needed</li>
</ul>
<p><strong>3. Uncertainty Quantification</strong></p>
<ul>
<li>Sparse particles = high uncertainty</li>
<li>Exploration naturally targets uncertain regions</li>
<li>Safety: avoid high-stakes decisions with low confidence</li>
</ul>
<p><strong>4. Interpretability</strong></p>
<ul>
<li>Energy landscape over prompt space</li>
<li>Visualize which responses are preferred</li>
<li>Particle inspection: "Why did you say that?"</li>
</ul>
<hr />
<h3 id="implementation-path">Implementation Path<a class="headerlink" href="#implementation-path" title="Permanent link">&para;</a></h3>
<p><strong>Phase 1: Discrete Tokens (Q1 2026)</strong></p>
<ul>
<li>Implement GRL on small model (GPT-2)</li>
<li>Reproduce PPO results on standard RLHF benchmarks</li>
<li>Show GRL recovers classical behavior</li>
</ul>
<p><strong>Phase 2: Comparison (Q2 2026)</strong></p>
<ul>
<li>Compare sample efficiency: GRL vs. PPO</li>
<li>Measure stability: fewer human labels needed?</li>
<li>Quantify uncertainty: does GRL know what it doesn't know?</li>
</ul>
<p><strong>Phase 3: Scale (Q3 2026)</strong></p>
<ul>
<li>Apply to larger model (LLaMA-7B or Mistral-7B)</li>
<li>Test on real human feedback datasets</li>
<li>Submit paper: "GRL for LLM Fine-tuning"</li>
</ul>
<hr />
<h3 id="key-takeaways_4">Key Takeaways<a class="headerlink" href="#key-takeaways_4" title="Permanent link">&para;</a></h3>
<p><strong>RLHF is GRL with</strong>:</p>
<ol>
<li><strong>Discrete action space</strong> (tokens)</li>
<li><strong>On-policy updates</strong> (PPO)</li>
<li><strong>Neural network approximation</strong> of field</li>
</ol>
<p><strong>What GRL adds for RLHF</strong>:</p>
<ul>
<li><strong>Off-policy learning</strong> (replay buffer of human feedback)</li>
<li><strong>Kernel generalization</strong> (transfer across prompts)</li>
<li><strong>Uncertainty</strong> (exploration where most uncertain)</li>
<li><strong>Interpretability</strong> (energy landscapes, particle inspection)</li>
</ul>
<p><strong>Strategic Impact</strong>: Demonstrating GRL on RLHF:</p>
<ul>
<li>Validates GRL on most commercially relevant RL problem</li>
<li>Opens door to industry adoption (OpenAI, Anthropic, Meta)</li>
<li>Natural bridge to scaling research</li>
</ul>
<hr />
<p><a name="beyond"></a></p>
<h2 id="7-what-grl-adds-beyond-classical-rl">7. What GRL Adds Beyond Classical RL<a class="headerlink" href="#7-what-grl-adds-beyond-classical-rl" title="Permanent link">&para;</a></h2>
<p>While GRL recovers classical methods, it also enables capabilities that are difficult or impossible in standard RL:</p>
<h3 id="1-continuous-action-generalization">1. Continuous Action Generalization<a class="headerlink" href="#1-continuous-action-generalization" title="Permanent link">&para;</a></h3>
<p><strong>Classical RL</strong>: Discretize continuous actions or use neural networks
<strong>GRL</strong>: Smooth field <span class="arithmatex">\(Q^+(s, \theta)\)</span> over continuous <span class="arithmatex">\(\theta\)</span> via kernels</p>
<p><strong>Example</strong>: Robot grasping
- Classical: Sample <span class="arithmatex">\(N\)</span> discrete grasp poses, learn Q-value for each
- GRL: Learn smooth field over continuous grasp space, interpolate between samples</p>
<hr />
<h3 id="2-compositional-actions">2. Compositional Actions<a class="headerlink" href="#2-compositional-actions" title="Permanent link">&para;</a></h3>
<p><strong>Classical RL</strong>: Actions are atomic
<strong>GRL</strong>: Actions are operators that can be composed</p>
<p><strong>Example</strong>: Multi-step manipulation
- Classical: Learn separate policies for "pick", "place", "push"
- GRL: Learn operators that compose: <span class="arithmatex">\(\hat{O}_{\text{place}} \circ \hat{O}_{\text{pick}}\)</span></p>
<hr />
<h3 id="3-uncertainty-quantification">3. Uncertainty Quantification<a class="headerlink" href="#3-uncertainty-quantification" title="Permanent link">&para;</a></h3>
<p><strong>Classical RL</strong>: Uncertainty requires ensembles or Bayesian NNs
<strong>GRL</strong>: Particle sparsity directly indicates uncertainty</p>
<p><strong>Example</strong>: Safe exploration
- Classical: Ensemble of Q-networks, high variance = uncertain
- GRL: Sparse particles = uncertain, avoid or explore based on risk</p>
<hr />
<h3 id="4-energy-based-regularization">4. Energy-Based Regularization<a class="headerlink" href="#4-energy-based-regularization" title="Permanent link">&para;</a></h3>
<p><strong>Classical RL</strong>: Entropy regularization (SAC), KL penalties (PPO)
<strong>GRL</strong>: Energy function <span class="arithmatex">\(E = -Q^+\)</span> naturally regularizes via least-action principle</p>
<p><strong>Example</strong>: Smooth, efficient policies
- Classical: Add entropy bonus to reward
- GRL: Energy naturally prefers smooth, low-energy paths (physics-inspired)</p>
<hr />
<h3 id="5-particle-based-interpretability">5. Particle-Based Interpretability<a class="headerlink" href="#5-particle-based-interpretability" title="Permanent link">&para;</a></h3>
<p><strong>Classical RL</strong>: Black-box neural networks
<strong>GRL</strong>: Particles are interpretable experiences</p>
<p><strong>Example</strong>: Debugging
- Classical: "Why did the policy fail?" → Inspect millions of weights
- GRL: "Why did the policy fail?" → Inspect nearby particles, visualize energy landscape</p>
<hr />
<h3 id="6-hierarchical-abstraction-part-ii">6. Hierarchical Abstraction (Part II)<a class="headerlink" href="#6-hierarchical-abstraction-part-ii" title="Permanent link">&para;</a></h3>
<p><strong>Classical RL</strong>: Hierarchical RL requires careful design
<strong>GRL</strong>: Concepts emerge via spectral clustering</p>
<p><strong>Example</strong>: Long-horizon tasks
- Classical: Manually define options/skills
- GRL: Spectral decomposition discovers concepts automatically</p>
<hr />
<p><a name="implementation"></a></p>
<h2 id="8-implementation-from-grl-to-classical">8. Implementation: From GRL to Classical<a class="headerlink" href="#8-implementation-from-grl-to-classical" title="Permanent link">&para;</a></h2>
<h3 id="code-example-q-learning-from-grl">Code Example: Q-Learning from GRL<a class="headerlink" href="#code-example-q-learning-from-grl" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">grl.core</span><span class="w"> </span><span class="kn">import</span> <span class="n">ParticleMemory</span><span class="p">,</span> <span class="n">DeltaKernel</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">grl.algorithms</span><span class="w"> </span><span class="kn">import</span> <span class="n">MemoryUpdate</span>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a><span class="c1"># Classical Q-learning setup</span>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="n">state_space</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;s1&quot;</span><span class="p">,</span> <span class="s2">&quot;s2&quot;</span><span class="p">,</span> <span class="s2">&quot;s3&quot;</span><span class="p">]</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="n">action_space</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;a1&quot;</span><span class="p">,</span> <span class="s2">&quot;a2&quot;</span><span class="p">]</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="c1"># GRL setup: Map discrete actions to fixed parameters</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="n">action_params</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="__span-0-10"><a id="__codelineno-0-10" name="__codelineno-0-10" href="#__codelineno-0-10"></a>    <span class="s2">&quot;a1&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">0.0</span><span class="p">]),</span>  <span class="c1"># θ_1</span>
</span><span id="__span-0-11"><a id="__codelineno-0-11" name="__codelineno-0-11" href="#__codelineno-0-11"></a>    <span class="s2">&quot;a2&quot;</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">]),</span>  <span class="c1"># θ_2</span>
</span><span id="__span-0-12"><a id="__codelineno-0-12" name="__codelineno-0-12" href="#__codelineno-0-12"></a><span class="p">}</span>
</span><span id="__span-0-13"><a id="__codelineno-0-13" name="__codelineno-0-13" href="#__codelineno-0-13"></a>
</span><span id="__span-0-14"><a id="__codelineno-0-14" name="__codelineno-0-14" href="#__codelineno-0-14"></a><span class="c1"># Particle memory (GRL)</span>
</span><span id="__span-0-15"><a id="__codelineno-0-15" name="__codelineno-0-15" href="#__codelineno-0-15"></a><span class="n">memory</span> <span class="o">=</span> <span class="n">ParticleMemory</span><span class="p">()</span>
</span><span id="__span-0-16"><a id="__codelineno-0-16" name="__codelineno-0-16" href="#__codelineno-0-16"></a>
</span><span id="__span-0-17"><a id="__codelineno-0-17" name="__codelineno-0-17" href="#__codelineno-0-17"></a><span class="c1"># Delta kernel (no generalization)</span>
</span><span id="__span-0-18"><a id="__codelineno-0-18" name="__codelineno-0-18" href="#__codelineno-0-18"></a><span class="n">kernel</span> <span class="o">=</span> <span class="n">DeltaKernel</span><span class="p">()</span>
</span><span id="__span-0-19"><a id="__codelineno-0-19" name="__codelineno-0-19" href="#__codelineno-0-19"></a>
</span><span id="__span-0-20"><a id="__codelineno-0-20" name="__codelineno-0-20" href="#__codelineno-0-20"></a><span class="c1"># Experience: (s, a, r, s&#39;)</span>
</span><span id="__span-0-21"><a id="__codelineno-0-21" name="__codelineno-0-21" href="#__codelineno-0-21"></a><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">s_next</span> <span class="o">=</span> <span class="s2">&quot;s1&quot;</span><span class="p">,</span> <span class="s2">&quot;a1&quot;</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="s2">&quot;s2&quot;</span>
</span><span id="__span-0-22"><a id="__codelineno-0-22" name="__codelineno-0-22" href="#__codelineno-0-22"></a>
</span><span id="__span-0-23"><a id="__codelineno-0-23" name="__codelineno-0-23" href="#__codelineno-0-23"></a><span class="c1"># Convert to GRL format</span>
</span><span id="__span-0-24"><a id="__codelineno-0-24" name="__codelineno-0-24" href="#__codelineno-0-24"></a><span class="n">z</span> <span class="o">=</span> <span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">action_params</span><span class="p">[</span><span class="n">a</span><span class="p">])</span>
</span><span id="__span-0-25"><a id="__codelineno-0-25" name="__codelineno-0-25" href="#__codelineno-0-25"></a><span class="n">w</span> <span class="o">=</span> <span class="n">r</span>
</span><span id="__span-0-26"><a id="__codelineno-0-26" name="__codelineno-0-26" href="#__codelineno-0-26"></a>
</span><span id="__span-0-27"><a id="__codelineno-0-27" name="__codelineno-0-27" href="#__codelineno-0-27"></a><span class="c1"># MemoryUpdate (GRL) ≡ Q-learning update (Classical)</span>
</span><span id="__span-0-28"><a id="__codelineno-0-28" name="__codelineno-0-28" href="#__codelineno-0-28"></a><span class="n">memory</span> <span class="o">=</span> <span class="n">memory_update</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
</span><span id="__span-0-29"><a id="__codelineno-0-29" name="__codelineno-0-29" href="#__codelineno-0-29"></a>
</span><span id="__span-0-30"><a id="__codelineno-0-30" name="__codelineno-0-30" href="#__codelineno-0-30"></a><span class="c1"># Query field (GRL) ≡ Q(s, a) (Classical)</span>
</span><span id="__span-0-31"><a id="__codelineno-0-31" name="__codelineno-0-31" href="#__codelineno-0-31"></a><span class="n">Q_sa</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">query</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">action_params</span><span class="p">[</span><span class="n">a</span><span class="p">]))</span>
</span><span id="__span-0-32"><a id="__codelineno-0-32" name="__codelineno-0-32" href="#__codelineno-0-32"></a>
</span><span id="__span-0-33"><a id="__codelineno-0-33" name="__codelineno-0-33" href="#__codelineno-0-33"></a><span class="c1"># This is Q-learning!</span>
</span></code></pre></div>
<hr />
<h3 id="code-example-dqn-from-grl">Code Example: DQN from GRL<a class="headerlink" href="#code-example-dqn-from-grl" title="Permanent link">&para;</a></h3>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">grl.core</span><span class="w"> </span><span class="kn">import</span> <span class="n">NeuralField</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">grl.algorithms</span><span class="w"> </span><span class="kn">import</span> <span class="n">FieldTDLearning</span>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="c1"># Neural network approximates reinforcement field</span>
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a><span class="n">field</span> <span class="o">=</span> <span class="n">NeuralField</span><span class="p">(</span><span class="n">state_dim</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">action_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">64</span><span class="p">)</span>
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a>
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a><span class="c1"># Experience replay (GRL particle memory)</span>
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a><span class="n">memory</span> <span class="o">=</span> <span class="n">ParticleMemory</span><span class="p">()</span>
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a><span class="c1"># Training loop</span>
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a><span class="k">for</span> <span class="n">episode</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a>    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_steps</span><span class="p">):</span>
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>        <span class="c1"># Sample from memory (experience replay)</span>
</span><span id="__span-1-14"><a id="__codelineno-1-14" name="__codelineno-1-14" href="#__codelineno-1-14"></a>        <span class="n">batch</span> <span class="o">=</span> <span class="n">memory</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</span><span id="__span-1-15"><a id="__codelineno-1-15" name="__codelineno-1-15" href="#__codelineno-1-15"></a>
</span><span id="__span-1-16"><a id="__codelineno-1-16" name="__codelineno-1-16" href="#__codelineno-1-16"></a>        <span class="c1"># Compute TD targets (same as DQN)</span>
</span><span id="__span-1-17"><a id="__codelineno-1-17" name="__codelineno-1-17" href="#__codelineno-1-17"></a>        <span class="n">td_targets</span> <span class="o">=</span> <span class="n">compute_td_targets</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">field</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mf">0.99</span><span class="p">)</span>
</span><span id="__span-1-18"><a id="__codelineno-1-18" name="__codelineno-1-18" href="#__codelineno-1-18"></a>
</span><span id="__span-1-19"><a id="__codelineno-1-19" name="__codelineno-1-19" href="#__codelineno-1-19"></a>        <span class="c1"># Update field (GRL) ≡ Update Q-network (DQN)</span>
</span><span id="__span-1-20"><a id="__codelineno-1-20" name="__codelineno-1-20" href="#__codelineno-1-20"></a>        <span class="n">loss</span> <span class="o">=</span> <span class="n">field</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">td_targets</span><span class="p">)</span>
</span><span id="__span-1-21"><a id="__codelineno-1-21" name="__codelineno-1-21" href="#__codelineno-1-21"></a>
</span><span id="__span-1-22"><a id="__codelineno-1-22" name="__codelineno-1-22" href="#__codelineno-1-22"></a><span class="c1"># This is DQN!</span>
</span></code></pre></div>
<hr />
<h2 id="conclusion">Conclusion<a class="headerlink" href="#conclusion" title="Permanent link">&para;</a></h2>
<p><strong>GRL is a unifying framework</strong> that:</p>
<ol>
<li>✅ <strong>Recovers classical RL</strong> (Q-learning, DQN, REINFORCE, PPO, SAC, RLHF)</li>
<li>✅ <strong>Extends to continuous actions</strong> (smooth generalization via kernels)</li>
<li>✅ <strong>Enables composition</strong> (operator algebra)</li>
<li>✅ <strong>Provides uncertainty</strong> (particle sparsity)</li>
<li>✅ <strong>Interprets naturally</strong> (energy landscapes, particles)</li>
<li>✅ <strong>Discovers structure</strong> (spectral concepts in Part II)</li>
</ol>
<p><strong>For practitioners</strong>: GRL gives you what you already know (classical RL), plus new tools for continuous control, uncertainty, and interpretability.</p>
<p><strong>For researchers</strong>: GRL provides a principled foundation for understanding why existing methods work and how to extend them.</p>
<p><strong>For industry</strong>: GRL applies to modern problems (RLHF for LLMs) while offering advantages (off-policy, uncertainty, sample efficiency).</p>
<hr />
<h2 id="next-steps">Next Steps<a class="headerlink" href="#next-steps" title="Permanent link">&para;</a></h2>
<p><strong>Reproduce Classical Results</strong>:</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Implement Q-learning recovery on GridWorld</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Implement DQN recovery on CartPole</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Implement PPO recovery on continuous control (Pendulum)</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Implement RLHF recovery on small LLM (GPT-2)</li>
</ul>
<p><strong>Document Connections</strong>:</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Add "Classical RL Recovery" section to each tutorial chapter</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Create comparison tables (classical vs. GRL)</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Write blog post: "GRL: A Unifying Framework for RL"</li>
</ul>
<p><strong>Validate</strong>:</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Benchmark: GRL vs. DQN on Atari</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Benchmark: GRL vs. SAC on MuJoCo</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Benchmark: GRL vs. PPO on RLHF tasks</li>
</ul>
<p><strong>Scale</strong>:</p>
<ul class="task-list">
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Apply GRL to LLaMA-7B fine-tuning</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Demonstrate advantages (sample efficiency, uncertainty)</li>
<li class="task-list-item"><label class="task-list-control"><input type="checkbox" disabled/><span class="task-list-indicator"></span></label> Submit paper: "GRL for LLM Fine-tuning"</li>
</ul>
<hr />
<p><strong>Last Updated</strong>: January 14, 2026<br />
<strong>See also</strong>: <a href="../implementation/">Implementation Roadmap</a> | <a href="../../ROADMAP/">Research Roadmap</a></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2026 GRL Research Team
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/pleiadian53/GRL" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>