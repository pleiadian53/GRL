
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Physics-grounded reinforcement learning with particle-based belief representations">
      
      
        <meta name="author" content="GRL Research Team">
      
      
        <link rel="canonical" href="https://pleiadian53.github.io/GRL/GRL0/tutorials/07c-experience-replay-and-particle-memory/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Chapter 07c: Experience Replay and Particle Memory - Generalized Reinforcement Learning (GRL)</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#chapter-07c-experience-replay-and-particle-memory" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="Generalized Reinforcement Learning (GRL)" class="md-header__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Generalized Reinforcement Learning (GRL)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Chapter 07c: Experience Replay and Particle Memory
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../" class="md-tabs__link">
          
  
  
  GRL v0 (Tutorial Paper)

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../ROADMAP/" class="md-tabs__link">
        
  
  
    
  
  Research Roadmap

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../notebooks/" class="md-tabs__link">
          
  
  
  Notebooks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../CONTRIBUTING/" class="md-tabs__link">
          
  
  
  About

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="Generalized Reinforcement Learning (GRL)" class="md-nav__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Generalized Reinforcement Learning (GRL)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    GRL v0 (Tutorial Paper)
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    GRL v0 (Tutorial Paper)
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Part I: Tutorials
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part I: Tutorials
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tutorial Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../00-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 0: Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../01-core-concepts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 1: Core Concepts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../02-rkhs-foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 2: RKHS Foundations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03-energy-and-fitness/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3: Energy and Fitness
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../03a-least-action-principle/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3a: Least Action Principle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04-reinforcement-field/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4: Reinforcement Field
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../04a-riesz-representer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4a: Riesz Representer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../05-particle-memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 5: Particle Memory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06-memory-update/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6: MemoryUpdate
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../06a-advanced-memory-dynamics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6a: Advanced Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07-rf-sarsa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7: RF-SARSA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../07a-continuous-policy-inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7a: Continuous Policy Inference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Quantum-Inspired Extensions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Quantum-Inspired Extensions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/01-rkhs-quantum-parallel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: RKHS-QM Parallel
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/01a-wavefunction-interpretation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Wavefunction Interpretation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/02-rkhs-basis-and-amplitudes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Basis and Amplitudes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/03-complex-rkhs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Complex RKHS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/04-action-and-state-fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    04: Action and State Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/05-concept-projections-and-measurements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    05: Concept Projections
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/06-agent-state-and-belief-evolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    06: Agent State and Belief
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/07-learning-the-field-beyond-gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    07: Learning Beyond GP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    08: Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../quantum_inspired/09-path-integrals-and-action-principles/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    09: Path Integrals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../implementation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Implementation Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../recovering_classical_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Recovering Classical RL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Research Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Notebooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Notebooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Field Series
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Field Series
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Series Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/00_intro_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    00: Introduction to Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/01_classical_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: Classical Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/01a_vector_fields_and_odes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Vector Fields and ODEs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/02_functional_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Functional Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2_7" >
        
          
          <label class="md-nav__link" for="__nav_4_2_7" id="__nav_4_2_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reinforcement Fields
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reinforcement Fields
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Reinforcement Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03a: Particle Coverage Effects
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Theory: Particle vs Gradient Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    About
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    About
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CONTRIBUTING/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Contributing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../LICENSE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    License
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        Introduction
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1-the-problem-that-experience-replay-solves" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. The Problem That Experience Replay Solves
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. The Problem That Experience Replay Solves">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-online-learning-is-wasteful" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 Online learning is wasteful
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-online-learning-is-correlated" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 Online learning is correlated
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-the-solution-store-and-resample" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.3 The solution: store and resample
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-experience-replay-in-practice" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. Experience Replay in Practice
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. Experience Replay in Practice">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-the-basic-mechanism-dqn" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 The basic mechanism (DQN)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-prioritized-experience-replay" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 Prioritized experience replay
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-other-replay-variants" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 Other replay variants
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-what-experience-replay-actually-does-abstractly" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. What Experience Replay Actually Does (Abstractly)
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. What Experience Replay Actually Does (Abstractly)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-three-functions-of-replay" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 Three functions of replay
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-the-key-abstraction" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 The key abstraction
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-grls-particle-memory-as-implicit-experience-replay" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. GRL's Particle Memory as Implicit Experience Replay
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. GRL&#39;s Particle Memory as Implicit Experience Replay">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-the-structural-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 The structural parallel
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-side-by-side-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 Side-by-side comparison
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-the-deep-difference-data-level-vs-function-level-reuse" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 The deep difference: data-level vs. function-level reuse
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-three-functions-of-replay-revisited-through-grl" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. Three Functions of Replay, Revisited Through GRL
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. Three Functions of Replay, Revisited Through GRL">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-temporal-persistence-of-information" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 Temporal persistence of information
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-distributional-smoothing" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 Distributional smoothing
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#53-multi-use-of-scarce-data" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.3 Multi-use of scarce data
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-prioritized-replay-and-kernel-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Prioritized Replay and Kernel Similarity
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="6. Prioritized Replay and Kernel Similarity">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#61-an-unexpected-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      
        6.1 An unexpected parallel
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#62-relevance-vs-surprise" class="md-nav__link">
    <span class="md-ellipsis">
      
        6.2 Relevance vs. surprise
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#63-can-grl-benefit-from-surprise-based-prioritization" class="md-nav__link">
    <span class="md-ellipsis">
      
        6.3 Can GRL benefit from surprise-based prioritization?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-where-replay-buffers-struggle-and-particle-memory-doesnt" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Where Replay Buffers Struggle and Particle Memory Doesn't
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Where Replay Buffers Struggle and Particle Memory Doesn&#39;t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-the-staleness-problem" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.1 The staleness problem
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-the-capacity-problem" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2 The capacity problem
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73-the-representation-gap" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.3 The representation gap
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-where-particle-memory-struggles-and-replay-buffers-dont" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Where Particle Memory Struggles and Replay Buffers Don't
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Where Particle Memory Struggles and Replay Buffers Don&#39;t">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#81-scalability" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.1 Scalability
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#82-flexibility-with-function-approximators" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.2 Flexibility with function approximators
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#83-off-policy-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.3 Off-policy learning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#84-high-dimensional-state-spaces" class="md-nav__link">
    <span class="md-ellipsis">
      
        8.4 High-dimensional state spaces
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#9-historical-context-grl-predated-dqns-experience-replay" class="md-nav__link">
    <span class="md-ellipsis">
      
        9. Historical Context: GRL Predated DQN's Experience Replay
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="9. Historical Context: GRL Predated DQN&#39;s Experience Replay">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#91-timeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        9.1 Timeline
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#92-different-motivations-convergent-solutions" class="md-nav__link">
    <span class="md-ellipsis">
      
        9.2 Different motivations, convergent solutions
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#93-what-grl-got-right-early" class="md-nav__link">
    <span class="md-ellipsis">
      
        9.3 What GRL got right early
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#10-bridging-the-two-worlds" class="md-nav__link">
    <span class="md-ellipsis">
      
        10. Bridging the Two Worlds
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="10. Bridging the Two Worlds">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#101-hybrid-architectures" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.1 Hybrid architectures
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#102-neural-experience-replay-as-learned-particle-memory" class="md-nav__link">
    <span class="md-ellipsis">
      
        10.2 Neural experience replay as learned particle memory
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#11-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        11. Summary
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="11. Summary">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#111-experience-replay-in-one-sentence" class="md-nav__link">
    <span class="md-ellipsis">
      
        11.1 Experience replay in one sentence
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#112-particle-memory-in-one-sentence" class="md-nav__link">
    <span class="md-ellipsis">
      
        11.2 Particle memory in one sentence
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#113-the-relationship" class="md-nav__link">
    <span class="md-ellipsis">
      
        11.3 The relationship
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#114-the-key-insight" class="md-nav__link">
    <span class="md-ellipsis">
      
        11.4 The key insight
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/pleiadian53/GRL/edit/main/docs/GRL0/tutorials/07c-experience-replay-and-particle-memory.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg>
    </a>
  
  


<h1 id="chapter-07c-experience-replay-and-particle-memory">Chapter 07c: Experience Replay and Particle Memory<a class="headerlink" href="#chapter-07c-experience-replay-and-particle-memory" title="Permanent link">&para;</a></h1>
<p><strong>Purpose</strong>: Understand experience replay in modern RL, and how GRL's particle memory is a structurally richer form of the same idea<br />
<strong>Prerequisites</strong>: Chapter 05 (Particle Memory), Chapter 07 (RF-SARSA)<br />
<strong>Key Concepts</strong>: Experience replay, replay buffers, prioritized replay, particle memory as implicit replay, functional vs. data-level reuse</p>
<hr />
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p>In 2013, DeepMind's DQN paper introduced <strong>experience replay</strong> as one of two key innovations (alongside target networks) that made deep Q-learning stable enough to play Atari games at superhuman level. The idea was simple: instead of learning from each transition once and discarding it, store transitions in a buffer and resample them for training.</p>
<p>Experience replay is now ubiquitous in off-policy deep RL. But the core idea — <strong>reusing past experience to improve learning efficiency</strong> — is older than DQN and, as we'll see, was already present in GRL's particle memory (2010), albeit in a fundamentally different form.</p>
<p>This chapter covers:</p>
<ol>
<li>What experience replay is and why it matters</li>
<li>How replay buffers work in practice (DQN, prioritized replay, etc.)</li>
<li>The deep structural comparison with GRL's particle memory</li>
<li>Why particle memory is "experience replay done at the function level"</li>
<li>What each approach can learn from the other</li>
</ol>
<hr />
<h2 id="1-the-problem-that-experience-replay-solves">1. The Problem That Experience Replay Solves<a class="headerlink" href="#1-the-problem-that-experience-replay-solves" title="Permanent link">&para;</a></h2>
<h3 id="11-online-learning-is-wasteful">1.1 Online learning is wasteful<a class="headerlink" href="#11-online-learning-is-wasteful" title="Permanent link">&para;</a></h3>
<p>In standard online TD learning, the agent:</p>
<ol>
<li>Observes transition <span class="arithmatex">\((s_t, a_t, r_t, s_{t+1})\)</span></li>
<li>Performs one update using this transition</li>
<li><strong>Discards the transition forever</strong></li>
</ol>
<p>This is enormously wasteful. Each interaction with the environment produces information that is used exactly once. In domains where environment interactions are expensive (robotics, simulation, real-world deployment), this is unacceptable.</p>
<h3 id="12-online-learning-is-correlated">1.2 Online learning is correlated<a class="headerlink" href="#12-online-learning-is-correlated" title="Permanent link">&para;</a></h3>
<p>Consecutive transitions are highly correlated — <span class="arithmatex">\(s_{t+1}\)</span> is similar to <span class="arithmatex">\(s_t\)</span>, the agent visits the same region of state space for many steps before moving on. Training a function approximator on correlated data violates the i.i.d. assumption that underlies stochastic gradient descent, leading to:</p>
<ul>
<li><strong>Oscillation</strong>: The approximator overfits to the current region, then overcorrects when the agent moves elsewhere</li>
<li><strong>Catastrophic forgetting</strong>: Learning about new states erases knowledge about old ones</li>
<li><strong>Poor convergence</strong>: Correlated updates bias the gradient estimates</li>
</ul>
<h3 id="13-the-solution-store-and-resample">1.3 The solution: store and resample<a class="headerlink" href="#13-the-solution-store-and-resample" title="Permanent link">&para;</a></h3>
<p><strong>Experience replay</strong> addresses both problems:</p>
<ul>
<li><strong>Efficiency</strong>: Each transition can be reused many times</li>
<li><strong>Decorrelation</strong>: Random sampling from a buffer breaks temporal correlations</li>
<li><strong>Stability</strong>: The training distribution is more uniform over the state space</li>
</ul>
<hr />
<h2 id="2-experience-replay-in-practice">2. Experience Replay in Practice<a class="headerlink" href="#2-experience-replay-in-practice" title="Permanent link">&para;</a></h2>
<h3 id="21-the-basic-mechanism-dqn">2.1 The basic mechanism (DQN)<a class="headerlink" href="#21-the-basic-mechanism-dqn" title="Permanent link">&para;</a></h3>
<p>The DQN algorithm (Mnih et al., 2013, 2015) introduced the standard replay buffer:</p>
<p><strong>Data structure</strong>: A fixed-capacity buffer <span class="arithmatex">\(\mathcal{D}\)</span> storing transitions:</p>
<div class="arithmatex">\[\mathcal{D} = \{(s_i, a_i, r_i, s_{i+1})\}_{i=1}^{|\mathcal{D}|}\]</div>
<p><strong>Collection</strong>: At each step, store the new transition in <span class="arithmatex">\(\mathcal{D}\)</span> (overwriting the oldest if full).</p>
<p><strong>Training</strong>: Sample a random mini-batch <span class="arithmatex">\(\mathcal{B} \subset \mathcal{D}\)</span> and perform a gradient update:</p>
<div class="arithmatex">\[w \leftarrow w - \alpha \frac{1}{|\mathcal{B}|} \sum_{(s,a,r,s') \in \mathcal{B}} \nabla_w \left[ Q_w(s, a) - (r + \gamma \max_{a'} Q_{w^-}(s', a')) \right]^2\]</div>
<p>where <span class="arithmatex">\(w^-\)</span> are the target network parameters (frozen periodically).</p>
<p><strong>Key properties</strong>:</p>
<ul>
<li><strong>Uniform sampling</strong>: Each transition is equally likely to be sampled</li>
<li><strong>FIFO eviction</strong>: When the buffer is full, the oldest transitions are discarded</li>
<li><strong>Off-policy</strong>: The transitions in the buffer were generated by old policies — the current policy may be very different</li>
</ul>
<h3 id="22-prioritized-experience-replay">2.2 Prioritized experience replay<a class="headerlink" href="#22-prioritized-experience-replay" title="Permanent link">&para;</a></h3>
<p>Schaul et al. (2016) observed that not all transitions are equally useful. Transitions with <strong>large TD errors</strong> are more informative — they indicate regions where the value function is most wrong.</p>
<p><strong>Priority</strong>: Assign each transition a priority proportional to its TD error:</p>
<div class="arithmatex">\[p_i = |\delta_i| + \epsilon\]</div>
<p>where <span class="arithmatex">\(\delta_i = r_i + \gamma \max_{a'} Q(s_{i+1}, a') - Q(s_i, a_i)\)</span> and <span class="arithmatex">\(\epsilon &gt; 0\)</span> prevents zero priority.</p>
<p><strong>Sampling</strong>: Draw transitions with probability:</p>
<div class="arithmatex">\[P(i) = \frac{p_i^\alpha}{\sum_j p_j^\alpha}\]</div>
<p>where <span class="arithmatex">\(\alpha \in [0, 1]\)</span> controls how much prioritization matters (<span class="arithmatex">\(\alpha = 0\)</span> is uniform, <span class="arithmatex">\(\alpha = 1\)</span> is fully prioritized).</p>
<p><strong>Importance sampling correction</strong>: Since prioritized sampling is biased (oversamples high-error transitions), correct with importance sampling weights:</p>
<div class="arithmatex">\[w_i = \left( \frac{1}{|\mathcal{D}|} \cdot \frac{1}{P(i)} \right)^\beta\]</div>
<p>where <span class="arithmatex">\(\beta\)</span> is annealed from a small value to 1 over training.</p>
<h3 id="23-other-replay-variants">2.3 Other replay variants<a class="headerlink" href="#23-other-replay-variants" title="Permanent link">&para;</a></h3>
<p>The field has produced many refinements:</p>
<table>
<thead>
<tr>
<th>Variant</th>
<th>Key Idea</th>
<th>Reference</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Hindsight Experience Replay (HER)</strong></td>
<td>Relabel failed episodes with achieved goals</td>
<td>Andrychowicz et al., 2017</td>
</tr>
<tr>
<td><strong>Distributed Replay</strong></td>
<td>Multiple actors fill a shared buffer</td>
<td>Horgan et al., 2018 (Ape-X)</td>
</tr>
<tr>
<td><strong>N-step Replay</strong></td>
<td>Store n-step returns instead of 1-step</td>
<td>Mnih et al., 2016 (A3C)</td>
</tr>
<tr>
<td><strong>Curiosity-Prioritized</strong></td>
<td>Prioritize by novelty, not just TD error</td>
<td>Pathak et al., 2017</td>
</tr>
<tr>
<td><strong>Reservoir Sampling</strong></td>
<td>Uniform sampling without FIFO bias</td>
<td>Isele &amp; Cosgun, 2018</td>
</tr>
</tbody>
</table>
<p>All of these operate at the <strong>data level</strong>: they store, organize, and resample raw transitions.</p>
<hr />
<h2 id="3-what-experience-replay-actually-does-abstractly">3. What Experience Replay Actually Does (Abstractly)<a class="headerlink" href="#3-what-experience-replay-actually-does-abstractly" title="Permanent link">&para;</a></h2>
<p>Before comparing with GRL, let's identify what experience replay achieves at a conceptual level, independent of implementation:</p>
<h3 id="31-three-functions-of-replay">3.1 Three functions of replay<a class="headerlink" href="#31-three-functions-of-replay" title="Permanent link">&para;</a></h3>
<p><strong>Function 1: Temporal persistence of information</strong></p>
<p>Without replay, information from a transition is used once and lost. Replay ensures that the <em>information content</em> of past experience persists and continues to influence learning.</p>
<p><strong>Function 2: Distributional smoothing</strong></p>
<p>Online learning sees a non-stationary, correlated stream of data. Replay transforms this into a more uniform, decorrelated distribution — closer to the stationary distribution needed for convergence.</p>
<p><strong>Function 3: Multi-use of scarce data</strong></p>
<p>Each environment interaction is expensive. Replay extracts more learning signal per interaction by revisiting transitions multiple times.</p>
<h3 id="32-the-key-abstraction">3.2 The key abstraction<a class="headerlink" href="#32-the-key-abstraction" title="Permanent link">&para;</a></h3>
<p>At its core, experience replay is about <strong>making past experience available for current learning</strong>. The specific mechanism (buffer, sampling, prioritization) is implementation detail. The essential property is:</p>
<blockquote>
<p><strong>Information from past interactions persists and influences future updates.</strong></p>
</blockquote>
<p>This is exactly what GRL's particle memory does — but through a completely different mechanism.</p>
<hr />
<h2 id="4-grls-particle-memory-as-implicit-experience-replay">4. GRL's Particle Memory as Implicit Experience Replay<a class="headerlink" href="#4-grls-particle-memory-as-implicit-experience-replay" title="Permanent link">&para;</a></h2>
<h3 id="41-the-structural-parallel">4.1 The structural parallel<a class="headerlink" href="#41-the-structural-parallel" title="Permanent link">&para;</a></h3>
<p>In GRL, when the agent experiences a transition and RF-SARSA produces a TD update, the result is a particle <span class="arithmatex">\((z, q)\)</span> added to memory <span class="arithmatex">\(\Omega\)</span>. This particle then influences <strong>every future field query</strong> <span class="arithmatex">\(Q^+(z')\)</span> through the kernel:</p>
<div class="arithmatex">\[Q^+(z') = \sum_{i=1}^N \alpha_i \, k(z', z_i)\]</div>
<p>Every past experience — encoded as a particle — contributes to every future prediction. The particle doesn't need to be "replayed" or "resampled" because it is <strong>always active</strong> as a basis function in the field representation.</p>
<p>This is experience replay, but at the <strong>function level</strong> rather than the <strong>data level</strong>.</p>
<h3 id="42-side-by-side-comparison">4.2 Side-by-side comparison<a class="headerlink" href="#42-side-by-side-comparison" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Aspect</th>
<th>Experience Replay (DQN)</th>
<th>Particle Memory (GRL)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>What is stored</strong></td>
<td>Raw transitions <span class="arithmatex">\((s, a, r, s')\)</span></td>
<td>Weighted points <span class="arithmatex">\((z, w)\)</span> in augmented space</td>
</tr>
<tr>
<td><strong>How past experience is reused</strong></td>
<td>Resampled and re-trained on</td>
<td>Always active as basis functions</td>
</tr>
<tr>
<td><strong>When reuse happens</strong></td>
<td>At training time (mini-batch sampling)</td>
<td>At inference time (every field query)</td>
</tr>
<tr>
<td><strong>Mechanism</strong></td>
<td>Stochastic gradient descent on replayed data</td>
<td>Kernel-weighted superposition</td>
</tr>
<tr>
<td><strong>Frequency of reuse</strong></td>
<td>Each transition sampled <span class="arithmatex">\(\sim k\)</span> times</td>
<td>Each particle contributes to every query</td>
</tr>
<tr>
<td><strong>Information loss</strong></td>
<td>Transitions eventually evicted (FIFO)</td>
<td>Particles merged/pruned but information preserved in neighbors</td>
</tr>
<tr>
<td><strong>Correlation breaking</strong></td>
<td>Random sampling from buffer</td>
<td>Kernel geometry (smooth, nonlocal)</td>
</tr>
<tr>
<td><strong>Prioritization</strong></td>
<td>Explicit (TD error priority)</td>
<td>Implicit (kernel similarity to query point)</td>
</tr>
<tr>
<td><strong>Off-policy correction</strong></td>
<td>Importance sampling weights</td>
<td>Not needed (on-policy with SARSA)</td>
</tr>
</tbody>
</table>
<h3 id="43-the-deep-difference-data-level-vs-function-level-reuse">4.3 The deep difference: data-level vs. function-level reuse<a class="headerlink" href="#43-the-deep-difference-data-level-vs-function-level-reuse" title="Permanent link">&para;</a></h3>
<p>This distinction is the crux of the comparison:</p>
<p><strong>Experience replay (data-level)</strong>:</p>
<ol>
<li>Store raw data: <span class="arithmatex">\((s, a, r, s')\)</span></li>
<li>Resample data</li>
<li>Recompute gradients from resampled data</li>
<li>Update function approximator</li>
</ol>
<p>The function approximator (neural network) is <strong>separate</strong> from the stored data. Replay feeds data <em>into</em> the approximator.</p>
<p><strong>Particle memory (function-level)</strong>:</p>
<ol>
<li>Convert experience into a basis function: <span class="arithmatex">\((z, w) \to w \cdot k(\cdot, z)\)</span></li>
<li>The basis function <strong>is</strong> part of the value function</li>
<li>Every future query automatically incorporates this experience</li>
<li>No resampling needed</li>
</ol>
<p>The stored experience <strong>is</strong> the function approximator. There is no separation between "data" and "model."</p>
<blockquote>
<p><strong>In DQN, past experience is replayed <em>to</em> the value function. In GRL, past experience <em>is</em> the value function.</strong></p>
</blockquote>
<hr />
<h2 id="5-three-functions-of-replay-revisited-through-grl">5. Three Functions of Replay, Revisited Through GRL<a class="headerlink" href="#5-three-functions-of-replay-revisited-through-grl" title="Permanent link">&para;</a></h2>
<p>Let's check whether GRL's particle memory achieves the three abstract functions of experience replay identified in Section 3.</p>
<h3 id="51-temporal-persistence-of-information">5.1 Temporal persistence of information<a class="headerlink" href="#51-temporal-persistence-of-information" title="Permanent link">&para;</a></h3>
<p><strong>Replay buffer</strong>: Transitions persist in the buffer until evicted (FIFO or priority-based). Information is lost when transitions are overwritten.</p>
<p><strong>Particle memory</strong>: Particles persist indefinitely (subject to merging/pruning). Even when particles are merged, their information is preserved in the merged particle's weight and position. Information loss is gradual and controlled, not abrupt.</p>
<p><strong>Verdict</strong>: Particle memory provides <strong>stronger</strong> temporal persistence. Information is never abruptly discarded — it is smoothly absorbed into the field.</p>
<h3 id="52-distributional-smoothing">5.2 Distributional smoothing<a class="headerlink" href="#52-distributional-smoothing" title="Permanent link">&para;</a></h3>
<p><strong>Replay buffer</strong>: Random sampling from the buffer approximates a uniform distribution over past experience. This breaks temporal correlations.</p>
<p><strong>Particle memory</strong>: The kernel provides automatic smoothing. When the agent queries <span class="arithmatex">\(Q^+(z)\)</span>, the prediction is a kernel-weighted average over <strong>all</strong> particles — not a random sample. This is a deterministic, smooth interpolation that naturally decorrelates the influence of any single experience.</p>
<p><strong>Verdict</strong>: Particle memory provides <strong>deterministic</strong> distributional smoothing (via kernel), compared to replay's <strong>stochastic</strong> smoothing (via random sampling). The kernel approach is arguably more principled — it weights past experience by <em>relevance</em> (kernel similarity) rather than by <em>chance</em> (random sampling).</p>
<h3 id="53-multi-use-of-scarce-data">5.3 Multi-use of scarce data<a class="headerlink" href="#53-multi-use-of-scarce-data" title="Permanent link">&para;</a></h3>
<p><strong>Replay buffer</strong>: Each transition is sampled <span class="arithmatex">\(\sim k\)</span> times on average before eviction. The replay ratio (updates per environment step) controls how much each transition is reused.</p>
<p><strong>Particle memory</strong>: Each particle contributes to <strong>every</strong> field query for its entire lifetime. A particle added at step 1 is still influencing predictions at step 10,000 (with influence decaying smoothly via the kernel as the agent moves to distant regions of augmented space).</p>
<p><strong>Verdict</strong>: Particle memory achieves <strong>maximal</strong> data reuse — every past experience influences every future prediction, weighted by relevance. This is the theoretical ideal that replay buffers approximate through sampling.</p>
<hr />
<h2 id="6-prioritized-replay-and-kernel-similarity">6. Prioritized Replay and Kernel Similarity<a class="headerlink" href="#6-prioritized-replay-and-kernel-similarity" title="Permanent link">&para;</a></h2>
<h3 id="61-an-unexpected-parallel">6.1 An unexpected parallel<a class="headerlink" href="#61-an-unexpected-parallel" title="Permanent link">&para;</a></h3>
<p>Prioritized experience replay (Schaul et al., 2016) samples transitions with probability proportional to their TD error. The intuition: transitions where the value function is most wrong are most informative.</p>
<p>GRL's particle memory has an analogous mechanism, but it operates through <strong>kernel similarity</strong> rather than explicit prioritization:</p>
<p>When the agent queries <span class="arithmatex">\(Q^+(z)\)</span>, each particle <span class="arithmatex">\(i\)</span> contributes:</p>
<div class="arithmatex">\[\text{contribution}_i = \alpha_i \, k(z, z_i)\]</div>
<p>Particles that are <strong>close</strong> to the query point (high <span class="arithmatex">\(k(z, z_i)\)</span>) contribute more. Particles that are <strong>far</strong> contribute less. This is <strong>automatic, implicit prioritization by relevance</strong>.</p>
<h3 id="62-relevance-vs-surprise">6.2 Relevance vs. surprise<a class="headerlink" href="#62-relevance-vs-surprise" title="Permanent link">&para;</a></h3>
<p>The two prioritization schemes optimize for different things:</p>
<table>
<thead>
<tr>
<th>Prioritized Replay</th>
<th>Particle Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td>Prioritizes by <strong>surprise</strong> (<span class="arithmatex">\(\lvert\delta\rvert\)</span>)</td>
<td>Prioritizes by <strong>relevance</strong> (<span class="arithmatex">\(k(z, z_i)\)</span>)</td>
</tr>
<tr>
<td>"Where am I most wrong?"</td>
<td>"What do I know about <em>here</em>?"</td>
</tr>
<tr>
<td>Global: any transition can be sampled</td>
<td>Local: nearby particles dominate</td>
</tr>
<tr>
<td>Requires explicit priority updates</td>
<td>Automatic via kernel geometry</td>
</tr>
</tbody>
</table>
<p><strong>Insight</strong>: These are complementary. Surprise-based prioritization helps <em>learning</em> (focus updates where errors are large). Relevance-based weighting helps <em>inference</em> (focus predictions on nearby evidence).</p>
<h3 id="63-can-grl-benefit-from-surprise-based-prioritization">6.3 Can GRL benefit from surprise-based prioritization?<a class="headerlink" href="#63-can-grl-benefit-from-surprise-based-prioritization" title="Permanent link">&para;</a></h3>
<p>Yes. MemoryUpdate already incorporates the TD error <span class="arithmatex">\(\delta\)</span> when updating particle weights (Chapter 6). Particles associated with large <span class="arithmatex">\(|\delta|\)</span> receive larger weight updates. This is a form of surprise-based prioritization at the <strong>update</strong> level, complementing the relevance-based weighting at the <strong>query</strong> level.</p>
<p>A more explicit version could weight particles by both kernel similarity and accumulated TD error magnitude — combining the strengths of both approaches.</p>
<hr />
<h2 id="7-where-replay-buffers-struggle-and-particle-memory-doesnt">7. Where Replay Buffers Struggle and Particle Memory Doesn't<a class="headerlink" href="#7-where-replay-buffers-struggle-and-particle-memory-doesnt" title="Permanent link">&para;</a></h2>
<h3 id="71-the-staleness-problem">7.1 The staleness problem<a class="headerlink" href="#71-the-staleness-problem" title="Permanent link">&para;</a></h3>
<p>Transitions in a replay buffer were generated by <strong>old policies</strong>. As the policy improves, old transitions become increasingly unrepresentative of the current policy's behavior. This is the <strong>staleness</strong> problem.</p>
<p><strong>Consequences</strong>:</p>
<ul>
<li>Old transitions may have incorrect state visitation frequencies</li>
<li>The replay distribution diverges from the current policy's distribution</li>
<li>Importance sampling corrections become high-variance</li>
</ul>
<p><strong>Particle memory</strong>: Particles are not "stale" in the same way. A particle <span class="arithmatex">\((z_i, w_i)\)</span> represents a <strong>belief about the value at location <span class="arithmatex">\(z_i\)</span></strong>, not a raw transition. MemoryUpdate continuously revises particle weights as new evidence arrives. Old particles don't represent old policies — they represent the agent's <strong>current best estimate</strong> at that location, informed by all evidence accumulated so far.</p>
<h3 id="72-the-capacity-problem">7.2 The capacity problem<a class="headerlink" href="#72-the-capacity-problem" title="Permanent link">&para;</a></h3>
<p>Replay buffers have fixed capacity. When full, old transitions are evicted — permanently losing information. Choosing what to evict is a non-trivial problem (FIFO? priority-based? reservoir sampling?).</p>
<p><strong>Particle memory</strong>: Particles can be <strong>merged</strong> rather than evicted. When two particles are close in augmented space, they can be combined into a single particle that preserves the aggregate information. This is information-preserving compression, not information-destroying eviction.</p>
<h3 id="73-the-representation-gap">7.3 The representation gap<a class="headerlink" href="#73-the-representation-gap" title="Permanent link">&para;</a></h3>
<p>A replay buffer stores raw data. The function approximator (neural network) must learn to extract useful representations from this data. There is a <strong>gap</strong> between what is stored (transitions) and what is needed (value function).</p>
<p><strong>Particle memory</strong>: There is no gap. Particles directly define the value function through kernel superposition. The representation <em>is</em> the stored experience.</p>
<hr />
<h2 id="8-where-particle-memory-struggles-and-replay-buffers-dont">8. Where Particle Memory Struggles and Replay Buffers Don't<a class="headerlink" href="#8-where-particle-memory-struggles-and-replay-buffers-dont" title="Permanent link">&para;</a></h2>
<p>The comparison is not one-sided. Replay buffers have genuine advantages:</p>
<h3 id="81-scalability">8.1 Scalability<a class="headerlink" href="#81-scalability" title="Permanent link">&para;</a></h3>
<p>Replay buffers store compact tuples and scale to millions of transitions with <span class="arithmatex">\(O(1)\)</span> insertion and <span class="arithmatex">\(O(1)\)</span> random access. GP-based particle memory scales as <span class="arithmatex">\(O(N^2)\)</span> for queries and <span class="arithmatex">\(O(N^3)\)</span> for hyperparameter updates.</p>
<p><strong>Mitigation</strong>: Sparse GPs, inducing points, random Fourier features (discussed in Chapter 7, Section 9).</p>
<h3 id="82-flexibility-with-function-approximators">8.2 Flexibility with function approximators<a class="headerlink" href="#82-flexibility-with-function-approximators" title="Permanent link">&para;</a></h3>
<p>Replay buffers are <strong>agnostic</strong> to the function approximator. They work with neural networks, linear models, decision trees — anything that can be trained on mini-batches. Particle memory is tied to kernel-based representations.</p>
<p><strong>Mitigation</strong>: Neural field replacements (Chapter 1 of the baseline concepts document) can bridge this gap.</p>
<h3 id="83-off-policy-learning">8.3 Off-policy learning<a class="headerlink" href="#83-off-policy-learning" title="Permanent link">&para;</a></h3>
<p>Replay buffers are designed for off-policy learning — the whole point is to reuse data from old policies. Particle memory, as used in RF-SARSA, is on-policy. Using particle memory for off-policy learning requires the safeguards discussed in Chapter 07b.</p>
<h3 id="84-high-dimensional-state-spaces">8.4 High-dimensional state spaces<a class="headerlink" href="#84-high-dimensional-state-spaces" title="Permanent link">&para;</a></h3>
<p>Kernel methods suffer from the curse of dimensionality in high-dimensional spaces (images, raw sensor data). Replay buffers combined with deep neural networks handle high-dimensional inputs naturally through learned representations.</p>
<p><strong>Mitigation</strong>: Learned embeddings (Chapter 07a, Section 4) or hybrid architectures.</p>
<hr />
<h2 id="9-historical-context-grl-predated-dqns-experience-replay">9. Historical Context: GRL Predated DQN's Experience Replay<a class="headerlink" href="#9-historical-context-grl-predated-dqns-experience-replay" title="Permanent link">&para;</a></h2>
<h3 id="91-timeline">9.1 Timeline<a class="headerlink" href="#91-timeline" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>1992</strong>: Lin introduced experience replay for neural network RL (Lin, 1992)</li>
<li><strong>2010</strong>: GRL proposed particle memory as a kernel-based experience representation</li>
<li><strong>2013</strong>: Mnih et al. (DQN) popularized experience replay as essential for deep RL stability</li>
<li><strong>2016</strong>: Schaul et al. introduced prioritized experience replay</li>
</ul>
<p>GRL's particle memory (2010) was developed independently and contemporaneously with the resurgence of interest in experience replay. While Lin's 1992 work predates both, the modern understanding of <em>why</em> replay matters (decorrelation, stability, efficiency) was crystallized by DQN in 2013.</p>
<h3 id="92-different-motivations-convergent-solutions">9.2 Different motivations, convergent solutions<a class="headerlink" href="#92-different-motivations-convergent-solutions" title="Permanent link">&para;</a></h3>
<p><strong>DQN's motivation</strong>: "Neural networks need i.i.d. data. Online RL data is correlated. Solution: buffer and resample."</p>
<p><strong>GRL's motivation</strong>: "Value functions are continuous fields. Experience provides sparse samples of this field. Solution: represent the field directly through kernel-weighted particles."</p>
<p>These are different starting points that arrive at the same abstract property: <strong>past experience persists and influences future learning</strong>. The implementations differ radically (data-level resampling vs. function-level superposition), but the functional role is the same.</p>
<h3 id="93-what-grl-got-right-early">9.3 What GRL got right early<a class="headerlink" href="#93-what-grl-got-right-early" title="Permanent link">&para;</a></h3>
<p>GRL's particle memory anticipated several ideas that the deep RL community discovered later:</p>
<ol>
<li><strong>Prioritization by relevance</strong> (kernel similarity) — formalized by Schaul et al. (2016) as prioritized replay</li>
<li><strong>Information-preserving compression</strong> (particle merging) — echoed in compressed replay buffers and memory-efficient replay</li>
<li><strong>Continuous reuse</strong> (every particle always active) — the theoretical ideal that high replay ratios approximate</li>
<li><strong>Uncertainty-aware inference</strong> (GP variance) — now pursued through ensemble methods and distributional RL</li>
</ol>
<p>What GRL did <em>not</em> anticipate was the scalability of deep neural networks and the practical dominance of data-level replay in high-dimensional domains. The kernel-based approach is more principled but less scalable — a trade-off that the field is still navigating.</p>
<hr />
<h2 id="10-bridging-the-two-worlds">10. Bridging the Two Worlds<a class="headerlink" href="#10-bridging-the-two-worlds" title="Permanent link">&para;</a></h2>
<h3 id="101-hybrid-architectures">10.1 Hybrid architectures<a class="headerlink" href="#101-hybrid-architectures" title="Permanent link">&para;</a></h3>
<p>The most promising direction may be to combine both approaches:</p>
<p><strong>Neural network + particle critic</strong>:</p>
<ul>
<li>Use a neural network for state representation (handles high-dimensional inputs)</li>
<li>Use particle memory in the learned representation space (provides kernel-based value estimation)</li>
<li>Use a replay buffer to train the neural network (provides data-level reuse)</li>
</ul>
<p>This is essentially the <strong>Actor-Critic in RKHS</strong> architecture from Chapter 07a, augmented with a replay buffer for the actor.</p>
<h3 id="102-neural-experience-replay-as-learned-particle-memory">10.2 Neural experience replay as learned particle memory<a class="headerlink" href="#102-neural-experience-replay-as-learned-particle-memory" title="Permanent link">&para;</a></h3>
<p>Modern work on <strong>neural episodic control</strong> (Pritzel et al., 2017) and <strong>differentiable neural dictionaries</strong> (Ritter et al., 2018) can be viewed as neural network implementations of particle memory:</p>
<ul>
<li>Store (key, value) pairs where keys are learned embeddings</li>
<li>Query via kernel-weighted lookup (often with learned kernels)</li>
<li>This is structurally identical to particle memory with learned features</li>
</ul>
<p>The convergence of these independent research lines suggests that the particle memory concept is a natural and powerful abstraction.</p>
<hr />
<h2 id="11-summary">11. Summary<a class="headerlink" href="#11-summary" title="Permanent link">&para;</a></h2>
<h3 id="111-experience-replay-in-one-sentence">11.1 Experience replay in one sentence<a class="headerlink" href="#111-experience-replay-in-one-sentence" title="Permanent link">&para;</a></h3>
<p>Experience replay stores raw transitions and resamples them for training, breaking temporal correlations and improving sample efficiency.</p>
<h3 id="112-particle-memory-in-one-sentence">11.2 Particle memory in one sentence<a class="headerlink" href="#112-particle-memory-in-one-sentence" title="Permanent link">&para;</a></h3>
<p>Particle memory converts experience into kernel basis functions that directly define the value function, providing automatic, continuous, relevance-weighted reuse of all past experience.</p>
<h3 id="113-the-relationship">11.3 The relationship<a class="headerlink" href="#113-the-relationship" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Level</th>
<th>Experience Replay</th>
<th>Particle Memory</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Data</strong></td>
<td>Stores and resamples transitions</td>
<td>Stores weighted points</td>
</tr>
<tr>
<td><strong>Function</strong></td>
<td>Trains a separate approximator</td>
<td>Points <em>are</em> the approximator</td>
</tr>
<tr>
<td><strong>Reuse</strong></td>
<td>Stochastic (random sampling)</td>
<td>Deterministic (kernel weighting)</td>
</tr>
<tr>
<td><strong>Prioritization</strong></td>
<td>Explicit (TD error)</td>
<td>Implicit (kernel similarity)</td>
</tr>
<tr>
<td><strong>Persistence</strong></td>
<td>Until eviction</td>
<td>Until merging (information-preserving)</td>
</tr>
</tbody>
</table>
<h3 id="114-the-key-insight">11.4 The key insight<a class="headerlink" href="#114-the-key-insight" title="Permanent link">&para;</a></h3>
<blockquote>
<p><strong>Experience replay and particle memory solve the same problem — making past experience available for current learning — but at different levels of abstraction. Replay operates at the data level (store and resample transitions). Particle memory operates at the function level (experience <em>is</em> the value function). GRL's particle memory can be understood as experience replay elevated from data management to functional representation.</strong></p>
</blockquote>
<hr />
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<ol>
<li>Lin, L.-J. (1992). "Self-improving reactive agents based on reinforcement learning, planning and teaching." <em>Machine Learning</em>, 8(3-4), 293-321. (Original experience replay)</li>
<li>Mnih, V., et al. (2013). "Playing Atari with deep reinforcement learning." <em>NeurIPS Deep Learning Workshop</em>. (DQN with replay)</li>
<li>Mnih, V., et al. (2015). "Human-level control through deep reinforcement learning." <em>Nature</em>, 518, 529-533. (DQN journal version)</li>
<li>Schaul, T., et al. (2016). "Prioritized experience replay." <em>ICLR</em>. (Prioritized replay)</li>
<li>Andrychowicz, M., et al. (2017). "Hindsight experience replay." <em>NeurIPS</em>. (HER)</li>
<li>Pritzel, A., et al. (2017). "Neural episodic control." <em>ICML</em>. (Neural particle memory analog)</li>
<li>Horgan, D., et al. (2018). "Distributed prioritized experience replay." <em>ICLR</em>. (Ape-X)</li>
<li>Chiu, C.-C. &amp; Huber, M. (2022). "Generalized Reinforcement Learning." <a href="https://arxiv.org/abs/2208.04822">arXiv:2208.04822</a>. (GRL particle memory)</li>
</ol>
<hr />
<p><strong><a href="../07-rf-sarsa/">← Back to Chapter 07: RF-SARSA</a></strong> | <strong><a href="../05-particle-memory/">Related: Chapter 05 - Particle Memory</a></strong></p>
<p><strong><a href="../07b-rf-q-learning-and-convergence/">Related: Chapter 07b - RF-Q-Learning and the Deadly Triad</a></strong> | <strong><a href="../06-memory-update/">Related: Chapter 06 - MemoryUpdate</a></strong></p>
<hr />
<p><em>Last Updated</em>: February 2026</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2026 GRL Research Team
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/pleiadian53/GRL" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit"], "search": "../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>