"""
Loss functions for GRL training.

These implement the key objectives:
- TD loss for value learning
- Policy gradient loss
- Least-action regularization
"""

import torch
import torch.nn.functional as F

from grl.operators.base import ActionOperator


def compute_td_loss(
    q_values: torch.Tensor,
    rewards: torch.Tensor,
    next_values: torch.Tensor,
    dones: torch.Tensor,
    gamma: float = 0.99,
) -> torch.Tensor:
    """
    Compute temporal difference loss.
    
    TD Target: r + Î³ * V(s') * (1 - done)
    Loss: MSE(Q(s, a), TD Target)
    
    Args:
        q_values: Predicted Q-values (batch_size,)
        rewards: Rewards (batch_size,)
        next_values: Value of next states (batch_size,)
        dones: Done flags (batch_size,)
        gamma: Discount factor
        
    Returns:
        Scalar TD loss
    """
    targets = rewards + gamma * (1 - dones.float()) * next_values
    return F.mse_loss(q_values, targets.detach())


def compute_policy_loss(
    q_values: torch.Tensor,
    log_probs: torch.Tensor = None,
    entropy_weight: float = 0.01,
) -> torch.Tensor:
    """
    Compute policy gradient loss.
    
    For operator policies, we maximize Q(s, O(s)) where O is
    the operator generated by the policy.
    
    Args:
        q_values: Q-values for policy actions (batch_size,)
        log_probs: Log probabilities of actions (optional, for entropy)
        entropy_weight: Weight for entropy bonus
        
    Returns:
        Scalar policy loss
    """
    # Maximize Q-value
    loss = -q_values.mean()
    
    # Entropy bonus for exploration (if using stochastic policy)
    if log_probs is not None:
        entropy = -log_probs.mean()
        loss = loss - entropy_weight * entropy
    
    return loss


def compute_least_action_loss(
    operator: ActionOperator,
    weight: float = 0.01,
) -> torch.Tensor:
    """
    Compute least-action regularization loss.
    
    This encourages the operator to use minimal "energy",
    inspired by the principle of least action in physics.
    
    Different operators define energy differently:
    - Affine: ||A - I||_F^2 + ||b||^2
    - Field: ||F(s)||^2
    - Kernel: entropy of attention weights
    
    Args:
        operator: The action operator
        weight: Regularization weight
        
    Returns:
        Scalar regularization loss
    """
    return weight * operator.energy()


def compute_smoothness_loss(
    states: torch.Tensor,
    next_states: torch.Tensor,
    predicted_next: torch.Tensor,
    weight: float = 0.1,
) -> torch.Tensor:
    """
    Encourage smooth predictions that align with actual transitions.
    
    This is a supervised component that can be used in model-based
    settings where we have access to the true dynamics.
    
    Args:
        states: Current states (batch_size, state_dim)
        next_states: True next states (batch_size, state_dim)
        predicted_next: Predicted next states (batch_size, state_dim)
        weight: Loss weight
        
    Returns:
        Scalar smoothness loss
    """
    return weight * F.mse_loss(predicted_next, next_states)


def compute_operator_consistency_loss(
    operator: ActionOperator,
    states: torch.Tensor,
    weight: float = 0.01,
) -> torch.Tensor:
    """
    Encourage the operator to be locally consistent.
    
    For nearby states, the operator should produce similar transformations.
    This is a Lipschitz-like regularizer.
    
    Args:
        operator: The action operator
        states: Batch of states (batch_size, state_dim)
        weight: Loss weight
        
    Returns:
        Scalar consistency loss
    """
    batch_size = states.shape[0]
    if batch_size < 2:
        return torch.tensor(0.0)
    
    # Apply operator to original and perturbed states
    outputs = operator(states)
    
    # Perturb states slightly
    noise = 0.01 * torch.randn_like(states)
    perturbed_outputs = operator(states + noise)
    
    # Difference in outputs should be proportional to difference in inputs
    input_diff = noise.norm(dim=-1)
    output_diff = (perturbed_outputs - outputs).norm(dim=-1)
    
    # Penalize if output changes too much relative to input
    ratio = output_diff / (input_diff + 1e-8)
    
    return weight * F.relu(ratio - 1.0).mean()  # Allow ratio up to 1.0
