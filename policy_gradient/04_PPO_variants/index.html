
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Physics-grounded reinforcement learning with particle-based belief representations">
      
      
        <meta name="author" content="GRL Research Team">
      
      
        <link rel="canonical" href="https://pleiadian53.github.io/GRL/policy_gradient/04_PPO_variants/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>PPO Variants and Modern Descendants - Generalized Reinforcement Learning (GRL)</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ppo-variants-and-modern-descendants" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Generalized Reinforcement Learning (GRL)" class="md-header__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Generalized Reinforcement Learning (GRL)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PPO Variants and Modern Descendants
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../GRL0/" class="md-tabs__link">
          
  
  
  GRL v0 (Tutorial Paper)

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../ROADMAP/" class="md-tabs__link">
        
  
  
    
  
  Research Roadmap

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../notebooks/" class="md-tabs__link">
          
  
  
  Notebooks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../CONTRIBUTING/" class="md-tabs__link">
          
  
  
  About

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Generalized Reinforcement Learning (GRL)" class="md-nav__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Generalized Reinforcement Learning (GRL)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    GRL v0 (Tutorial Paper)
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    GRL v0 (Tutorial Paper)
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Part I: Tutorials
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part I: Tutorials
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tutorial Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/00-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 0: Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/01-core-concepts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 1: Core Concepts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/02-rkhs-foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 2: RKHS Foundations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/03-energy-and-fitness/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3: Energy and Fitness
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/03a-least-action-principle/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3a: Least Action Principle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/04-reinforcement-field/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4: Reinforcement Field
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/04a-riesz-representer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4a: Riesz Representer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/05-particle-memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 5: Particle Memory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/06-memory-update/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6: MemoryUpdate
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/06a-advanced-memory-dynamics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6a: Advanced Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/07-rf-sarsa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7: RF-SARSA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/07a-continuous-policy-inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7a: Continuous Policy Inference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Quantum-Inspired Extensions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Quantum-Inspired Extensions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/01-rkhs-quantum-parallel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: RKHS-QM Parallel
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/01a-wavefunction-interpretation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Wavefunction Interpretation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Basis and Amplitudes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/03-complex-rkhs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Complex RKHS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/04-action-and-state-fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    04: Action and State Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/05-concept-projections-and-measurements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    05: Concept Projections
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/06-agent-state-and-belief-evolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    06: Agent State and Belief
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/07-learning-the-field-beyond-gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    07: Learning Beyond GP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    08: Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/09-path-integrals-and-action-principles/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    09: Path Integrals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/implementation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Implementation Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/recovering_classical_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Recovering Classical RL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Research Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Notebooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Notebooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Field Series
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Field Series
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Series Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/00_intro_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    00: Introduction to Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/01_classical_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: Classical Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/01a_vector_fields_and_odes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Vector Fields and ODEs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/02_functional_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Functional Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2_7" >
        
          
          <label class="md-nav__link" for="__nav_4_2_7" id="__nav_4_2_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reinforcement Fields
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reinforcement Fields
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Reinforcement Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03a: Particle Coverage Effects
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Theory: Particle vs Gradient Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    About
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    About
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CONTRIBUTING/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Contributing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LICENSE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    License
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-why-ppo-needed-successors" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. Why PPO Needed Successors
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="1. Why PPO Needed Successors">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11-the-four-model-burden-rlhf" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 The Four-Model Burden (RLHF)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12-ratio-variance-in-high-dimensions" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 Ratio Variance in High Dimensions
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13-advantage-estimation-difficulty" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.3 Advantage Estimation Difficulty
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-grpo-group-relative-policy-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. GRPO: Group Relative Policy Optimization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. GRPO: Group Relative Policy Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-the-core-idea" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 The Core Idea
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-the-grpo-objective" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 The GRPO Objective
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-what-grpo-eliminates" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 What GRPO Eliminates
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#24-why-group-normalization-works" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.4 Why Group Normalization Works
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#25-grpo-in-practice" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.5 GRPO in Practice
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-dpo-direct-preference-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. DPO: Direct Preference Optimization
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. DPO: Direct Preference Optimization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-the-insight" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 The Insight
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-the-derivation" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 The Derivation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-the-dpo-loss" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 The DPO Loss
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-what-dpo-eliminates" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4 What DPO Eliminates
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-dpos-relationship-to-policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.5 DPO's Relationship to Policy Gradient
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-limitations-of-dpo" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.6 Limitations of DPO
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-other-notable-variants" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Other Notable Variants
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Other Notable Variants">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-remax-reinforce-style-baseline-for-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 ReMax: Reinforce-Style Baseline for LLMs
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-rloo-reinforce-leave-one-out" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 RLOO: REINFORCE Leave-One-Out
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-rpo-relative-policy-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 RPO: Relative Policy Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-kto-kahneman-tversky-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.4 KTO: Kahneman-Tversky Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#45-rlhf-era-ppo-tweaks" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.5 RLHF-Era PPO Tweaks
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-the-landscape-a-comparative-view" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. The Landscape: A Comparative View
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. The Landscape: A Comparative View">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-what-each-method-optimizes" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 What Each Method Optimizes
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-the-fundamental-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 The Fundamental Tradeoff
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-where-ppos-ratio-still-hides-the-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. Where PPO's Ratio Still Hides the Gradient
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-open-questions-and-future-directions" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Open Questions and Future Directions
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Open Questions and Future Directions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-beyond-scalar-rewards" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.1 Beyond Scalar Rewards
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-process-supervision-vs-outcome-supervision" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2 Process Supervision vs. Outcome Supervision
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#73-scaling-laws-for-rl-fine-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.3 Scaling Laws for RL Fine-Tuning
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#74-connection-to-control-and-robotics" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.4 Connection to Control and Robotics
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Summary
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Summary">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#the-evolution" class="md-nav__link">
    <span class="md-ellipsis">
      
        The evolution
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-pattern" class="md-nav__link">
    <span class="md-ellipsis">
      
        The pattern
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-hasnt-changed" class="md-nav__link">
    <span class="md-ellipsis">
      
        What hasn't changed
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/pleiadian53/GRL/edit/main/docs/policy_gradient/04_PPO_variants.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg>
    </a>
  
  


<h1 id="ppo-variants-and-modern-descendants">PPO Variants and Modern Descendants<a class="headerlink" href="#ppo-variants-and-modern-descendants" title="Permanent link">&para;</a></h1>
<p><strong>Where PPO breaks, why GRPO exists, and what "state of the art" means in 2026</strong></p>
<blockquote>
<p><em>Prerequisites: <a href="../03_PPO/">PPO</a>, <a href="03a_pg_in_ppo.md">Where Is the Policy Gradient in PPO?</a></em></p>
</blockquote>
<hr />
<h2 id="1-why-ppo-needed-successors">1. Why PPO Needed Successors<a class="headerlink" href="#1-why-ppo-needed-successors" title="Permanent link">&para;</a></h2>
<p>PPO (<a href="../03_PPO/">Chapter 03</a>) is remarkably robust for a first-order method. But as it was applied to increasingly demanding problems — large language models, high-dimensional robotics, multi-agent systems — several failure modes became clear.</p>
<h3 id="11-the-four-model-burden-rlhf">1.1 The Four-Model Burden (RLHF)<a class="headerlink" href="#11-the-four-model-burden-rlhf" title="Permanent link">&para;</a></h3>
<p>In the RLHF pipeline for LLM alignment, PPO requires <strong>four models in memory simultaneously</strong>:</p>
<ol>
<li><strong>Policy</strong> <span class="arithmatex">\(\pi_\theta\)</span> — the language model being fine-tuned</li>
<li><strong>Reference policy</strong> <span class="arithmatex">\(\pi_{\text{ref}}\)</span> — the pre-trained model (frozen, for KL penalty)</li>
<li><strong>Reward model</strong> <span class="arithmatex">\(r_\phi\)</span> — trained from human preferences</li>
<li><strong>Value function</strong> <span class="arithmatex">\(V_\psi\)</span> — the critic, needed for advantage estimation</li>
</ol>
<p>For a 70B-parameter LLM, this means ~280B parameters in GPU memory. The computational cost is enormous, and the value function <span class="arithmatex">\(V_\psi\)</span> is notoriously hard to train well in the LLM setting (the "state" is a variable-length token sequence, and the reward is sparse — given only at the end of a full response).</p>
<h3 id="12-ratio-variance-in-high-dimensions">1.2 Ratio Variance in High Dimensions<a class="headerlink" href="#12-ratio-variance-in-high-dimensions" title="Permanent link">&para;</a></h3>
<p>Recall the PPO ratio:</p>
<div class="arithmatex">\[r_t(\theta) = \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_k}(a_t \mid s_t)}\]</div>
<p>For an LLM generating a response of <span class="arithmatex">\(T\)</span> tokens, the full-sequence ratio is a <strong>product</strong> of per-token ratios:</p>
<div class="arithmatex">\[r(\theta) = \prod_{t=1}^T \frac{\pi_\theta(a_t \mid s_t)}{\pi_{\theta_k}(a_t \mid s_t)}\]</div>
<p>Products of many terms near 1.0 can still produce extreme values. If each per-token ratio deviates by <span class="arithmatex">\(\pm 5\%\)</span>, the full-sequence ratio after 100 tokens can range from <span class="arithmatex">\(0.006\)</span> to <span class="arithmatex">\(170\)</span>. Clipping helps, but it also <strong>kills the gradient</strong> for many samples (see <a href="03a_pg_in_ppo.md">Chapter 03a, Section 5</a>), reducing sample efficiency.</p>
<h3 id="13-advantage-estimation-difficulty">1.3 Advantage Estimation Difficulty<a class="headerlink" href="#13-advantage-estimation-difficulty" title="Permanent link">&para;</a></h3>
<p>GAE (<a href="../01_PG/">Chapter 01, Section 6</a>) requires a good value function <span class="arithmatex">\(V(s)\)</span>. In robotics, the state is a fixed-size vector and the value function is easy to learn. In LLM alignment:</p>
<ul>
<li>The "state" is a variable-length token sequence</li>
<li>The reward is typically given only at the end of a full response (sparse)</li>
<li>The value function must generalize across diverse prompts and response lengths</li>
</ul>
<p>A poor value function produces noisy advantages, which produce noisy gradients, which produce unstable training. This is the single biggest practical complaint about PPO for RLHF.</p>
<hr />
<h2 id="2-grpo-group-relative-policy-optimization">2. GRPO: Group Relative Policy Optimization<a class="headerlink" href="#2-grpo-group-relative-policy-optimization" title="Permanent link">&para;</a></h2>
<p><strong>GRPO</strong> (Shao et al., 2024) was developed at DeepSeek specifically to address the value function problem in LLM alignment. Its key insight: <strong>replace the learned value function with a group-based baseline computed from the policy's own samples.</strong></p>
<h3 id="21-the-core-idea">2.1 The Core Idea<a class="headerlink" href="#21-the-core-idea" title="Permanent link">&para;</a></h3>
<p>Instead of training a separate critic <span class="arithmatex">\(V_\psi(s)\)</span>, GRPO:</p>
<ol>
<li>For each prompt <span class="arithmatex">\(q\)</span>, samples a <strong>group</strong> of <span class="arithmatex">\(G\)</span> responses: <span class="arithmatex">\(\{o_1, o_2, \ldots, o_G\} \sim \pi_{\theta_k}(\cdot \mid q)\)</span></li>
<li>Scores each response with the reward model: <span class="arithmatex">\(r_1, r_2, \ldots, r_G\)</span></li>
<li>Computes a <strong>group-normalized advantage</strong> for each response:</li>
</ol>
<div class="arithmatex">\[\hat{A}_i = \frac{r_i - \text{mean}(\{r_j\}_{j=1}^G)}{\text{std}(\{r_j\}_{j=1}^G)}\]</div>
<p>This is a simple z-score within the group. No value function needed.</p>
<h3 id="22-the-grpo-objective">2.2 The GRPO Objective<a class="headerlink" href="#22-the-grpo-objective" title="Permanent link">&para;</a></h3>
<p>GRPO uses a PPO-style clipped objective, but with the group-normalized advantage:</p>
<div class="arithmatex">\[L^{\text{GRPO}}(\theta) = \mathbb{E}_{q \sim \mathcal{D}} \; \frac{1}{G} \sum_{i=1}^G \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \left[ \min\Big( r_{i,t}(\theta) \, \hat{A}_i, \;\; \text{clip}\big(r_{i,t}(\theta), 1-\epsilon, 1+\epsilon\big) \, \hat{A}_i \Big) \right] - \beta \, D_{\mathrm{KL}}(\pi_\theta \| \pi_{\text{ref}})\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} \mid q, o_{i,&lt;t})}{\pi_{\theta_k}(o_{i,t} \mid q, o_{i,&lt;t})}\)</span> is the per-token ratio for the <span class="arithmatex">\(i\)</span>-th response</li>
<li><span class="arithmatex">\(\hat{A}_i\)</span> is the <strong>same</strong> group-normalized advantage for all tokens in response <span class="arithmatex">\(i\)</span> (response-level, not token-level)</li>
<li><span class="arithmatex">\(\beta\)</span> controls the KL penalty against the reference policy <span class="arithmatex">\(\pi_{\text{ref}}\)</span></li>
<li><span class="arithmatex">\(|o_i|\)</span> is the length of response <span class="arithmatex">\(i\)</span> (the <span class="arithmatex">\(1/|o_i|\)</span> normalizes across different response lengths)</li>
</ul>
<h3 id="23-what-grpo-eliminates">2.3 What GRPO Eliminates<a class="headerlink" href="#23-what-grpo-eliminates" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>PPO</th>
<th>GRPO</th>
</tr>
</thead>
<tbody>
<tr>
<td>Policy <span class="arithmatex">\(\pi_\theta\)</span></td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Reference policy <span class="arithmatex">\(\pi_{\text{ref}}\)</span></td>
<td>Yes (for KL)</td>
<td>Yes (for KL)</td>
</tr>
<tr>
<td>Reward model <span class="arithmatex">\(r_\phi\)</span></td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Value function <span class="arithmatex">\(V_\psi\)</span></td>
<td><strong>Yes</strong></td>
<td><strong>No</strong></td>
</tr>
<tr>
<td>Models in memory</td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<p>Removing the value function saves ~25% of GPU memory and eliminates the hardest training signal in the pipeline.</p>
<h3 id="24-why-group-normalization-works">2.4 Why Group Normalization Works<a class="headerlink" href="#24-why-group-normalization-works" title="Permanent link">&para;</a></h3>
<p>The group-normalized advantage <span class="arithmatex">\(\hat{A}_i\)</span> is a <strong>relative</strong> ranking within the group. It doesn't estimate absolute value — it only says "response <span class="arithmatex">\(i\)</span> is better/worse than the group average for this prompt."</p>
<p>This works because:</p>
<ul>
<li><strong>Prompt-specific</strong>: each group is conditioned on the same prompt, so the baseline is relevant</li>
<li><strong>Self-normalizing</strong>: the z-score has zero mean and unit variance by construction, which stabilizes gradients</li>
<li><strong>No generalization needed</strong>: unlike <span class="arithmatex">\(V_\psi(s)\)</span>, which must generalize across all prompts, the group baseline is computed fresh for each prompt</li>
</ul>
<p>The tradeoff: GRPO requires <span class="arithmatex">\(G\)</span> samples per prompt (typically <span class="arithmatex">\(G = 4\)</span> to <span class="arithmatex">\(16\)</span>), which increases inference cost. But inference is cheaper than training a value function, especially for large models.</p>
<h3 id="25-grpo-in-practice">2.5 GRPO in Practice<a class="headerlink" href="#25-grpo-in-practice" title="Permanent link">&para;</a></h3>
<p>GRPO was used to train <strong>DeepSeek-R1</strong> and its variants. Key practical details:</p>
<ul>
<li><strong>Group size</strong>: <span class="arithmatex">\(G = 16\)</span> (larger groups give better baselines but cost more inference)</li>
<li><strong>KL penalty</strong>: <span class="arithmatex">\(\beta\)</span> is annealed during training</li>
<li><strong>Token-level vs. response-level</strong>: the advantage <span class="arithmatex">\(\hat{A}_i\)</span> is response-level (same for all tokens in a response), which is simpler but coarser than PPO's token-level GAE advantages</li>
<li><strong>Outcome supervision</strong>: GRPO naturally supports outcome-based rewards (e.g., "is the final answer correct?") without needing dense per-token rewards</li>
</ul>
<hr />
<h2 id="3-dpo-direct-preference-optimization">3. DPO: Direct Preference Optimization<a class="headerlink" href="#3-dpo-direct-preference-optimization" title="Permanent link">&para;</a></h2>
<p><strong>DPO</strong> (Rafailov et al., 2023) takes a more radical approach: <strong>eliminate the reward model entirely</strong> and optimize directly from preference data.</p>
<h3 id="31-the-insight">3.1 The Insight<a class="headerlink" href="#31-the-insight" title="Permanent link">&para;</a></h3>
<p>The standard RLHF pipeline is:</p>
<div class="arithmatex">\[\text{Preferences} \xrightarrow{\text{train}} \text{Reward Model} \xrightarrow{\text{PPO}} \text{Aligned Policy}\]</div>
<p>DPO collapses this into a single step:</p>
<div class="arithmatex">\[\text{Preferences} \xrightarrow{\text{DPO}} \text{Aligned Policy}\]</div>
<p>The key mathematical insight: the optimal policy under a KL-constrained reward maximization objective has a closed-form relationship with the reward function.</p>
<h3 id="32-the-derivation">3.2 The Derivation<a class="headerlink" href="#32-the-derivation" title="Permanent link">&para;</a></h3>
<p>The RLHF objective is:</p>
<div class="arithmatex">\[\max_{\pi_\theta} \; \mathbb{E}_{q \sim \mathcal{D}, \, o \sim \pi_\theta(\cdot|q)} \big[ r(q, o) \big] - \beta \, D_{\mathrm{KL}}\big(\pi_\theta(\cdot|q) \| \pi_{\text{ref}}(\cdot|q)\big)\]</div>
<p>The optimal policy for this objective is (derivable via calculus of variations):</p>
<div class="arithmatex">\[\pi^*(o \mid q) = \frac{1}{Z(q)} \pi_{\text{ref}}(o \mid q) \exp\!\left(\frac{r(q, o)}{\beta}\right)\]</div>
<p>where <span class="arithmatex">\(Z(q)\)</span> is a normalizing constant. Rearranging for the reward:</p>
<div class="arithmatex">\[r(q, o) = \beta \log \frac{\pi^*(o \mid q)}{\pi_{\text{ref}}(o \mid q)} + \beta \log Z(q)\]</div>
<p>Now substitute this into the <strong>Bradley-Terry preference model</strong> <span class="arithmatex">\(p(o_w \succ o_l \mid q) = \sigma(r(q, o_w) - r(q, o_l))\)</span>, where <span class="arithmatex">\(\sigma\)</span> is the sigmoid. The <span class="arithmatex">\(Z(q)\)</span> terms cancel:</p>
<div class="arithmatex">\[p(o_w \succ o_l \mid q) = \sigma\!\left( \beta \log \frac{\pi^*(o_w \mid q)}{\pi_{\text{ref}}(o_w \mid q)} - \beta \log \frac{\pi^*(o_l \mid q)}{\pi_{\text{ref}}(o_l \mid q)} \right)\]</div>
<h3 id="33-the-dpo-loss">3.3 The DPO Loss<a class="headerlink" href="#33-the-dpo-loss" title="Permanent link">&para;</a></h3>
<p>Replace <span class="arithmatex">\(\pi^*\)</span> with the learnable policy <span class="arithmatex">\(\pi_\theta\)</span> and maximize the log-likelihood of observed preferences:</p>
<div class="arithmatex">\[L^{\text{DPO}}(\theta) = -\mathbb{E}_{(q, o_w, o_l) \sim \mathcal{D}} \left[ \log \sigma\!\left( \beta \log \frac{\pi_\theta(o_w \mid q)}{\pi_{\text{ref}}(o_w \mid q)} - \beta \log \frac{\pi_\theta(o_l \mid q)}{\pi_{\text{ref}}(o_l \mid q)} \right) \right]\]</div>
<p>This is a <strong>supervised learning loss</strong> — no RL loop, no sampling, no reward model, no value function.</p>
<h3 id="34-what-dpo-eliminates">3.4 What DPO Eliminates<a class="headerlink" href="#34-what-dpo-eliminates" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>PPO</th>
<th>GRPO</th>
<th>DPO</th>
</tr>
</thead>
<tbody>
<tr>
<td>Policy <span class="arithmatex">\(\pi_\theta\)</span></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Reference policy <span class="arithmatex">\(\pi_{\text{ref}}\)</span></td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
</tr>
<tr>
<td>Reward model <span class="arithmatex">\(r_\phi\)</span></td>
<td>Yes</td>
<td>Yes</td>
<td><strong>No</strong></td>
</tr>
<tr>
<td>Value function <span class="arithmatex">\(V_\psi\)</span></td>
<td>Yes</td>
<td>No</td>
<td><strong>No</strong></td>
</tr>
<tr>
<td>RL training loop</td>
<td>Yes</td>
<td>Yes</td>
<td><strong>No</strong></td>
</tr>
<tr>
<td>Models in memory</td>
<td>4</td>
<td>3</td>
<td>2</td>
</tr>
</tbody>
</table>
<h3 id="35-dpos-relationship-to-policy-gradient">3.5 DPO's Relationship to Policy Gradient<a class="headerlink" href="#35-dpos-relationship-to-policy-gradient" title="Permanent link">&para;</a></h3>
<p>Despite looking like supervised learning, DPO is implicitly performing policy optimization. The gradient of the DPO loss is:</p>
<div class="arithmatex">\[\nabla_\theta L^{\text{DPO}} = -\beta \, \mathbb{E} \Big[ \underbrace{\sigma(\hat{r}_l - \hat{r}_w)}_{\text{weight}} \Big( \nabla_\theta \log \pi_\theta(o_w \mid q) - \nabla_\theta \log \pi_\theta(o_l \mid q) \Big) \Big]\]</div>
<p>where <span class="arithmatex">\(\hat{r}_i = \beta \log \frac{\pi_\theta(o_i \mid q)}{\pi_{\text{ref}}(o_i \mid q)}\)</span> is the implicit reward.</p>
<p>The structure is recognizable: <strong>score functions</strong> <span class="arithmatex">\(\nabla_\theta \log \pi_\theta\)</span> weighted by an advantage-like term. DPO increases the probability of the preferred response and decreases the probability of the dispreferred one, with a weight that is larger when the model currently disagrees with the preference label.</p>
<h3 id="36-limitations-of-dpo">3.6 Limitations of DPO<a class="headerlink" href="#36-limitations-of-dpo" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Offline only</strong>: DPO trains on a fixed dataset of preferences. It cannot explore — if the preference data doesn't cover a behavior, DPO cannot learn it.</li>
<li><strong>Distribution shift</strong>: as <span class="arithmatex">\(\pi_\theta\)</span> moves away from the policy that generated the preference data, the implicit reward becomes unreliable.</li>
<li><strong>No iterative improvement</strong>: PPO and GRPO can generate new data and improve iteratively. DPO is a single-pass optimization.</li>
</ul>
<p>These limitations have led to <strong>online DPO variants</strong> (OAIF, online DPO) that alternate between generating new responses and optimizing preferences — effectively re-introducing the RL loop that DPO was designed to eliminate.</p>
<hr />
<h2 id="4-other-notable-variants">4. Other Notable Variants<a class="headerlink" href="#4-other-notable-variants" title="Permanent link">&para;</a></h2>
<h3 id="41-remax-reinforce-style-baseline-for-llms">4.1 ReMax: Reinforce-Style Baseline for LLMs<a class="headerlink" href="#41-remax-reinforce-style-baseline-for-llms" title="Permanent link">&para;</a></h3>
<p><strong>ReMax</strong> (Li et al., 2023) takes a different approach to eliminating the value function. Instead of group normalization (GRPO), it uses a <strong>REINFORCE-style baseline</strong>:</p>
<div class="arithmatex">\[\hat{A}(o) = r(o) - r(\bar{o})\]</div>
<p>where <span class="arithmatex">\(\bar{o}\)</span> is the <strong>greedy response</strong> (generated by argmax decoding from the current policy). This requires only one extra forward pass (no sampling a group), making it cheaper than GRPO.</p>
<h3 id="42-rloo-reinforce-leave-one-out">4.2 RLOO: REINFORCE Leave-One-Out<a class="headerlink" href="#42-rloo-reinforce-leave-one-out" title="Permanent link">&para;</a></h3>
<p><strong>RLOO</strong> (Ahmadian et al., 2024) uses a leave-one-out baseline: for each response <span class="arithmatex">\(o_i\)</span> in a group, the baseline is the mean reward of the <em>other</em> responses:</p>
<div class="arithmatex">\[\hat{A}_i = r_i - \frac{1}{G-1} \sum_{j \neq i} r_j\]</div>
<p>This is an unbiased estimator with lower variance than a single-sample baseline (ReMax) but doesn't require a learned value function (like PPO). It sits between ReMax and GRPO in the bias-variance tradeoff.</p>
<h3 id="43-rpo-relative-policy-optimization">4.3 RPO: Relative Policy Optimization<a class="headerlink" href="#43-rpo-relative-policy-optimization" title="Permanent link">&para;</a></h3>
<p><strong>RPO</strong> modifies the PPO objective to include a <strong>contrastive term</strong> that directly compares preferred and dispreferred responses within the clipped objective. It can be seen as a hybrid of PPO's ratio-based update and DPO's preference-based signal.</p>
<h3 id="44-kto-kahneman-tversky-optimization">4.4 KTO: Kahneman-Tversky Optimization<a class="headerlink" href="#44-kto-kahneman-tversky-optimization" title="Permanent link">&para;</a></h3>
<p><strong>KTO</strong> (Ethayarajh et al., 2024) is inspired by prospect theory. Instead of requiring paired preferences <span class="arithmatex">\((o_w, o_l)\)</span>, KTO works with <strong>unpaired</strong> binary feedback (good/bad labels on individual responses). The loss function is asymmetric, reflecting the psychological finding that humans weight losses more heavily than gains:</p>
<div class="arithmatex">\[L^{\text{KTO}}(\theta) = \mathbb{E}_{o \sim \mathcal{D}} \Big[ w(o) \cdot \big(1 - \sigma(r_\theta(o) - z_{\text{ref}})\big) \Big]\]</div>
<p>where <span class="arithmatex">\(w(o)\)</span> is a label-dependent weight (higher for "bad" examples) and <span class="arithmatex">\(z_{\text{ref}}\)</span> is a reference point.</p>
<h3 id="45-rlhf-era-ppo-tweaks">4.5 RLHF-Era PPO Tweaks<a class="headerlink" href="#45-rlhf-era-ppo-tweaks" title="Permanent link">&para;</a></h3>
<p>Several practical modifications to PPO have emerged from the RLHF community:</p>
<ul>
<li><strong>Reward whitening</strong>: normalize rewards across the batch (similar in spirit to GRPO's group normalization, but applied to PPO)</li>
<li><strong>Reward clipping</strong>: cap extreme reward model outputs to prevent ratio explosion</li>
<li><strong>KL penalty scheduling</strong>: start with a high <span class="arithmatex">\(\beta\)</span> (stay close to reference) and anneal down</li>
<li><strong>Response-level vs. token-level KL</strong>: penalizing KL at the response level is more stable than per-token KL for long sequences</li>
<li><strong>Separate reference model</strong>: keep <span class="arithmatex">\(\pi_{\text{ref}}\)</span> frozen throughout training (don't update it periodically)</li>
</ul>
<hr />
<h2 id="5-the-landscape-a-comparative-view">5. The Landscape: A Comparative View<a class="headerlink" href="#5-the-landscape-a-comparative-view" title="Permanent link">&para;</a></h2>
<h3 id="51-what-each-method-optimizes">5.1 What Each Method Optimizes<a class="headerlink" href="#51-what-each-method-optimizes" title="Permanent link">&para;</a></h3>
<p>All these methods ultimately optimize the same underlying objective — maximize reward while staying close to a reference policy — but they differ in <em>how</em> they decompose the problem:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Reward Signal</th>
<th>Baseline / Advantage</th>
<th>Trust Region</th>
<th>Training Loop</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PPO</strong></td>
<td>Reward model</td>
<td>Learned <span class="arithmatex">\(V_\psi\)</span> (GAE)</td>
<td>Clipping</td>
<td>Online RL</td>
</tr>
<tr>
<td><strong>GRPO</strong></td>
<td>Reward model</td>
<td>Group mean (z-score)</td>
<td>Clipping + KL</td>
<td>Online RL</td>
</tr>
<tr>
<td><strong>ReMax</strong></td>
<td>Reward model</td>
<td>Greedy response</td>
<td>Clipping</td>
<td>Online RL</td>
</tr>
<tr>
<td><strong>RLOO</strong></td>
<td>Reward model</td>
<td>Leave-one-out mean</td>
<td>REINFORCE-style</td>
<td>Online RL</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>Implicit (from preferences)</td>
<td>Implicit (in loss)</td>
<td>KL (implicit)</td>
<td>Supervised</td>
</tr>
<tr>
<td><strong>KTO</strong></td>
<td>Binary feedback</td>
<td>Reference point</td>
<td>KL (implicit)</td>
<td>Supervised</td>
</tr>
</tbody>
</table>
<h3 id="52-the-fundamental-tradeoff">5.2 The Fundamental Tradeoff<a class="headerlink" href="#52-the-fundamental-tradeoff" title="Permanent link">&para;</a></h3>
<p>There is a spectrum from <strong>more RL</strong> to <strong>more supervised</strong>:</p>
<div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>PPO ← GRPO ← RLOO ← ReMax ← Online DPO ← DPO ← KTO
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a> │                                                    │
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a> │  More exploration, more compute, more stable        │
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a> │  Better for iterative improvement                   │
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a> │                                                    │
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a> │                    Less compute, simpler pipeline   │
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a> │                    But offline, no exploration      │
</span></code></pre></div>
<p>Methods on the left (PPO, GRPO) can <strong>explore</strong> — they generate new responses, evaluate them, and improve. Methods on the right (DPO, KTO) are <strong>offline</strong> — they optimize a fixed dataset.</p>
<p>For frontier LLM training (GPT-4, Claude, Gemini, DeepSeek), the trend is toward <strong>online methods with cheap baselines</strong> (GRPO, RLOO) — they retain the exploration benefits of RL while eliminating the value function bottleneck.</p>
<hr />
<h2 id="6-where-ppos-ratio-still-hides-the-gradient">6. Where PPO's Ratio Still Hides the Gradient<a class="headerlink" href="#6-where-ppos-ratio-still-hides-the-gradient" title="Permanent link">&para;</a></h2>
<p>A unifying observation across all these variants (except pure DPO/KTO): the <strong>probability ratio</strong> <span class="arithmatex">\(r_t(\theta) = \pi_\theta / \pi_{\theta_k}\)</span> remains the core mechanism. As shown in <a href="03a_pg_in_ppo.md">Chapter 03a</a>, differentiating this ratio recovers the score function:</p>
<div class="arithmatex">\[\nabla_\theta \, r_t(\theta) = r_t(\theta) \; \nabla_\theta \log \pi_\theta(a_t \mid s_t)\]</div>
<p>GRPO, ReMax, and RLOO all use this same ratio — they differ only in how they estimate the advantage <span class="arithmatex">\(\hat{A}_t\)</span>. The gradient machinery is identical to PPO's.</p>
<p>DPO is the exception: it replaces the ratio with a <strong>log-ratio difference</strong> between preferred and dispreferred responses. But even there, the gradient contains <span class="arithmatex">\(\nabla_\theta \log \pi_\theta\)</span> — the same score function that drives all policy gradient methods.</p>
<p><strong>The score function <span class="arithmatex">\(\nabla_\theta \log \pi_\theta\)</span> is the universal engine.</strong> What varies across methods is the signal (advantage, preference, binary feedback) that multiplies it.</p>
<hr />
<h2 id="7-open-questions-and-future-directions">7. Open Questions and Future Directions<a class="headerlink" href="#7-open-questions-and-future-directions" title="Permanent link">&para;</a></h2>
<h3 id="71-beyond-scalar-rewards">7.1 Beyond Scalar Rewards<a class="headerlink" href="#71-beyond-scalar-rewards" title="Permanent link">&para;</a></h3>
<p>All current methods assume a scalar reward. But human preferences are multidimensional (helpfulness, harmlessness, honesty, style, ...). <strong>Multi-objective RLHF</strong> is an active research area — how to balance competing objectives without collapsing them into a single number.</p>
<h3 id="72-process-supervision-vs-outcome-supervision">7.2 Process Supervision vs. Outcome Supervision<a class="headerlink" href="#72-process-supervision-vs-outcome-supervision" title="Permanent link">&para;</a></h3>
<p>PPO with GAE provides <strong>token-level</strong> (process) supervision. GRPO provides <strong>response-level</strong> (outcome) supervision. Which is better? Process supervision gives denser signal but requires a harder-to-train value function. Outcome supervision is simpler but coarser. The optimal granularity likely depends on the task.</p>
<h3 id="73-scaling-laws-for-rl-fine-tuning">7.3 Scaling Laws for RL Fine-Tuning<a class="headerlink" href="#73-scaling-laws-for-rl-fine-tuning" title="Permanent link">&para;</a></h3>
<p>We have scaling laws for pre-training (Chinchilla, etc.) but not for RL fine-tuning. How does the optimal group size <span class="arithmatex">\(G\)</span> in GRPO scale with model size? How many RL steps are needed? These questions are largely unanswered.</p>
<h3 id="74-connection-to-control-and-robotics">7.4 Connection to Control and Robotics<a class="headerlink" href="#74-connection-to-control-and-robotics" title="Permanent link">&para;</a></h3>
<p>The RLHF variants (GRPO, DPO, etc.) were developed for LLMs, but their ideas apply to any policy gradient setting. Group-normalized advantages could replace GAE in robotics. DPO-style preference learning could train robot policies from human demonstrations without reward engineering. This cross-pollination is just beginning.</p>
<hr />
<h2 id="8-summary">8. Summary<a class="headerlink" href="#8-summary" title="Permanent link">&para;</a></h2>
<h3 id="the-evolution">The evolution<a class="headerlink" href="#the-evolution" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Era</th>
<th>Method</th>
<th>Key Innovation</th>
</tr>
</thead>
<tbody>
<tr>
<td>2017</td>
<td><strong>PPO</strong></td>
<td>Clipped ratio for stable updates</td>
</tr>
<tr>
<td>2023</td>
<td><strong>DPO</strong></td>
<td>Eliminate reward model; train from preferences directly</td>
</tr>
<tr>
<td>2024</td>
<td><strong>GRPO</strong></td>
<td>Eliminate value function; group-normalized advantages</td>
</tr>
<tr>
<td>2024</td>
<td><strong>KTO</strong></td>
<td>Work with unpaired binary feedback</td>
</tr>
<tr>
<td>2024+</td>
<td><strong>Online DPO, RLOO</strong></td>
<td>Hybrid: online exploration + cheap baselines</td>
</tr>
</tbody>
</table>
<h3 id="the-pattern">The pattern<a class="headerlink" href="#the-pattern" title="Permanent link">&para;</a></h3>
<p>Each successor removes a component that was hard to train or expensive to maintain:</p>
<ul>
<li>PPO → GRPO: remove the value function</li>
<li>PPO → DPO: remove the reward model <em>and</em> the value function <em>and</em> the RL loop</li>
<li>DPO → Online DPO: add back exploration (because offline-only is limiting)</li>
</ul>
<p>The field is converging toward methods that are <strong>online</strong> (can explore and improve iteratively) but <strong>critic-free</strong> (don't need a learned value function). GRPO is currently the best representative of this sweet spot.</p>
<h3 id="what-hasnt-changed">What hasn't changed<a class="headerlink" href="#what-hasnt-changed" title="Permanent link">&para;</a></h3>
<p>Across all variants, the fundamental mechanism is the same: <strong>compute a score function</strong> <span class="arithmatex">\(\nabla_\theta \log \pi_\theta\)</span>, <strong>multiply by an advantage signal</strong>, and <strong>constrain the update</strong> to prevent catastrophic policy changes. The score function is the invariant; everything else is engineering.</p>
<hr />
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>[Schulman et al., 2017]</strong> — <em>Proximal Policy Optimization Algorithms.</em> <a href="https://arxiv.org/abs/1707.06347">arXiv:1707.06347</a> (PPO)</li>
<li><strong>[Rafailov et al., 2023]</strong> — <em>Direct Preference Optimization: Your Language Model Is Secretly a Reward Model.</em> NeurIPS 2023. <a href="https://arxiv.org/abs/2305.18290">arXiv:2305.18290</a> (DPO)</li>
<li><strong>[Shao et al., 2024]</strong> — <em>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.</em> <a href="https://arxiv.org/abs/2402.03300">arXiv:2402.03300</a> (GRPO)</li>
<li><strong>[Ahmadian et al., 2024]</strong> — <em>Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs.</em> <a href="https://arxiv.org/abs/2402.14740">arXiv:2402.14740</a> (RLOO)</li>
<li><strong>[Li et al., 2023]</strong> — <em>ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models.</em> <a href="https://arxiv.org/abs/2310.10505">arXiv:2310.10505</a> (ReMax)</li>
<li><strong>[Ethayarajh et al., 2024]</strong> — <em>KTO: Model Alignment as Prospect Theoretic Optimization.</em> <a href="https://arxiv.org/abs/2402.01306">arXiv:2402.01306</a> (KTO)</li>
<li><strong>[Zheng et al., 2023]</strong> — <em>Secrets of RLHF in Large Language Models Part I: PPO.</em> <a href="https://arxiv.org/abs/2307.04964">arXiv:2307.04964</a></li>
</ul>
<hr />
<p><em>Previous: <a href="../03_PPO/">Proximal Policy Optimization (PPO)</a></em> | <em>Next: <a href="../05_actions_as_operators/">Actions as Operators</a></em></p>
<p><em>Supplements: <a href="../04a_PPO_vs_GRPO_in_LLM/">PPO vs GRPO in LLMs</a> | <a href="../04b_GRPO_QA/">GRPO Without a Critic</a></em></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2026 GRL Research Team
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/pleiadian53/GRL" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>