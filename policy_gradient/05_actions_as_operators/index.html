
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Physics-grounded reinforcement learning with particle-based belief representations">
      
      
        <meta name="author" content="GRL Research Team">
      
      
        <link rel="canonical" href="https://pleiadian53.github.io/GRL/policy_gradient/05_actions_as_operators/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>Actions as Operators: From Flat Vectors to Structured Control - Generalized Reinforcement Learning (GRL)</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#actions-as-operators-from-flat-vectors-to-structured-control" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Generalized Reinforcement Learning (GRL)" class="md-header__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Generalized Reinforcement Learning (GRL)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Actions as Operators: From Flat Vectors to Structured Control
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../GRL0/" class="md-tabs__link">
          
  
  
  GRL v0 (Tutorial Paper)

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../ROADMAP/" class="md-tabs__link">
        
  
  
    
  
  Research Roadmap

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../notebooks/" class="md-tabs__link">
          
  
  
  Notebooks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../CONTRIBUTING/" class="md-tabs__link">
          
  
  
  About

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Generalized Reinforcement Learning (GRL)" class="md-nav__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Generalized Reinforcement Learning (GRL)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    GRL v0 (Tutorial Paper)
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    GRL v0 (Tutorial Paper)
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Part I: Tutorials
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part I: Tutorials
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tutorial Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/00-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 0: Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/01-core-concepts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 1: Core Concepts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/02-rkhs-foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 2: RKHS Foundations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/03-energy-and-fitness/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3: Energy and Fitness
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/03a-least-action-principle/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3a: Least Action Principle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/04-reinforcement-field/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4: Reinforcement Field
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/04a-riesz-representer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4a: Riesz Representer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/05-particle-memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 5: Particle Memory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/06-memory-update/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6: MemoryUpdate
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/06a-advanced-memory-dynamics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6a: Advanced Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/07-rf-sarsa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7: RF-SARSA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/07a-continuous-policy-inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7a: Continuous Policy Inference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Quantum-Inspired Extensions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Quantum-Inspired Extensions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/01-rkhs-quantum-parallel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: RKHS-QM Parallel
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/01a-wavefunction-interpretation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Wavefunction Interpretation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Basis and Amplitudes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/03-complex-rkhs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Complex RKHS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/04-action-and-state-fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    04: Action and State Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/05-concept-projections-and-measurements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    05: Concept Projections
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/06-agent-state-and-belief-evolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    06: Agent State and Belief
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/07-learning-the-field-beyond-gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    07: Learning Beyond GP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    08: Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/09-path-integrals-and-action-principles/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    09: Path Integrals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/implementation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Implementation Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/recovering_classical_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Recovering Classical RL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Research Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Notebooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Notebooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Field Series
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Field Series
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Series Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/00_intro_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    00: Introduction to Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/01_classical_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: Classical Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/01a_vector_fields_and_odes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Vector Fields and ODEs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/02_functional_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Functional Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2_7" >
        
          
          <label class="md-nav__link" for="__nav_4_2_7" id="__nav_4_2_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reinforcement Fields
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reinforcement Fields
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Reinforcement Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03a: Particle Coverage Effects
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Theory: Particle vs Gradient Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    About
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    About
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CONTRIBUTING/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Contributing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LICENSE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    License
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-the-gap-in-standard-policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. The Gap in Standard Policy Gradient
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-whats-missing-three-limitations-of-flat-actions" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. What's Missing: Three Limitations of Flat Actions
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="2. What&#39;s Missing: Three Limitations of Flat Actions">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-no-notion-of-similarity" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 No Notion of Similarity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-no-compositionality" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 No Compositionality
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23-no-physical-semantics" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3 No Physical Semantics
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-actions-as-operators-the-key-idea" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. Actions as Operators: The Key Idea
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. Actions as Operators: The Key Idea">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-concrete-examples" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 Concrete Examples
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-the-operator-hierarchy" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 The Operator Hierarchy
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-how-this-changes-policy-gradient" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. How This Changes Policy Gradient
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. How This Changes Policy Gradient">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-policy-over-operator-parameters" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 Policy Over Operator Parameters
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-the-value-function-over-augmented-space" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 The Value Function Over Augmented Space
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-two-approaches-to-optimization" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 Two Approaches to Optimization
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#44-can-we-combine-both" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.4 Can We Combine Both?
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-from-structured-actions-to-structured-policies" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. From Structured Actions to Structured Policies
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="5. From Structured Actions to Structured Policies">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-factorized-operators" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 Factorized Operators
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#52-hierarchical-operators" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.2 Hierarchical Operators
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-the-spectrum-of-action-representations" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. The Spectrum of Action Representations
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-practical-implications" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Practical Implications
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="7. Practical Implications">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#71-when-to-use-structured-actions" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.1 When to Use Structured Actions
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#72-the-design-choice" class="md-nav__link">
    <span class="md-ellipsis">
      
        7.2 The Design Choice
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#8-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        8. Summary
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="8. Summary">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-this-chapter-adds-to-the-series" class="md-nav__link">
    <span class="md-ellipsis">
      
        What this chapter adds to the series
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-key-insight" class="md-nav__link">
    <span class="md-ellipsis">
      
        The key insight
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-series-so-far" class="md-nav__link">
    <span class="md-ellipsis">
      
        The series so far
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/pleiadian53/GRL/edit/main/docs/policy_gradient/05_actions_as_operators.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg>
    </a>
  
  


<h1 id="actions-as-operators-from-flat-vectors-to-structured-control">Actions as Operators: From Flat Vectors to Structured Control<a class="headerlink" href="#actions-as-operators-from-flat-vectors-to-structured-control" title="Permanent link">&para;</a></h1>
<p><strong>Why parameterizing the distribution over actions is not enough — and what happens when we parameterize the actions themselves</strong></p>
<blockquote>
<p><em>Prerequisites: <a href="../01a_parameterizing_policy/">Parameterizing the Policy</a>, <a href="../04_PPO_variants/">PPO Variants</a></em></p>
<p><em>Bridge to: <a href="../../GRL0/tutorials/01-core-concepts/">GRL Core Concepts</a>, <a href="../../GRL0/tutorials/04-reinforcement-field/">Reinforcement Field</a></em></p>
</blockquote>
<hr />
<h2 id="1-the-gap-in-standard-policy-gradient">1. The Gap in Standard Policy Gradient<a class="headerlink" href="#1-the-gap-in-standard-policy-gradient" title="Permanent link">&para;</a></h2>
<p>Chapters 01–04 of this series developed the full policy gradient toolkit: REINFORCE, baselines, GAE, TRPO, PPO, GRPO, DPO. All of these methods optimize a parameterized policy:</p>
<div class="arithmatex">\[\pi_\theta(a \mid s)\]</div>
<p>In <a href="../01a_parameterizing_policy/">Chapter 01a</a>, we saw how this distribution can be implemented — Gaussian, mixture, normalizing flow, trajectory-level. But notice what all these parameterizations have in common: <strong>the action <span class="arithmatex">\(a\)</span> is always a flat vector</strong>.</p>
<ul>
<li>A Gaussian policy outputs <span class="arithmatex">\(a \in \mathbb{R}^d\)</span> — a vector of torques, forces, or velocities</li>
<li>A categorical policy outputs <span class="arithmatex">\(a \in \{1, \ldots, K\}\)</span> — an index</li>
<li>Even a trajectory-level policy outputs <span class="arithmatex">\(\tau \in \mathbb{R}^{H \times 2}\)</span> — a sequence of waypoints, flattened</li>
</ul>
<p>The policy gradient machinery treats <span class="arithmatex">\(a\)</span> as an opaque object. It only needs <span class="arithmatex">\(\log \pi_\theta(a \mid s)\)</span> and its gradient. It never asks <em>what <span class="arithmatex">\(a\)</span> means</em> — what physical effect it produces, how it composes with other actions, or what structure it carries.</p>
<p>This is both a strength (generality) and a limitation (missed structure).</p>
<hr />
<h2 id="2-whats-missing-three-limitations-of-flat-actions">2. What's Missing: Three Limitations of Flat Actions<a class="headerlink" href="#2-whats-missing-three-limitations-of-flat-actions" title="Permanent link">&para;</a></h2>
<h3 id="21-no-notion-of-similarity">2.1 No Notion of Similarity<a class="headerlink" href="#21-no-notion-of-similarity" title="Permanent link">&para;</a></h3>
<p>In a standard continuous policy, actions <span class="arithmatex">\(a\)</span> and <span class="arithmatex">\(a'\)</span> are "similar" only if they are close in Euclidean distance: <span class="arithmatex">\(\|a - a'\|\)</span> is small. But Euclidean distance in action space often has no physical meaning.</p>
<p><strong>Example — robotic arm:</strong></p>
<ul>
<li>Action <span class="arithmatex">\(a_1 = (0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\)</span>: apply torque only to shoulder</li>
<li>Action <span class="arithmatex">\(a_2 = (0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.5)\)</span>: apply torque only to wrist</li>
</ul>
<p>These are equidistant in <span class="arithmatex">\(\mathbb{R}^7\)</span> from the zero action, but they produce completely different physical effects. The policy gradient treats them as equally "far" from each other — it has no notion of functional similarity.</p>
<h3 id="22-no-compositionality">2.2 No Compositionality<a class="headerlink" href="#22-no-compositionality" title="Permanent link">&para;</a></h3>
<p>Real-world actions are often <strong>composed</strong> from simpler primitives:</p>
<ul>
<li>"Reach for the cup" = move arm to position + open gripper</li>
<li>"Overtake" = accelerate + steer left + steer right + decelerate</li>
<li>"Walk" = cyclic composition of stance, swing, push-off phases</li>
</ul>
<p>A flat action vector <span class="arithmatex">\(a \in \mathbb{R}^d\)</span> cannot represent this compositional structure. Every action is independent — there is no way to say "this action is a combination of these two simpler actions."</p>
<h3 id="23-no-physical-semantics">2.3 No Physical Semantics<a class="headerlink" href="#23-no-physical-semantics" title="Permanent link">&para;</a></h3>
<p>A torque vector <span class="arithmatex">\(\tau \in \mathbb{R}^7\)</span> is not just a number — it is a <strong>physical operator</strong> that acts on the robot's state through the equations of motion:</p>
<div class="arithmatex">\[M(q)\ddot{q} + C(q, \dot{q})\dot{q} + g(q) = \tau\]</div>
<p>The action <span class="arithmatex">\(\tau\)</span> has meaning only in the context of the dynamics. A policy that outputs <span class="arithmatex">\(\tau\)</span> without knowing the dynamics is flying blind — it must learn the entire mapping from torques to outcomes through trial and error.</p>
<hr />
<h2 id="3-actions-as-operators-the-key-idea">3. Actions as Operators: The Key Idea<a class="headerlink" href="#3-actions-as-operators-the-key-idea" title="Permanent link">&para;</a></h2>
<p>What if, instead of treating actions as flat vectors, we treat them as <strong>parameterized operators</strong> that act on the environment state?</p>
<div class="arithmatex">\[\theta_a \;\longrightarrow\; \hat{O}(\theta_a)\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\theta_a \in \Theta\)</span> is the <strong>action parameter vector</strong> (the "knobs" the agent controls)</li>
<li><span class="arithmatex">\(\hat{O}(\theta_a)\)</span> is the <strong>operator</strong> specified by those parameters (the physical effect)</li>
</ul>
<p>The distinction is subtle but important:</p>
<table>
<thead>
<tr>
<th>Flat action</th>
<th>Action as operator</th>
</tr>
</thead>
<tbody>
<tr>
<td><span class="arithmatex">\(a \in \mathbb{R}^d\)</span></td>
<td><span class="arithmatex">\(\theta_a \in \Theta \to \hat{O}(\theta_a)\)</span></td>
</tr>
<tr>
<td>Opaque vector</td>
<td>Parameterized physical effect</td>
</tr>
<tr>
<td>Similarity = Euclidean distance</td>
<td>Similarity = effect similarity</td>
</tr>
<tr>
<td>No internal structure</td>
<td>Composable, interpretable</td>
</tr>
<tr>
<td>Policy learns mapping blindly</td>
<td>Structure aids generalization</td>
</tr>
</tbody>
</table>
<h3 id="31-concrete-examples">3.1 Concrete Examples<a class="headerlink" href="#31-concrete-examples" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Domain</th>
<th>Parameters <span class="arithmatex">\(\theta_a\)</span></th>
<th>Operator <span class="arithmatex">\(\hat{O}(\theta_a)\)</span></th>
<th>What structure buys you</th>
</tr>
</thead>
<tbody>
<tr>
<td>2D navigation</td>
<td><span class="arithmatex">\((F, \alpha)\)</span></td>
<td>Force of magnitude <span class="arithmatex">\(F\)</span> at angle <span class="arithmatex">\(\alpha\)</span></td>
<td>Nearby angles → similar trajectories</td>
</tr>
<tr>
<td>Robotic arm</td>
<td><span class="arithmatex">\((\mathbf{p}_{\text{target}}, \text{stiffness})\)</span></td>
<td>Impedance controller to target pose</td>
<td>Smooth interpolation between targets</td>
</tr>
<tr>
<td>Autonomous driving</td>
<td><span class="arithmatex">\((v_{\text{ref}}, \kappa)\)</span></td>
<td>Track reference velocity <span class="arithmatex">\(v_{\text{ref}}\)</span> along curvature <span class="arithmatex">\(\kappa\)</span></td>
<td>Curvature space is geometrically meaningful</td>
</tr>
<tr>
<td>Manipulation</td>
<td><span class="arithmatex">\((\text{grasp\_type}, \mathbf{p}, \phi)\)</span></td>
<td>Grasp object at position <span class="arithmatex">\(\mathbf{p}\)</span> with orientation <span class="arithmatex">\(\phi\)</span></td>
<td>Discrete grasp type + continuous placement</td>
</tr>
</tbody>
</table>
<p>In each case, the parameters <span class="arithmatex">\(\theta_a\)</span> have <strong>physical meaning</strong>, and the operator <span class="arithmatex">\(\hat{O}(\theta_a)\)</span> specifies a well-defined physical effect. Nearby parameters produce similar effects — not because of Euclidean distance, but because of the operator's structure.</p>
<h3 id="32-the-operator-hierarchy">3.2 The Operator Hierarchy<a class="headerlink" href="#32-the-operator-hierarchy" title="Permanent link">&para;</a></h3>
<p>Actions can be organized into a hierarchy of abstraction:</p>
<p><strong>Level 0 — Raw actuator commands:</strong></p>
<div class="arithmatex">\[a = \tau \in \mathbb{R}^d \quad \text{(joint torques, motor voltages)}\]</div>
<p>This is what standard policy gradient operates on. No structure, no semantics.</p>
<p><strong>Level 1 — Parameterized controllers:</strong></p>
<div class="arithmatex">\[\theta_a = (\mathbf{p}_{\text{target}}, K_p, K_d) \quad \longrightarrow \quad \hat{O}(\theta_a) = K_p(\mathbf{p}_{\text{target}} - \mathbf{p}) + K_d(\dot{\mathbf{p}}_{\text{target}} - \dot{\mathbf{p}})\]</div>
<p>The agent chooses <em>where to go</em> and <em>how stiffly</em>, not individual torques. The PD controller handles the low-level execution.</p>
<p><strong>Level 2 — Parameterized skills:</strong></p>
<div class="arithmatex">\[\theta_a = (\text{skill\_id}, \text{duration}, \text{target}) \quad \longrightarrow \quad \hat{O}(\theta_a) = \text{execute skill for duration toward target}\]</div>
<p>The agent composes pre-learned skills (reach, grasp, place) with continuous parameters (where, how long).</p>
<p><strong>Level 3 — Parameterized plans:</strong></p>
<div class="arithmatex">\[\theta_a = (\text{waypoints}, \text{timing}, \text{constraints}) \quad \longrightarrow \quad \hat{O}(\theta_a) = \text{trajectory satisfying constraints}\]</div>
<p>The agent specifies a full motion plan. The operator is a trajectory optimizer or motion planner.</p>
<p>Each level adds structure and reduces the dimensionality of what the policy must learn, at the cost of reduced flexibility.</p>
<hr />
<h2 id="4-how-this-changes-policy-gradient">4. How This Changes Policy Gradient<a class="headerlink" href="#4-how-this-changes-policy-gradient" title="Permanent link">&para;</a></h2>
<h3 id="41-policy-over-operator-parameters">4.1 Policy Over Operator Parameters<a class="headerlink" href="#41-policy-over-operator-parameters" title="Permanent link">&para;</a></h3>
<p>Instead of <span class="arithmatex">\(\pi_\theta(a \mid s)\)</span>, we write:</p>
<div class="arithmatex">\[\pi_\theta(\theta_a \mid s)\]</div>
<p>The policy outputs a distribution over <strong>operator parameters</strong>, not raw actions. Everything from Chapters 01–04 still applies — we still need <span class="arithmatex">\(\log \pi_\theta(\theta_a \mid s)\)</span>, we still compute the ratio <span class="arithmatex">\(r_t(\theta)\)</span>, we still clip.</p>
<p>But the gradient now flows through a more structured space. If the operator parameters have physical meaning, the policy gradient "knows" that changing the target position slightly will produce a slightly different trajectory — because the operator enforces this smoothness.</p>
<h3 id="42-the-value-function-over-augmented-space">4.2 The Value Function Over Augmented Space<a class="headerlink" href="#42-the-value-function-over-augmented-space" title="Permanent link">&para;</a></h3>
<p>Here is where the connection to GRL becomes direct.</p>
<p>In standard RL, the value function is <span class="arithmatex">\(Q(s, a)\)</span> — value of taking flat action <span class="arithmatex">\(a\)</span> in state <span class="arithmatex">\(s\)</span>.</p>
<p>If actions are parameterized operators, we can define:</p>
<div class="arithmatex">\[Q^+(s, \theta_a) \quad \text{— value of executing operator } \hat{O}(\theta_a) \text{ in state } s\]</div>
<p>This is a function over the <strong>augmented space</strong> <span class="arithmatex">\(\mathcal{Z} = \mathcal{S} \times \Theta\)</span>, where states and action parameters live together. This is exactly the construction in GRL (<a href="../../GRL0/tutorials/01-core-concepts/">Chapter 01: Core Concepts</a>):</p>
<div class="arithmatex">\[z = (s, \theta_a) \in \mathcal{Z}\]</div>
<p>The value function <span class="arithmatex">\(Q^+(z)\)</span> is smooth over this joint space, enabling:</p>
<ul>
<li><strong>Generalization across actions</strong>: similar operator parameters → similar values</li>
<li><strong>Gradient-based action selection</strong>: <span class="arithmatex">\(\theta_a^* = \arg\max_{\theta_a} Q^+(s, \theta_a)\)</span> via gradient ascent</li>
<li><strong>Kernel-based similarity</strong>: the kernel <span class="arithmatex">\(k(z, z')\)</span> defines a meaningful notion of similarity between state-action pairs</li>
</ul>
<h3 id="43-two-approaches-to-optimization">4.3 Two Approaches to Optimization<a class="headerlink" href="#43-two-approaches-to-optimization" title="Permanent link">&para;</a></h3>
<p>With actions as operators, there are two complementary ways to find good actions:</p>
<p><strong>Approach A — Policy gradient (this series):</strong></p>
<p>Learn an explicit policy <span class="arithmatex">\(\pi_\theta(\theta_a \mid s)\)</span> and optimize it via PPO/GRPO:</p>
<div class="arithmatex">\[\nabla_\theta J = \mathbb{E}\left[ \nabla_\theta \log \pi_\theta(\theta_a \mid s) \; \hat{A}_t \right]\]</div>
<p>The policy is a neural network that maps states to distributions over operator parameters. Standard PG machinery applies unchanged.</p>
<p><strong>Approach B — Field navigation (GRL):</strong></p>
<p>Learn the value landscape <span class="arithmatex">\(Q^+(s, \theta_a)\)</span> and derive the policy implicitly:</p>
<div class="arithmatex">\[\theta_a^* = \arg\max_{\theta_a} Q^+(s, \theta_a) \quad \text{or} \quad \theta_a \sim \exp\!\left(\frac{Q^+(s, \theta_a)}{\lambda}\right)\]</div>
<p>No explicit policy network. The "policy" is the act of navigating the reinforcement field — following the gradient <span class="arithmatex">\(\nabla_{\theta_a} Q^+(s, \theta_a)\)</span> toward high-value regions.</p>
<p>This is the central insight of GRL (<a href="../../GRL0/tutorials/04-reinforcement-field/">Chapter 04: Reinforcement Field</a>):</p>
<blockquote>
<p><strong>The policy is not a function to be learned. It is a trajectory induced by the reinforcement field geometry.</strong></p>
</blockquote>
<h3 id="44-can-we-combine-both">4.4 Can We Combine Both?<a class="headerlink" href="#44-can-we-combine-both" title="Permanent link">&para;</a></h3>
<p>Yes — and this is a natural research direction. Consider an <strong>actor-critic architecture</strong> where:</p>
<ul>
<li><strong>Critic</strong>: the reinforcement field <span class="arithmatex">\(Q^+(s, \theta_a)\)</span>, represented non-parametrically via particles in RKHS (GRL's approach)</li>
<li><strong>Actor</strong>: an explicit policy <span class="arithmatex">\(\pi_\phi(\theta_a \mid s)\)</span>, trained via policy gradient using the field as the critic</li>
</ul>
<p>The actor provides efficient sampling (single forward pass), while the critic provides a smooth, non-parametric value landscape with kernel-induced generalization. The policy gradient update becomes:</p>
<div class="arithmatex">\[\phi \leftarrow \phi + \beta \, \nabla_\phi \log \pi_\phi(\theta_a \mid s) \; \hat{A}_t\]</div>
<p>where the advantage <span class="arithmatex">\(\hat{A}_t\)</span> is computed from the reinforcement field:</p>
<div class="arithmatex">\[\hat{A}_t = Q^+(s_t, \theta_{a,t}) - V(s_t), \quad V(s) = \mathbb{E}_{\theta_a \sim \pi_\phi}[Q^+(s, \theta_a)]\]</div>
<p>This is already sketched in <a href="../../GRL0/tutorials/07a-continuous-policy-inference/">GRL Chapter 07a: Continuous Policy Inference</a> as "Actor-Critic in RKHS."</p>
<hr />
<h2 id="5-from-structured-actions-to-structured-policies">5. From Structured Actions to Structured Policies<a class="headerlink" href="#5-from-structured-actions-to-structured-policies" title="Permanent link">&para;</a></h2>
<h3 id="51-factorized-operators">5.1 Factorized Operators<a class="headerlink" href="#51-factorized-operators" title="Permanent link">&para;</a></h3>
<p>When the operator has internal structure, the policy can mirror that structure:</p>
<div class="arithmatex">\[\hat{O}(\theta_a) = \hat{O}_{\text{move}}(\theta_{\text{move}}) \circ \hat{O}_{\text{grasp}}(\theta_{\text{grasp}})\]</div>
<p>The policy factorizes accordingly:</p>
<div class="arithmatex">\[\pi_\theta(\theta_a \mid s) = \pi_{\text{move}}(\theta_{\text{move}} \mid s) \cdot \pi_{\text{grasp}}(\theta_{\text{grasp}} \mid s, \theta_{\text{move}})\]</div>
<p>This is a <strong>conditional factorization</strong> — the grasp parameters depend on where the arm moved. Policy gradient handles this naturally (log-probs add, gradients flow through the chain).</p>
<h3 id="52-hierarchical-operators">5.2 Hierarchical Operators<a class="headerlink" href="#52-hierarchical-operators" title="Permanent link">&para;</a></h3>
<p>For temporally extended actions (options, skills), the operator includes a <strong>duration</strong> or <strong>termination condition</strong>:</p>
<div class="arithmatex">\[\theta_a = (\text{skill\_id}, \theta_{\text{skill}}, T_{\text{max}})\]</div>
<p>The high-level policy selects which skill to execute and with what parameters. The low-level skill executes for up to <span class="arithmatex">\(T_{\text{max}}\)</span> steps. This is the <strong>options framework</strong> (Sutton et al., 1999) expressed in the language of parameterized operators.</p>
<p>Policy gradient at the high level uses the cumulative reward over the skill's execution as the return signal. GRPO's group-normalized advantages are particularly natural here — sample multiple skill parameterizations for the same state and rank them by outcome.</p>
<hr />
<h2 id="6-the-spectrum-of-action-representations">6. The Spectrum of Action Representations<a class="headerlink" href="#6-the-spectrum-of-action-representations" title="Permanent link">&para;</a></h2>
<p>Pulling together the entire series, we can now see a spectrum of how actions are represented in RL:</p>
<table>
<thead>
<tr>
<th>Representation</th>
<th>Example</th>
<th>Policy gradient</th>
<th>Value function</th>
<th>Generalization</th>
</tr>
</thead>
<tbody>
<tr>
<td>Discrete symbols</td>
<td><span class="arithmatex">\(a \in \{1, \ldots, K\}\)</span></td>
<td>Categorical <span class="arithmatex">\(\pi_\theta\)</span></td>
<td><span class="arithmatex">\(Q(s, a)\)</span> table</td>
<td>None across actions</td>
</tr>
<tr>
<td>Flat continuous</td>
<td><span class="arithmatex">\(a \in \mathbb{R}^d\)</span></td>
<td>Gaussian <span class="arithmatex">\(\pi_\theta\)</span></td>
<td><span class="arithmatex">\(Q(s, a)\)</span> network</td>
<td>Euclidean only</td>
</tr>
<tr>
<td>Parameterized operators</td>
<td><span class="arithmatex">\(\theta_a \to \hat{O}(\theta_a)\)</span></td>
<td><span class="arithmatex">\(\pi_\theta(\theta_a \mid s)\)</span></td>
<td><span class="arithmatex">\(Q^+(s, \theta_a)\)</span> in augmented space</td>
<td>Operator-induced</td>
</tr>
<tr>
<td>Field-navigated operators</td>
<td><span class="arithmatex">\(\theta_a \to \hat{O}(\theta_a)\)</span></td>
<td>Implicit (from field)</td>
<td><span class="arithmatex">\(Q^+(z)\)</span> in RKHS</td>
<td>Kernel-induced</td>
</tr>
</tbody>
</table>
<p>Moving down the table, we gain more structure, more generalization, and more physical meaning — but also more design choices and more assumptions about the domain.</p>
<p>Standard policy gradient (Chapters 01–04) operates in the first two rows. GRL operates in the last two. This chapter is the bridge.</p>
<hr />
<h2 id="7-practical-implications">7. Practical Implications<a class="headerlink" href="#7-practical-implications" title="Permanent link">&para;</a></h2>
<h3 id="71-when-to-use-structured-actions">7.1 When to Use Structured Actions<a class="headerlink" href="#71-when-to-use-structured-actions" title="Permanent link">&para;</a></h3>
<p><strong>Use flat actions when:</strong></p>
<ul>
<li>The action space is low-dimensional (<span class="arithmatex">\(d \leq 10\)</span>)</li>
<li>You have abundant training data (millions of environment steps)</li>
<li>The task doesn't require compositional or hierarchical behavior</li>
<li>You want maximum generality with minimal domain knowledge</li>
</ul>
<p><strong>Use parameterized operators when:</strong></p>
<ul>
<li>The action space is high-dimensional or has known structure</li>
<li>You have domain knowledge about what "good" actions look like (controllers, skills)</li>
<li>The task requires compositional behavior (reach + grasp + place)</li>
<li>Sample efficiency matters (structured actions reduce the search space)</li>
</ul>
<h3 id="72-the-design-choice">7.2 The Design Choice<a class="headerlink" href="#72-the-design-choice" title="Permanent link">&para;</a></h3>
<p>Choosing the operator parameterization is an engineering decision analogous to choosing the neural network architecture. Too little structure (flat actions) wastes data on learning what the domain already knows. Too much structure (rigid skill library) prevents the agent from discovering novel behaviors.</p>
<p>The sweet spot depends on the domain, the data budget, and the desired level of autonomy.</p>
<hr />
<h2 id="8-summary">8. Summary<a class="headerlink" href="#8-summary" title="Permanent link">&para;</a></h2>
<h3 id="what-this-chapter-adds-to-the-series">What this chapter adds to the series<a class="headerlink" href="#what-this-chapter-adds-to-the-series" title="Permanent link">&para;</a></h3>
<p>The policy gradient series (Chapters 01–04) developed the <em>optimization machinery</em> — how to compute gradients, stabilize updates, and scale to large models. This chapter adds the <em>representation question</em> — what the policy is optimizing <em>over</em>.</p>
<h3 id="the-key-insight">The key insight<a class="headerlink" href="#the-key-insight" title="Permanent link">&para;</a></h3>
<p><strong>Standard PG parameterizes the distribution over actions. GRL parameterizes the actions themselves.</strong> These are complementary:</p>
<ul>
<li>Policy gradient provides the optimization algorithm (PPO, GRPO)</li>
<li>Parametric actions provide the representation (operators, augmented space, kernel similarity)</li>
</ul>
<p>Combining them — policy gradient over operator parameters, with a reinforcement field as the critic — is a natural synthesis that inherits the strengths of both.</p>
<h3 id="the-series-so-far">The series so far<a class="headerlink" href="#the-series-so-far" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Chapter</th>
<th>Topic</th>
<th>Key contribution</th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="../01_PG/">01</a></td>
<td>Policy gradient fundamentals</td>
<td>The score function <span class="arithmatex">\(\nabla_\theta \log \pi_\theta\)</span></td>
</tr>
<tr>
<td><a href="../01a_parameterizing_policy/">01a</a></td>
<td>Parameterizing the policy</td>
<td>Concrete distribution families</td>
</tr>
<tr>
<td><a href="../02_TRPO/">02</a></td>
<td>TRPO</td>
<td>KL-constrained trust regions</td>
</tr>
<tr>
<td><a href="../03_PPO/">03</a></td>
<td>PPO</td>
<td>Clipped ratio for stability</td>
</tr>
<tr>
<td><a href="03a_pg_in_ppo.md">03a</a></td>
<td>PG in PPO</td>
<td>Where <span class="arithmatex">\(\nabla_\theta\)</span> hides in the ratio</td>
</tr>
<tr>
<td><a href="../04_PPO_variants/">04</a></td>
<td>PPO variants</td>
<td>GRPO, DPO, critic-free methods</td>
</tr>
<tr>
<td><strong>05</strong></td>
<td><strong>Actions as operators</strong></td>
<td><strong>From flat vectors to structured control; bridge to GRL</strong></td>
</tr>
</tbody>
</table>
<hr />
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>[Masson et al., 2016]</strong> — <em>Reinforcement Learning with Parameterized Actions.</em> AAAI 2016.</li>
<li><strong>[Sutton et al., 1999]</strong> — <em>Between MDPs and Semi-MDPs: A Framework for Temporal Abstraction in Reinforcement Learning.</em> Artificial Intelligence. (Options framework)</li>
<li><strong>[Hausman et al., 2018]</strong> — <em>Learning an Embedding Space for Transferable Robot Skills.</em> ICLR 2018.</li>
<li><strong>[Dalal et al., 2021]</strong> — <em>Accelerating Robotic Reinforcement Learning via Parameterized Action Primitives.</em> CoRL 2021.</li>
<li><strong>GRL Tutorials</strong> — <a href="../../GRL0/tutorials/01-core-concepts/">Core Concepts</a>, <a href="../../GRL0/tutorials/04-reinforcement-field/">Reinforcement Field</a>, <a href="../../GRL0/tutorials/07a-continuous-policy-inference/">Continuous Policy Inference</a></li>
</ul>
<hr />
<p><em>Previous: <a href="../04_PPO_variants/">PPO Variants and Modern Descendants</a></em> | <em>Bridge to: <a href="../../GRL0/tutorials/">GRL Tutorial Series</a></em></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2026 GRL Research Team
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/pleiadian53/GRL" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>