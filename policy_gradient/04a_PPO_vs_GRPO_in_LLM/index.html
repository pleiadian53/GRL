
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Physics-grounded reinforcement learning with particle-based belief representations">
      
      
        <meta name="author" content="GRL Research Team">
      
      
        <link rel="canonical" href="https://pleiadian53.github.io/GRL/policy_gradient/04a_PPO_vs_GRPO_in_LLM/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.1">
    
    
      
        <title>PPO vs GRPO: Credit Assignment in LLM Fine-Tuning - Generalized Reinforcement Learning (GRL)</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ppo-vs-grpo-credit-assignment-in-llm-fine-tuning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <div data-md-color-scheme="default" data-md-component="outdated" hidden>
        
      </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="Generalized Reinforcement Learning (GRL)" class="md-header__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Generalized Reinforcement Learning (GRL)
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PPO vs GRPO: Credit Assignment in LLM Fine-Tuning
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../GRL0/" class="md-tabs__link">
          
  
  
  GRL v0 (Tutorial Paper)

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../ROADMAP/" class="md-tabs__link">
        
  
  
    
  
  Research Roadmap

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../notebooks/" class="md-tabs__link">
          
  
  
  Notebooks

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../CONTRIBUTING/" class="md-tabs__link">
          
  
  
  About

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="Generalized Reinforcement Learning (GRL)" class="md-nav__button md-logo" aria-label="Generalized Reinforcement Learning (GRL)" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Generalized Reinforcement Learning (GRL)
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/pleiadian53/GRL" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
  </div>
  <div class="md-source__repository">
    pleiadian53/GRL
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    GRL v0 (Tutorial Paper)
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    GRL v0 (Tutorial Paper)
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Part I: Tutorials
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Part I: Tutorials
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Tutorial Index
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/00-overview/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 0: Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/01-core-concepts/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 1: Core Concepts
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/02-rkhs-foundations/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 2: RKHS Foundations
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/03-energy-and-fitness/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3: Energy and Fitness
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/03a-least-action-principle/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 3a: Least Action Principle
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/04-reinforcement-field/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4: Reinforcement Field
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/04a-riesz-representer/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 4a: Riesz Representer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/05-particle-memory/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 5: Particle Memory
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/06-memory-update/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6: MemoryUpdate
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/06a-advanced-memory-dynamics/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 6a: Advanced Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/07-rf-sarsa/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7: RF-SARSA
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/tutorials/07a-continuous-policy-inference/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Ch 7a: Continuous Policy Inference
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Quantum-Inspired Extensions
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    Quantum-Inspired Extensions
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/01-rkhs-quantum-parallel/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: RKHS-QM Parallel
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/01a-wavefunction-interpretation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Wavefunction Interpretation
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Basis and Amplitudes
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/03-complex-rkhs/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Complex RKHS
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/04-action-and-state-fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    04: Action and State Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/05-concept-projections-and-measurements/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    05: Concept Projections
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/06-agent-state-and-belief-evolution/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    06: Agent State and Belief
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/07-learning-the-field-beyond-gp/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    07: Learning Beyond GP
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    08: Memory Dynamics
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/quantum_inspired/09-path-integrals-and-action-principles/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    09: Path Integrals
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/implementation/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Implementation Guide
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../GRL0/recovering_classical_rl/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Recovering Classical RL
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Research Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Notebooks
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    Notebooks
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2" >
        
          
          <label class="md-nav__link" for="__nav_4_2" id="__nav_4_2_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Field Series
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_4_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2">
            <span class="md-nav__icon md-icon"></span>
            
  
    Field Series
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Series Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/ROADMAP/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Roadmap
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/00_intro_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    00: Introduction to Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/01_classical_vector_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01: Classical Vector Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/01a_vector_fields_and_odes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    01a: Vector Fields and ODEs
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/02_functional_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    02: Functional Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_4_2_7" >
        
          
          <label class="md-nav__link" for="__nav_4_2_7" id="__nav_4_2_7_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Reinforcement Fields
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="3" aria-labelledby="__nav_4_2_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4_2_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    Reinforcement Fields
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Overview
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03: Reinforcement Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    03a: Particle Coverage Effects
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Theory: Particle vs Gradient Fields
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    About
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    About
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../CONTRIBUTING/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Contributing
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../LICENSE/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    License
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-the-credit-assignment-problem-in-llms" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. The Credit Assignment Problem in LLMs
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-ppos-approach-learned-value-function" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. PPO's Approach: Learned Value Function
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-grpos-approach-group-normalized-no-critic" class="md-nav__link">
    <span class="md-ellipsis">
      
        3. GRPO's Approach: Group-Normalized, No Critic
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. GRPO&#39;s Approach: Group-Normalized, No Critic">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-clarifying-a-common-confusion" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 Clarifying a Common Confusion
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-the-correct-statement" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 The Correct Statement
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#4-why-grpo-works-despite-coarser-credit" class="md-nav__link">
    <span class="md-ellipsis">
      
        4. Why GRPO Works Despite Coarser Credit
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="4. Why GRPO Works Despite Coarser Credit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-group-comparison-is-a-strong-signal" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 Group Comparison Is a Strong Signal
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-the-ratio-provides-implicit-token-level-differentiation" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 The Ratio Provides Implicit Token-Level Differentiation
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-the-value-function-was-the-bottleneck" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 The Value Function Was the Bottleneck
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#5-when-does-token-level-credit-matter" class="md-nav__link">
    <span class="md-ellipsis">
      
        5. When Does Token-Level Credit Matter?
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#6-the-full-picture-post-training-pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      
        6. The Full Picture: Post-Training Pipeline
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#7-summary" class="md-nav__link">
    <span class="md-ellipsis">
      
        7. Summary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    <span class="md-ellipsis">
      
        References
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
    <a href="https://github.com/pleiadian53/GRL/edit/main/docs/policy_gradient/04a_PPO_vs_GRPO_in_LLM.md" title="Edit this page" class="md-content__button md-icon" rel="edit">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75z"/></svg>
    </a>
  
  


<h1 id="ppo-vs-grpo-credit-assignment-in-llm-fine-tuning">PPO vs GRPO: Credit Assignment in LLM Fine-Tuning<a class="headerlink" href="#ppo-vs-grpo-credit-assignment-in-llm-fine-tuning" title="Permanent link">&para;</a></h1>
<p><strong>How reward signals flow from response-level scores to token-level updates</strong></p>
<blockquote>
<p><em>Prerequisites: <a href="../03_PPO/">PPO</a>, <a href="../04_PPO_variants/">PPO Variants</a></em></p>
</blockquote>
<hr />
<h2 id="1-the-credit-assignment-problem-in-llms">1. The Credit Assignment Problem in LLMs<a class="headerlink" href="#1-the-credit-assignment-problem-in-llms" title="Permanent link">&para;</a></h2>
<p>When fine-tuning a language model with RL, the fundamental challenge is <strong>credit assignment</strong>: a reward model scores the <em>entire response</em>, but the policy made a decision at <em>every token</em>. Which tokens were responsible for the good (or bad) outcome?</p>
<p>Consider a model generating a 200-token math solution. The reward is binary: 1 if the final answer is correct, 0 otherwise. The model needs to learn that token 47 (where it chose the right algebraic step) mattered far more than token 150 (where it wrote "therefore"). But the reward signal says nothing about individual tokens.</p>
<p>This is the <strong>temporal credit assignment problem</strong> — familiar from RL in general, but especially acute in LLMs because:</p>
<ul>
<li>Sequences are long (hundreds to thousands of tokens)</li>
<li>Rewards are sparse (typically given only at the end)</li>
<li>The action space is enormous (vocabulary size ~32K–128K per step)</li>
</ul>
<p>PPO and GRPO handle this problem very differently.</p>
<hr />
<h2 id="2-ppos-approach-learned-value-function">2. PPO's Approach: Learned Value Function<a class="headerlink" href="#2-ppos-approach-learned-value-function" title="Permanent link">&para;</a></h2>
<p>In the standard RLHF pipeline, PPO trains a <strong>value network</strong> <span class="arithmatex">\(V_\psi(s_t)\)</span> that predicts the expected return from each token position <span class="arithmatex">\(t\)</span>:</p>
<div class="arithmatex">\[V_\psi(s_t) = \mathbb{E}\left[ R \mid \text{tokens generated so far} = (o_1, \ldots, o_t) \right]\]</div>
<p>The advantage at each token is then computed via GAE:</p>
<div class="arithmatex">\[\hat{A}_t = \sum_{l=0}^{T-t} (\gamma \lambda)^l \, \delta_{t+l}, \quad \delta_t = r_t + \gamma V_\psi(s_{t+1}) - V_\psi(s_t)\]</div>
<p>In principle, this gives <strong>token-level credit</strong> — each token gets its own advantage. In practice, there are two problems:</p>
<p><strong>Problem 1 — The value function is hard to train.</strong> The "state" <span class="arithmatex">\(s_t\)</span> is the entire token sequence so far. The value network must generalize across all possible prefixes of all possible responses to all possible prompts. This is a harder learning problem than the policy itself.</p>
<p><strong>Problem 2 — Sparse rewards degrade the signal.</strong> When the reward is given only at the final token, the TD errors <span class="arithmatex">\(\delta_t\)</span> for intermediate tokens depend entirely on the value function's predictions. If <span class="arithmatex">\(V_\psi\)</span> is inaccurate (and it usually is early in training), the per-token advantages are noisy. Many implementations fall back to <strong>response-level advantages</strong> — assigning the same advantage to every token in the sequence:</p>
<div class="arithmatex">\[\hat{A}_t \approx R - b \quad \text{for all } t\]</div>
<p>where <span class="arithmatex">\(b\)</span> is some baseline (e.g., the mean reward in the batch). This is simpler but throws away all temporal structure.</p>
<hr />
<h2 id="3-grpos-approach-group-normalized-no-critic">3. GRPO's Approach: Group-Normalized, No Critic<a class="headerlink" href="#3-grpos-approach-group-normalized-no-critic" title="Permanent link">&para;</a></h2>
<p>GRPO eliminates the value network entirely. For each prompt <span class="arithmatex">\(q\)</span>, it samples a group of <span class="arithmatex">\(G\)</span> responses and computes a <strong>response-level</strong> advantage via z-scoring:</p>
<div class="arithmatex">\[\hat{A}_i = \frac{r_i - \text{mean}(\{r_j\}_{j=1}^G)}{\text{std}(\{r_j\}_{j=1}^G)}\]</div>
<p>This advantage <span class="arithmatex">\(\hat{A}_i\)</span> is the <strong>same for all tokens</strong> in response <span class="arithmatex">\(i\)</span>. GRPO does not assign per-token credit through a value function.</p>
<h3 id="31-clarifying-a-common-confusion">3.1 Clarifying a Common Confusion<a class="headerlink" href="#31-clarifying-a-common-confusion" title="Permanent link">&para;</a></h3>
<p>There is a widespread misconception that "GRPO uses token-level credit assignment." This confusion arises from two different meanings of "value model":</p>
<p><strong>Meaning A — A learned critic network <span class="arithmatex">\(V_\psi(s_t)\)</span>:</strong></p>
<p>A separate neural network trained to predict values at each token position. This is what PPO uses. GRPO does <strong>not</strong> have this. This is what papers mean when they say "GRPO requires no value model."</p>
<p><strong>Meaning B — Any mechanism that produces per-token advantage signals:</strong></p>
<p>All RL algorithms need <em>some</em> way to weight the policy gradient at each token. GRPO applies the same response-level advantage <span class="arithmatex">\(\hat{A}_i\)</span> to every token in the response, but the PPO-style clipped ratio <span class="arithmatex">\(r_{i,t}(\theta)\)</span> is still computed <strong>per token</strong>:</p>
<div class="arithmatex">\[r_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} \mid q, o_{i,&lt;t})}{\pi_{\theta_k}(o_{i,t} \mid q, o_{i,&lt;t})}\]</div>
<p>So the <em>gradient signal</em> varies across tokens (because the ratio varies), even though the <em>advantage</em> is constant. This can look like "token-level credit" in practice, but the mechanism is fundamentally different from a learned critic.</p>
<h3 id="32-the-correct-statement">3.2 The Correct Statement<a class="headerlink" href="#32-the-correct-statement" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Concept</th>
<th>PPO</th>
<th>GRPO</th>
</tr>
</thead>
<tbody>
<tr>
<td>Trainable value network <span class="arithmatex">\(V_\psi\)</span></td>
<td>Yes</td>
<td>No</td>
</tr>
<tr>
<td>Advantage granularity</td>
<td>Token-level (via GAE) or response-level (fallback)</td>
<td>Response-level (group z-score)</td>
</tr>
<tr>
<td>Per-token gradient variation</td>
<td>Via ratio <span class="arithmatex">\(\times\)</span> advantage</td>
<td>Via ratio <span class="arithmatex">\(\times\)</span> (constant advantage)</td>
</tr>
<tr>
<td>Memory cost</td>
<td>High (critic = another LLM-sized model)</td>
<td>Low (no critic)</td>
</tr>
</tbody>
</table>
<p><strong>GRPO assigns response-level credit without training a value network. The per-token gradient variation comes from the probability ratio, not from a learned critic.</strong></p>
<hr />
<h2 id="4-why-grpo-works-despite-coarser-credit">4. Why GRPO Works Despite Coarser Credit<a class="headerlink" href="#4-why-grpo-works-despite-coarser-credit" title="Permanent link">&para;</a></h2>
<p>If GRPO's advantages are response-level (coarser than PPO's token-level GAE), why does it work so well?</p>
<h3 id="41-group-comparison-is-a-strong-signal">4.1 Group Comparison Is a Strong Signal<a class="headerlink" href="#41-group-comparison-is-a-strong-signal" title="Permanent link">&para;</a></h3>
<p>For tasks with verifiable outcomes (math, code, factual QA), the reward is often binary: correct or incorrect. In this setting, the group comparison is highly informative:</p>
<ul>
<li>If 12 out of 16 responses are correct, the 4 incorrect ones get strongly negative advantages</li>
<li>The model learns to avoid the <em>patterns</em> that distinguish incorrect responses from correct ones</li>
<li>Over many prompts and groups, this signal is sufficient to identify which token-level behaviors matter</li>
</ul>
<h3 id="42-the-ratio-provides-implicit-token-level-differentiation">4.2 The Ratio Provides Implicit Token-Level Differentiation<a class="headerlink" href="#42-the-ratio-provides-implicit-token-level-differentiation" title="Permanent link">&para;</a></h3>
<p>Even with a constant advantage <span class="arithmatex">\(\hat{A}_i\)</span>, the gradient at each token is:</p>
<div class="arithmatex">\[\nabla_\theta \log \pi_\theta(o_{i,t} \mid q, o_{i,&lt;t}) \cdot \hat{A}_i\]</div>
<p>Tokens where the policy is uncertain (high entropy) produce larger gradients than tokens where the policy is confident. This provides a natural form of token-level weighting without an explicit critic.</p>
<h3 id="43-the-value-function-was-the-bottleneck">4.3 The Value Function Was the Bottleneck<a class="headerlink" href="#43-the-value-function-was-the-bottleneck" title="Permanent link">&para;</a></h3>
<p>In practice, PPO's value function for LLMs is often so poorly trained that its token-level advantages are noisier than GRPO's response-level advantages. A clean, unbiased response-level signal can outperform a noisy, biased token-level signal.</p>
<hr />
<h2 id="5-when-does-token-level-credit-matter">5. When Does Token-Level Credit Matter?<a class="headerlink" href="#5-when-does-token-level-credit-matter" title="Permanent link">&para;</a></h2>
<p>Response-level credit (GRPO) works well when:</p>
<ul>
<li>Rewards are sparse and binary (correct/incorrect)</li>
<li>Responses are relatively short (&lt; 500 tokens)</li>
<li>The task has clear success/failure modes</li>
</ul>
<p>Token-level credit (PPO with a good critic, or process reward models) becomes important when:</p>
<ul>
<li>Responses are very long (multi-step reasoning chains)</li>
<li>Intermediate steps have independent quality (each step in a proof can be right or wrong)</li>
<li>The task requires fine-grained behavioral shaping (tone, style, safety at specific points)</li>
</ul>
<p>This is why <strong>process reward models</strong> (PRMs) — which score intermediate reasoning steps, not just the final answer — are an active research direction. They provide dense reward signals that can be used with either PPO or GRPO.</p>
<hr />
<h2 id="6-the-full-picture-post-training-pipeline">6. The Full Picture: Post-Training Pipeline<a class="headerlink" href="#6-the-full-picture-post-training-pipeline" title="Permanent link">&para;</a></h2>
<p>The modern LLM post-training pipeline typically follows:</p>
<div class="arithmatex">\[\text{Pre-training} \to \text{SFT} \to \text{RL alignment}\]</div>
<p>where the RL alignment step uses one of:</p>
<table>
<thead>
<tr>
<th>Method</th>
<th>Reward source</th>
<th>Credit assignment</th>
<th>Critic needed</th>
<th>Exploration</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>PPO</strong></td>
<td>Reward model</td>
<td>Token-level (GAE) or response-level (fallback)</td>
<td>Yes</td>
<td>Online</td>
</tr>
<tr>
<td><strong>GRPO</strong></td>
<td>Reward model</td>
<td>Response-level (group z-score)</td>
<td>No</td>
<td>Online</td>
</tr>
<tr>
<td><strong>DPO</strong></td>
<td>Preference pairs</td>
<td>Implicit (in loss)</td>
<td>No</td>
<td>Offline</td>
</tr>
<tr>
<td><strong>RLOO</strong></td>
<td>Reward model</td>
<td>Response-level (leave-one-out)</td>
<td>No</td>
<td>Online</td>
</tr>
</tbody>
</table>
<p>GRPO has emerged as the preferred method for <strong>reasoning tasks</strong> (math, code) where rewards are verifiable and binary. PPO remains relevant for <strong>open-ended generation</strong> where dense, nuanced credit assignment matters. DPO is used when preference data is available but online generation is too expensive.</p>
<hr />
<h2 id="7-summary">7. Summary<a class="headerlink" href="#7-summary" title="Permanent link">&para;</a></h2>
<p>The core difference between PPO and GRPO in the LLM setting is not <em>whether</em> they assign token-level credit, but <em>how</em>:</p>
<ul>
<li><strong>PPO</strong> trains a separate value network to estimate per-token advantages. This is powerful but expensive (4 models in memory) and fragile (the critic is hard to train for variable-length text).</li>
<li><strong>GRPO</strong> uses response-level group-normalized advantages with no critic. The per-token gradient variation comes from the probability ratio, not a learned value function. This is cheaper, simpler, and often more stable.</li>
</ul>
<p>Both methods use the same PPO-style clipped objective and the same score function <span class="arithmatex">\(\nabla_\theta \log \pi_\theta\)</span> at their core. The difference is entirely in how the advantage signal is computed.</p>
<hr />
<h2 id="references">References<a class="headerlink" href="#references" title="Permanent link">&para;</a></h2>
<ul>
<li><strong>[Shao et al., 2024]</strong> — <em>DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models.</em> <a href="https://arxiv.org/abs/2402.03300">arXiv:2402.03300</a> (GRPO)</li>
<li><strong>[Zheng et al., 2023]</strong> — <em>Secrets of RLHF in Large Language Models Part I: PPO.</em> <a href="https://arxiv.org/abs/2307.04964">arXiv:2307.04964</a></li>
<li><strong>[Lightman et al., 2023]</strong> — <em>Let's Verify Step by Step.</em> <a href="https://arxiv.org/abs/2305.20050">arXiv:2305.20050</a> (Process reward models)</li>
</ul>
<hr />
<p><em>Previous: <a href="../04_PPO_variants/">PPO Variants and Modern Descendants</a></em> | <em>See also: <a href="03a_pg_in_ppo.md">Where Is the Policy Gradient in PPO?</a></em></p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2026 GRL Research Team
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        
<div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/pleiadian53/GRL" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../..", "features": ["navigation.instant", "navigation.tracking", "navigation.tabs", "navigation.tabs.sticky", "navigation.sections", "navigation.expand", "navigation.top", "search.suggest", "search.highlight", "content.code.copy", "content.action.edit"], "search": "../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": {"provider": "mike"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../javascripts/mathjax.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>