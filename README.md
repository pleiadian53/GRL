# GRL: Generalized Reinforcement Learning

**Actions as Operators on State Space**

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.1+](https://img.shields.io/badge/pytorch-2.1+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## üéØ What is GRL?

**Generalized Reinforcement Learning (GRL)** redefines the concept of "action" in reinforcement learning. Instead of treating actions as discrete indices or fixed-dimensional vectors, GRL models actions as **parametric operators** that transform the state space.

```mermaid
graph LR
    subgraph Traditional RL
        S1[State s] --> P1[Policy œÄ]
        P1 --> A1[Action a ‚àà A]
        A1 --> S2[Next State s']
    end
    
    subgraph GRL
        S3[State s] --> P2[Policy œÄ]
        P2 --> O1[Operator Parameters Œ∏]
        O1 --> O2[Operator √î<sub>Œ∏</sub>]
        O2 --> S4[State Transformation]
    end
    
    style S1 fill:#e1f5ff,stroke:#01579b
    style S2 fill:#e1f5ff,stroke:#01579b
    style A1 fill:#fff3e0,stroke:#e65100
    style P1 fill:#f3e5f5,stroke:#4a148c
    
    style S3 fill:#e1f5ff,stroke:#01579b
    style S4 fill:#e8f5e9,stroke:#1b5e20
    style O1 fill:#fff9c4,stroke:#f57f17
    style O2 fill:#ffe0b2,stroke:#e65100
    style P2 fill:#f3e5f5,stroke:#4a148c
```

This formulation, inspired by the **least-action principle** in physics, leads to policies that are not only optimal but also physically grounded‚Äîpreferring smooth, efficient transformations over abrupt changes.

---

## üìñ Tutorial Paper: Understanding GRL

We present GRL as a comprehensive **tutorial paper**, allowing you to learn at your own pace:

### [Start Learning ‚Üí docs/GRL0/](docs/GRL0/)

| Part | Chapters | What You'll Learn |
|------|----------|-------------------|
| **I: Foundations** | [0: Overview](docs/GRL0/tutorials/00-overview.md), [1: Core Concepts](docs/GRL0/tutorials/01-core-concepts.md), 2-3 | Augmented space, particles, kernels, RKHS |
| **II: Reinforcement Field** | 4-5 | Value functions over augmented space |
| **III: Algorithms** | 6-7 | MemoryUpdate, RF-SARSA |
| **IV: Theory** | 8-10 | Soft transitions, POMDP interpretation |
| **V: Implementation** | 14-16 | From theory to code |

**Reading time**: ~2 hours for overview, ~8 hours for complete understanding

---

## üîë Key Innovations

| Aspect | Classical RL | GRL |
|--------|--------------|-----|
| **Action** | Discrete index or vector | Parametric operator $\hat{O}(\theta)$ |
| **Action Space** | Finite or bounded | Continuous manifold |
| **Value Function** | $Q(s, a)$ | Reinforcement field $Q^+(s, \theta)$ over augmented space |
| **Experience** | Replay buffer | Particle memory in RKHS |
| **Policy** | Learned function | Inferred from energy landscape |
| **Uncertainty** | External (dropout, ensembles) | Emergent from particle sparsity |

### Why GRL?

- **Continuous action generation**: No discretization, full precision
- **Smooth generalization**: Nearby parameters ‚Üí similar behavior  
- **Compositional actions**: Operators can be composed
- **Physical interpretability**: Parameters have meaning (forces, torques)
- **Uncertainty quantification**: Sparse particles = high uncertainty

---

## üöÄ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/pleiadian53/GRL.git
cd GRL

# Create environment with mamba/conda
mamba env create -f environment.yml
mamba activate grl

# Install in development mode
pip install -e .

# Verify installation (auto-detects CPU/GPU/MPS)
python scripts/verify_installation.py
```

See [INSTALL.md](INSTALL.md) for detailed instructions.

### First Steps

1. **Read the tutorial**: Start with [Chapter 0: Overview](docs/GRL0/tutorials/00-overview.md)
2. **Explore concepts**: Work through [Chapter 1: Core Concepts](docs/GRL0/tutorials/01-core-concepts.md)
3. **Understand algorithms**: See the algorithm chapters (coming soon)
4. **Implement**: Follow the [implementation guide](docs/GRL0/implementation/)

---

## üìÅ Project Structure

```
GRL/
‚îú‚îÄ‚îÄ src/grl/                    # Core library
‚îÇ   ‚îú‚îÄ‚îÄ core/                   # Particle memory, kernels
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/             # MemoryUpdate, RF-SARSA
‚îÇ   ‚îú‚îÄ‚îÄ envs/                   # Environments
‚îÇ   ‚îî‚îÄ‚îÄ visualization/          # Plotting tools
‚îú‚îÄ‚îÄ docs/                       # üìö Public documentation
‚îÇ   ‚îî‚îÄ‚îÄ GRL0/                   # Tutorial paper for GRL-v0
‚îÇ       ‚îú‚îÄ‚îÄ tutorials/          # Tutorial chapters
‚îÇ       ‚îú‚îÄ‚îÄ paper/              # Paper-ready sections
‚îÇ       ‚îî‚îÄ‚îÄ implementation/     # Implementation specs
‚îú‚îÄ‚îÄ notebooks/                  # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ vector_field.ipynb     # Vector field demonstrations
‚îú‚îÄ‚îÄ examples/                   # Runnable examples
‚îú‚îÄ‚îÄ demo/                       # Demo folder (DQN examples)
‚îú‚îÄ‚îÄ scripts/                    # Utility scripts
‚îú‚îÄ‚îÄ tests/                      # Unit tests
‚îî‚îÄ‚îÄ configs/                    # Configuration files
```

---

## üìÑ Documentation

### GRL-v0 Tutorial Paper

The comprehensive guide to understanding GRL:

- **[Overview](docs/GRL0/README.md)** ‚Äî Start here
- **[Tutorials](docs/GRL0/tutorials/)** ‚Äî Chapter-by-chapter learning
- **[Implementation](docs/GRL0/implementation/)** ‚Äî Technical specifications

### Additional Resources

- **[Installation Guide](INSTALL.md)** ‚Äî Detailed setup instructions
- **[Theory](docs/theory/)** ‚Äî Mathematical foundations
- **[Vector Field Demo](notebooks/vector_field.ipynb)** ‚Äî Interactive visualizations

---

## üî¨ Research Roadmap

### Current: GRL-v0 (Baseline)

Understanding and reimplementing the original GRL framework with:
- Particle-based belief representation
- Kernel-induced reinforcement field  
- Two-layer RF-SARSA algorithm
- Emergent soft state transitions
- POMDP interpretation

### Planned Extensions

| Paper | Focus | Status |
|-------|-------|--------|
| **Paper A** | Operator algebra and theory | üìã Planned |
| **Paper B** | Algorithms and neural operators | üìã Planned |
| **Paper C** | Applications and experiments | üìã Planned |

---

## üìä How GRL Works: Particle-Based Learning

```mermaid
graph TD
    A[Environment State s] --> B[Query Particle Memory Œ©]
    B --> C[Compute Reinforcement Field<br/>Q‚Å∫<sub>z</sub> = Œ£ w<sub>i</sub> k<sub>z, z<sub>i</sub></sub>]
    C --> D[Infer Action Parameters Œ∏<br/>via Energy Minimization]
    D --> E[Execute Operator √î<sub>Œ∏</sub>]
    E --> F[Observe s', r]
    F --> G[Create/Update Particle<br/>z = <sub>s, Œ∏</sub> with weight w]
    G --> H{MemoryUpdate}
    H -->|Kernel Association| I[Merge or Add Particle]
    I --> B
    
    style A fill:#e1f5ff,stroke:#01579b,stroke-width:2px
    style B fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style C fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    style D fill:#fff9c4,stroke:#f57f17,stroke-width:2px
    style E fill:#ffe0b2,stroke:#e65100,stroke-width:2px
    style F fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
    style G fill:#fce4ec,stroke:#880e4f,stroke-width:2px
    style H fill:#e0f2f1,stroke:#004d40,stroke-width:3px
    style I fill:#fff3e0,stroke:#e65100,stroke-width:2px
```

### Code Example

```python
from grl.core import ParticleMemory
from grl.core import RBFKernel
from grl.algorithms import MemoryUpdate, RFSarsa

# Create particle memory (the agent's belief state)
memory = ParticleMemory()

# Define similarity kernel
kernel = RBFKernel(lengthscale=1.0)

# Learning loop
for episode in range(num_episodes):
    state = env.reset()
    
    for step in range(max_steps):
        # Infer action from particle memory
        action = infer_action(memory, state, kernel)
        
        # Execute and observe
        next_state, reward, done = env.step(action)
        
        # Update particle memory (belief transition)
        memory = memory_update(memory, state, action, reward, kernel)
        
        state = next_state
```

---

## üìù Citation

```bibtex
@article{grl2026,
  title={Generalized Reinforcement Learning: A Tutorial},
  author={[Author]},
  year={2026}
}
```

---

## üìú License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## üôè Acknowledgments

- Inspired by the **least-action principle** in classical mechanics
- Built on insights from **kernel methods** and **Gaussian processes**
- Connections to **energy-based models**, **POMDPs**, and **belief-based control**

---

**[üìö Start the Tutorial ‚Üí](docs/GRL0/tutorials/00-overview.md)**
