{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"GRL: Generalized Reinforcement Learning","text":"<p>Actions as Operators on State Space</p> <p> </p>"},{"location":"#what-is-grl","title":"\ud83c\udfaf What is GRL?","text":"<p>Generalized Reinforcement Learning (GRL) redefines the concept of \"action\" in reinforcement learning. Instead of treating actions as discrete indices or fixed-dimensional vectors, GRL models actions as parametric operators that transform the state space.</p> <pre><code>flowchart TB\n    subgraph TRL[\"\ud83d\udd35 Traditional RL\"]\n        direction LR\n        S1[\"&lt;b&gt;State&lt;/b&gt;&lt;br/&gt;s\"] --&gt; P1[\"&lt;b&gt;Policy&lt;/b&gt;&lt;br/&gt;\u03c0\"]\n        P1 --&gt; A1[\"&lt;b&gt;Action Symbol&lt;/b&gt;&lt;br/&gt;a \u2208 A\"]\n        A1 --&gt; NS1[\"&lt;b&gt;Next State&lt;/b&gt;&lt;br/&gt;s'\"]\n    end\n\n    TRL --&gt; GRL\n\n    subgraph GRL[\"\u2728 Generalized RL\"]\n        direction LR\n        S2[\"&lt;b&gt;State&lt;/b&gt;&lt;br/&gt;s\"] --&gt; P2[\"&lt;b&gt;Policy&lt;/b&gt;&lt;br/&gt;\u03c0\"]\n        P2 --&gt; AP[\"&lt;b&gt;Operator Params&lt;/b&gt;&lt;br/&gt;\u03b8\"]\n        AP --&gt; OP[\"&lt;b&gt;Operator&lt;/b&gt;&lt;br/&gt;\u00d4&lt;sub&gt;\u03b8&lt;/sub&gt;\"]\n        OP --&gt; ST[\"&lt;b&gt;State Transform&lt;/b&gt;&lt;br/&gt;s' = \u00d4&lt;sub&gt;\u03b8&lt;/sub&gt;(s)\"]\n    end\n\n    style S1 fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style NS1 fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style A1 fill:#fff9c4,stroke:#f57c00,stroke-width:3px,color:#000\n    style P1 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n\n    style S2 fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style ST fill:#c8e6c9,stroke:#388e3c,stroke-width:3px,color:#000\n    style AP fill:#fff59d,stroke:#fbc02d,stroke-width:3px,color:#000\n    style OP fill:#ffcc80,stroke:#f57c00,stroke-width:3px,color:#000\n    style P2 fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n\n    style TRL fill:#fafafa,stroke:#666,stroke-width:2px\n    style GRL fill:#fafafa,stroke:#666,stroke-width:2px\n\n    linkStyle 4 stroke:#666,stroke-width:2px</code></pre> <p>This formulation, inspired by the least-action principle in physics, leads to policies that are not only optimal but also physically grounded\u2014preferring smooth, efficient transformations over abrupt changes.</p>"},{"location":"#tutorial-papers","title":"\ud83d\udcd6 Tutorial Papers","text":""},{"location":"#part-i-reinforcement-fields-particle-based-learning","title":"Part I: Reinforcement Fields \u2014 Particle-Based Learning","text":"<p>Status: \ud83d\udd04 In progress (9/10 chapters complete)</p> <p>Particle-based belief representation, energy landscapes, and functional learning over augmented state-action space.</p> <p>Start Learning \u2192 | Research Roadmap \u2192</p> Section Chapters Topics Foundations 0, 1, 2, 3 Augmented space, particles, RKHS, energy Field &amp; Memory 4, 4a, 5, 6, 6a Functional fields, Riesz theorem, belief states, MemoryUpdate, advanced memory Algorithms 7 RF-SARSA (next) Interpretation 8-10 Soft transitions, POMDP, synthesis"},{"location":"#part-ii-reinforcement-fields-emergent-structure-spectral-abstraction","title":"Part II: Reinforcement Fields \u2014 Emergent Structure &amp; Spectral Abstraction","text":"<p>Status: \ud83d\udccb Planned (after Part I)</p> <p>Spectral discovery of hierarchical concepts through functional clustering in RKHS.</p> Section Chapters Topics Functional Clustering 11 Clustering in function space Spectral Concepts 12 Concepts as eigenmodes Hierarchical Control 13 Multi-level abstraction <p>Based on: Section V of the original paper</p> <p>Reading time: ~10 hours total (both parts)</p>"},{"location":"#quantum-inspired-extensions","title":"Quantum-Inspired Extensions","text":"<p>Status: \ud83d\udd2c Advanced topics (9 chapters complete)</p> <p>Mathematical connections to quantum mechanics and novel probability formulations for ML.</p> <p>Explore Advanced Topics \u2192</p> Theme Chapters Topics Foundations 01, 01a, 02 RKHS-QM parallel, state vs. wavefunction, amplitude interpretation Complex RKHS 03 Complex-valued kernels, interference, phase semantics Projections 04, 05, 06 Action/state fields, concept subspaces, belief dynamics Learning &amp; Memory 07, 08 Beyond GP, memory dynamics, principled consolidation <p>Novel Contributions: - Amplitude-based RL: Complex-valued value functions with phase semantics - MDL consolidation: Information-theoretic memory management - Concept-based MoE: Hierarchical RL via subspace projections</p>"},{"location":"#key-innovations","title":"\ud83d\udd11 Key Innovations","text":"Aspect Classical RL GRL Action Discrete index or vector Parametric operator \\(\\hat{O}(\\theta)\\) Action Space Finite or bounded Continuous manifold Value Function \\(Q(s, a)\\) Reinforcement field \\(Q^+(s, \\theta)\\) over augmented space Experience Replay buffer Particle memory in RKHS Policy Learned function Inferred from energy landscape Uncertainty External (dropout, ensembles) Emergent from particle sparsity"},{"location":"#grl-as-a-unifying-framework","title":"GRL as a Unifying Framework","text":"<p>Key Insight: Traditional RL algorithms (Q-learning, DQN, PPO, SAC, RLHF for LLMs) are special cases of GRL!</p> <p>When you: - Discretize actions \u2192 GRL recovers Q-learning - Use neural networks \u2192 GRL recovers DQN - Apply Boltzmann policies \u2192 GRL recovers REINFORCE/Actor-Critic - Fine-tune LLMs \u2192 GRL generalizes RLHF</p> <p>See: Recovering Classical RL from GRL \u2192</p>"},{"location":"#why-grl","title":"Why GRL?","text":"<ul> <li>Generalization: Subsumes existing methods as special cases</li> <li>Continuous actions: No discretization, full precision</li> <li>Smooth interpolation: Nearby parameters \u2192 similar behavior  </li> <li>Compositional: Operators can be composed (operator algebra)</li> <li>Uncertainty: Sparse particles = high uncertainty (no ensembles needed)</li> <li>Interpretability: Energy landscapes, particle inspection</li> <li>Modern applications: Applies to RLHF, prompt optimization, neural architecture search</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/pleiadian53/GRL.git\ncd GRL\n\n# Create environment with mamba/conda\nmamba env create -f environment.yml\nmamba activate grl\n\n# Install in development mode\npip install -e .\n\n# Verify installation (auto-detects CPU/GPU/MPS)\npython scripts/verify_installation.py\n</code></pre> <p>Installation instructions coming soon.</p>"},{"location":"#first-steps","title":"First Steps","text":"<ol> <li>Read the tutorial: Start with Chapter 0: Overview</li> <li>Explore concepts: Work through Chapter 1: Core Concepts</li> <li>Understand algorithms: See the algorithm chapters (coming soon)</li> <li>Implement: Follow the implementation guide</li> </ol>"},{"location":"#project-structure","title":"\ud83d\udcc1 Project Structure","text":"<pre><code>GRL/\n\u251c\u2500\u2500 src/grl/                    # Core library\n\u2502   \u251c\u2500\u2500 core/                   # Particle memory, kernels\n\u2502   \u251c\u2500\u2500 algorithms/             # MemoryUpdate, RF-SARSA\n\u2502   \u251c\u2500\u2500 envs/                   # Environments\n\u2502   \u2514\u2500\u2500 visualization/          # Plotting tools\n\u251c\u2500\u2500 docs/                       # \ud83d\udcda Public documentation\n\u2502   \u2514\u2500\u2500 GRL0/                   # Tutorial paper (Reinforcement Fields)\n\u2502       \u251c\u2500\u2500 tutorials/          # Tutorial chapters (6/10 complete)\n\u2502       \u251c\u2500\u2500 paper/              # Paper-ready sections\n\u2502       \u2514\u2500\u2500 implementation/     # Implementation specs\n\u251c\u2500\u2500 notebooks/                  # Jupyter notebooks\n\u2502   \u2514\u2500\u2500 vector_field.ipynb     # Vector field demonstrations\n\u251c\u2500\u2500 examples/                   # Runnable examples\n\u251c\u2500\u2500 scripts/                    # Utility scripts\n\u251c\u2500\u2500 tests/                      # Unit tests\n\u2514\u2500\u2500 configs/                    # Configuration files\n</code></pre>"},{"location":"#documentation","title":"\ud83d\udcc4 Documentation","text":""},{"location":"#tutorial-papers-reinforcement-fields-two-parts","title":"Tutorial Papers: Reinforcement Fields (Two Parts)","text":"<p>Part I: Particle-Based Learning (6/10 chapters complete)</p> <ul> <li>Start Here \u2014 Overview</li> <li>Tutorials \u2014 Chapter-by-chapter learning</li> <li>Implementation \u2014 Technical specifications</li> </ul> <p>Part II: Emergent Structure &amp; Spectral Abstraction (Planned)</p>"},{"location":"#additional-resources","title":"Additional Resources","text":"<ul> <li>Implementation Guide \u2014 Technical specifications</li> <li>Research Roadmap \u2014 Future directions</li> </ul>"},{"location":"#research-papers","title":"\ud83d\udd2c Research Papers","text":""},{"location":"#original-paper-arxiv-2022","title":"Original Paper (arXiv 2022)","text":"<p>Generalized Reinforcement Learning: Experience Particles, Action Operator, Reinforcement Field, Memory Association, and Decision Concepts</p> <p>Po-Hsiang Chiu, Manfred Huber arXiv:2208.04822 (2022) \u2014 37 pages, 15 figures</p> <p>The foundational work introducing particle-based belief states, reinforcement fields, and concept-driven learning.</p>"},{"location":"#tutorial-papers-this-repository","title":"Tutorial Papers (This Repository)","text":"<p>Reinforcement Fields Framework \u2014 Enhanced exposition with modern formalization</p> <p>Part I: Particle-Based Learning - Functional fields over augmented state-action space - Particle memory as belief state in RKHS - MemoryUpdate and RF-SARSA algorithms - Emergent soft state transitions, POMDP interpretation</p> <p>Status: \ud83d\udd04 Tutorial in progress (6/10 chapters complete)</p> <p>Part II: Emergent Structure &amp; Spectral Abstraction - Functional clustering (clustering functions, not points) - Spectral methods on kernel matrices - Concepts as coherent subspaces of the reinforcement field - Hierarchical policy organization</p> <p>Status: \ud83d\udccb Planned (after Part I)</p>"},{"location":"#planned-extensions","title":"Planned Extensions","text":"Paper Title Status Progress Paper A Generalized Reinforcement Learning \u2014 Actions as Operators \ud83d\udfe2 Draft Complete ~70% Operator algebra, generalized Bellman equation, energy regularization Complete draft, 3/7 figures, proofs outlined Paper B Operator Policies \u2014 Learning State-Space Operators with Neural Operator Networks (tentative) \u23f3 Planned ~0% Neural operators, scalable training, operator-actor-critic After Paper A Paper C Applications of GRL to Physics, Robotics, and Differentiable Control (tentative) \u23f3 Planned ~0% Physics-based control, compositional behaviors, transfer learning After Paper B <p>Timeline: - Paper A: Target submission April 2026 (NeurIPS/ICML) - Paper B: Target submission June 2026 (ICML/NeurIPS) - Paper C: Target submission July 2026 (CoRL)</p> <p>See: Research Roadmap for detailed timeline and additional research directions.</p>"},{"location":"#how-grl-works-particle-based-learning","title":"\ud83d\udcca How GRL Works: Particle-Based Learning","text":"<pre><code>flowchart LR\n    A[\"\ud83c\udf0d &lt;b&gt;State&lt;/b&gt;&lt;br/&gt;s\"] --&gt; B[\"\ud83d\udcbe &lt;b&gt;Query&lt;/b&gt;&lt;br/&gt;Memory \u03a9\"]\n    B --&gt; C[\"\ud83d\udcca &lt;b&gt;Compute&lt;/b&gt;&lt;br/&gt;Field Q\u207a\"]\n    C --&gt; D[\"\ud83c\udfaf &lt;b&gt;Infer&lt;/b&gt;&lt;br/&gt;Action \u03b8\"]\n    D --&gt; E[\"\u26a1 &lt;b&gt;Execute&lt;/b&gt;&lt;br/&gt;Operator\"]\n    E --&gt; F[\"\ud83d\udc41\ufe0f &lt;b&gt;Observe&lt;/b&gt;&lt;br/&gt;s', r\"]\n    F --&gt; G[\"\u2728 &lt;b&gt;Create&lt;/b&gt;&lt;br/&gt;Particle\"]\n    G --&gt; H[\"\ud83d\udd04 &lt;b&gt;Update&lt;/b&gt;&lt;br/&gt;Memory\"]\n    H --&gt;|Loop| B\n\n    style A fill:#e3f2fd,stroke:#1976d2,stroke-width:3px,color:#000\n    style B fill:#fff9c4,stroke:#f57c00,stroke-width:3px,color:#000\n    style C fill:#f3e5f5,stroke:#7b1fa2,stroke-width:3px,color:#000\n    style D fill:#fff59d,stroke:#fbc02d,stroke-width:3px,color:#000\n    style E fill:#ffcc80,stroke:#f57c00,stroke-width:3px,color:#000\n    style F fill:#c8e6c9,stroke:#388e3c,stroke-width:3px,color:#000\n    style G fill:#f8bbd0,stroke:#c2185b,stroke-width:3px,color:#000\n    style H fill:#b2dfdb,stroke:#00796b,stroke-width:3px,color:#000</code></pre>"},{"location":"#code-example","title":"Code Example","text":"<pre><code>from grl.core import ParticleMemory\nfrom grl.core import RBFKernel\nfrom grl.algorithms import MemoryUpdate, RFSarsa\n\n# Create particle memory (the agent's belief state)\nmemory = ParticleMemory()\n\n# Define similarity kernel\nkernel = RBFKernel(lengthscale=1.0)\n\n# Learning loop\nfor episode in range(num_episodes):\n    state = env.reset()\n\n    for step in range(max_steps):\n        # Infer action from particle memory\n        action = infer_action(memory, state, kernel)\n\n        # Execute and observe\n        next_state, reward, done = env.step(action)\n\n        # Update particle memory (belief transition)\n        memory = memory_update(memory, state, action, reward, kernel)\n\n        state = next_state\n</code></pre>"},{"location":"#citation","title":"\ud83d\udcdd Citation","text":""},{"location":"#original-arxiv-paper","title":"Original arXiv Paper","text":"<p>The foundational work is available on arXiv:</p> <p>Chiu, P.-H., &amp; Huber, M. (2022). Generalized Reinforcement Learning: Experience Particles, Action Operator, Reinforcement Field, Memory Association, and Decision Concepts. arXiv:2208.04822.</p> <pre><code>@article{chiu2022generalized,\n  title={Generalized Reinforcement Learning: Experience Particles, Action Operator, \n         Reinforcement Field, Memory Association, and Decision Concepts},\n  author={Chiu, Po-Hsiang and Huber, Manfred},\n  journal={arXiv preprint arXiv:2208.04822},\n  year={2022},\n  url={https://arxiv.org/abs/2208.04822}\n}\n</code></pre> <p>Read on arXiv \u2192</p>"},{"location":"#tutorial-papers-this-repository_1","title":"Tutorial Papers (This Repository)","text":"<p>The tutorial series provides enhanced exposition and modern formalization:</p> <p>Part I: Particle-Based Learning (In progress) <pre><code>@article{chiu2026part1,\n  title={Reinforcement Fields: Particle-Based Learning},\n  author={Chiu, Po-Hsiang and Huber, Manfred},\n  journal={In preparation},\n  year={2026}\n}\n</code></pre></p> <p>Part II: Emergent Structure &amp; Spectral Abstraction (Planned) <pre><code>@article{chiu2026part2,\n  title={Reinforcement Fields: Emergent Structure and Spectral Abstraction},\n  author={Chiu, Po-Hsiang and Huber, Manfred},\n  journal={In preparation},\n  year={2026}\n}\n</code></pre></p>"},{"location":"#operator-extensions-future-work","title":"Operator Extensions (Future Work)","text":"<pre><code>@article{chiu2026operators,\n  title={Generalized Reinforcement Learning \u2014 Actions as Operators},\n  author={Chiu, Po-Hsiang},\n  journal={In preparation},\n  year={2026+}\n}\n</code></pre>"},{"location":"#license","title":"\ud83d\udcdc License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"#the-grl-framework","title":"\ud83c\udf1f The GRL Framework","text":"<p>GRL (Generalized Reinforcement Learning) is a family of methods that rethink how actions are represented and learned.</p> <p>Original paper: arXiv:2208.04822 (Chiu &amp; Huber, 2022)</p>"},{"location":"#reinforcement-fields-this-repository","title":"Reinforcement Fields (This Repository)","text":"<p>Two-Part Tutorial Series:</p> <p>Part I: Particle-Based Learning - Actions as continuous parameters in augmented state-action space - Particle memory as belief state, kernel-induced value functions - Learning through energy landscape navigation</p> <p>Part II: Emergent Structure &amp; Spectral Abstraction - Concepts emerge from functional clustering in RKHS - Spectral methods discover hierarchical structure - Multi-level policy organization</p> <p>Key Innovation: Learning emerges from particle dynamics in function space, not explicit policy optimization.</p>"},{"location":"#actions-as-operators-paper-a-in-development","title":"Actions as Operators (Paper A \u2014 In Development)","text":"<p>Core Idea: Actions as parametric operators that transform state space, with operator algebra providing compositional structure.</p> <p>Key Innovation: Operator manifolds replace fixed action spaces, enabling compositional behaviors and physical interpretability.</p>"},{"location":"#acknowledgments","title":"\ud83d\ude4f Acknowledgments","text":""},{"location":"#mathematical-foundations","title":"Mathematical Foundations","text":"<p>Core Framework: - Formulated in Reproducing Kernel Hilbert Spaces (RKHS) \u2014 the functional framework for particle-based belief states - Kernel methods define the geometry and similarity structure of augmented state-action space - Inspired by the least-action principle in classical mechanics</p> <p>Quantum-Inspired Probability: - Probability amplitudes instead of direct probabilities \u2014 RKHS inner products as amplitude overlaps - Complex-valued RKHS enabling interference effects and phase semantics for temporal/contextual dynamics - Wave function analogy \u2014 The reinforcement field as a superposition of particle basis states - This formulation is novel to mainstream ML and opens new directions for probabilistic reasoning</p> <p>See: Quantum-Inspired Extensions for technical details.</p>"},{"location":"#conceptual-connections","title":"Conceptual Connections","text":"<ul> <li>Energy-based models (EBMs) \u2014 Control as energy landscape navigation</li> <li>POMDPs and belief-based control \u2014 Particle ensembles as implicit belief states</li> <li>Score-based methods \u2014 Energy gradients guide policy inference</li> </ul>"},{"location":"#implementation-tools","title":"Implementation Tools","text":"<ul> <li>Gaussian process regression can model scalar energy fields (but is not essential to the framework)</li> <li>Neural operators for learning parametric action transformations</li> <li>Diffusion models share the gradient-field perspective</li> </ul> <p>\ud83d\udcda Start the Tutorial \u2192</p>"},{"location":"CONTRIBUTING/","title":"Contributing to GRL","text":"<p>Thank you for your interest in contributing to Generalized Reinforcement Learning (GRL)!</p>"},{"location":"CONTRIBUTING/#how-to-contribute","title":"How to Contribute","text":""},{"location":"CONTRIBUTING/#reporting-issues","title":"Reporting Issues","text":"<p>If you find a bug or have a suggestion: 1. Check if it's already reported in GitHub Issues 2. If not, create a new issue with:    - Clear description    - Steps to reproduce (for bugs)    - Expected vs. actual behavior</p>"},{"location":"CONTRIBUTING/#contributing-code","title":"Contributing Code","text":"<ol> <li>Fork the repository</li> <li>Create a branch: <code>git checkout -b feature/your-feature-name</code></li> <li>Make your changes</li> <li>Test your changes: Ensure all tests pass</li> <li>Commit: Use clear, descriptive commit messages</li> <li>Push: <code>git push origin feature/your-feature-name</code></li> <li>Create a Pull Request</li> </ol>"},{"location":"CONTRIBUTING/#contributing-documentation","title":"Contributing Documentation","text":"<p>Documentation improvements are highly valued!</p> <ul> <li>Tutorial chapters: <code>docs/GRL0/tutorials/</code></li> <li>Implementation guides: <code>docs/GRL0/implementation/</code></li> <li>Examples and notebooks: <code>notebooks/</code></li> </ul> <p>For math-heavy docs: - Use standard LaTeX: <code>$...$</code> for inline, <code>$$...$$</code> for display - Preview locally: <code>mkdocs serve</code> - The documentation site will render math automatically</p>"},{"location":"CONTRIBUTING/#code-style","title":"Code Style","text":"<ul> <li>Python: Follow PEP 8</li> <li>Docstrings: Use Google style</li> <li>Type hints: Preferred for new code</li> <li>Comments: Explain \"why,\" not \"what\"</li> </ul>"},{"location":"CONTRIBUTING/#running-tests","title":"Running Tests","text":"<pre><code>pytest tests/\n</code></pre>"},{"location":"CONTRIBUTING/#building-documentation-locally","title":"Building Documentation Locally","text":"<pre><code>pip install -r requirements-docs.txt\nmkdocs serve\n</code></pre> <p>Then visit <code>http://localhost:8000</code></p>"},{"location":"CONTRIBUTING/#questions","title":"Questions?","text":"<p>Feel free to open an issue for questions or join discussions!</p> <p>Thank you for contributing to GRL! \ud83c\udf89</p>"},{"location":"LICENSE/","title":"License","text":"<p>MIT License</p> <p>Copyright \u00a9 2026</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"ROADMAP/","title":"GRL Research Roadmap","text":"<p>Last Updated: January 14, 2026 Purpose: High-level plan for GRL research, documentation, and implementation</p>"},{"location":"ROADMAP/#current-focus-grl-v0-baseline","title":"Current Focus: GRL v0 (Baseline)","text":"<p>Status: \ud83d\udd04 In Progress \u2014 Documentation &amp; Formalization Goal: Complete tutorial paper and reference implementation of the original GRL framework</p>"},{"location":"ROADMAP/#part-i-particle-based-learning","title":"Part I: Particle-Based Learning","text":"<p>Status: \u2705 7/10 chapters complete</p> Chapter Title Status 00 Overview \u2705 Complete 01 Core Concepts \u2705 Complete 02 RKHS Foundations \u2705 Complete 03 Energy and Fitness \u2705 Complete 04 Reinforcement Field \u2705 Complete 04a Riesz Representer \u2705 Complete 05 Particle Memory \u2705 Complete 06 MemoryUpdate Algorithm \u2705 Complete 07 RF-SARSA Algorithm \u23f3 Next 08 Soft State Transitions \u23f3 Planned 09 POMDP Interpretation \u23f3 Planned 10 Practical Implementation \u23f3 Planned <p>Priority: Complete Chapters 07-10 by February 2026</p>"},{"location":"ROADMAP/#part-ii-emergent-structure-spectral-abstraction","title":"Part II: Emergent Structure &amp; Spectral Abstraction","text":"<p>Status: \ud83c\udfaf Foundation laid, rewrite needed</p> <p>Original Section V topics: - Functional clustering in RKHS - Spectral concept discovery - Hierarchical policy organization - Experience compression</p> <p>New formalization available: - Chapter 5: Concept Projections and Measurements provides mathematical foundation</p> <p>Next Steps: - Rewrite Section V content using concept subspace formalism - Add tutorial chapters on spectral clustering algorithms - Implement concept discovery in code</p> <p>Priority: Start after Part I complete (March 2026)</p>"},{"location":"ROADMAP/#quantum-inspired-extensions","title":"Quantum-Inspired Extensions","text":"<p>Status: \ud83d\udd2c Advanced topics \u2014 9 chapters complete Goal: Explore mathematical connections to QM and novel probability formulations</p>"},{"location":"ROADMAP/#completed-chapters","title":"Completed Chapters","text":"Chapter Title Key Contribution 01 RKHS-Quantum Parallel Structural mapping between RKHS and QM Hilbert spaces 01a Wavefunction Interpretation State vector vs. wavefunction clarity 02 RKHS Basis and Amplitudes Why GRL doesn't need Born rule normalization 03 Complex-Valued RKHS Interference effects, phase semantics 04 Action and State Fields Slicing \\(Q^+\\) into projections 05 Concept Projections Formal subspace theory (foundation for Part II) 06 Agent State &amp; Belief Evolution What \"the state\" is in GRL 07 Learning Beyond GP Alternative learning mechanisms 08 Memory Dynamics Formation, consolidation, retrieval"},{"location":"ROADMAP/#future-directions-from-these-chapters","title":"Future Directions from These Chapters","text":"<p>1. Amplitude-Based Reinforcement Learning \ud83d\udd25 High Priority</p> <p>Motivation: QM's probability amplitude formulation hasn't been applied to RL</p> <p>Proposal: - Complex-valued value functions: \\(Q^+(z) \\in \\mathbb{C}\\) - Policy from Born rule: \\(\\pi(a|s) \\propto |Q^+(s,a)|^2\\) - Phase semantics: temporal, directional, or contextual structure - Interference-based TD updates</p> <p>Next Steps: - Expand Chapter 03 with detailed algorithms - Design toy problems where phase helps - Implement and benchmark</p> <p>Target: Paper submission NeurIPS 2026 or ICML 2027</p> <p>2. Information-Theoretic Memory Consolidation \ud83d\udd25 High Priority</p> <p>Motivation: Replace hard threshold \\(\\tau\\) in MemoryUpdate with principled criteria</p> <p>Proposal: - MDL objective: \\(\\min_{\\Omega'} \\text{TD-error}(Q^+(\\Omega')) + \\lambda |\\Omega'|\\) - Surprise-gated consolidation: store distinct if high TD-error, merge if low - Adaptive top-k neighbors (density-aware)</p> <p>Next Steps: - Implement MDL consolidation algorithm - Compare: hard threshold vs. soft vs. top-k vs. MDL vs. surprise-gating - Meta-learn consolidation parameters</p> <p>Target: Paper submission ICML 2026 or NeurIPS 2027</p> <p>3. Mixture of Experts with Concept-Based Gating</p> <p>Motivation: Multiple local fields for scalability and modularity</p> <p>Proposal: - \\(Q^+(z) = \\sum_m g_m(z) Q_m(z)\\) - Gate by concept activation: \\(g_m(z) \\propto \\|P_{\\mathcal{C}_m} k(z, \\cdot)\\|^2\\) - Each expert specializes on a concept subspace</p> <p>Next Steps: - Connect to Part II concept discovery - Implement hierarchical composition - Benchmark on multi-task environments</p> <p>Target: Part of larger hierarchical RL paper (2027)</p> <p>4. Hybrid Neural-Particle Architecture</p> <p>Motivation: Combine scalability of deep nets with fast adaptation of particles</p> <p>Proposal: - \\(Q^+(z) = Q_\\theta(z) + \\beta \\sum_{i \\in \\text{recent}} w_i k(z_i, z)\\) - Neural net: long-term memory (slow updates) - Particle buffer: short-term memory (fast updates, bounded)</p> <p>Next Steps: - Implement distillation from buffer to network - Large-scale continuous control experiments - Compare to pure GP and pure neural baselines</p> <p>Target: Practical algorithms paper (2027)</p>"},{"location":"ROADMAP/#grl-extensions-papers-a-b-c","title":"GRL Extensions: Papers A, B, C","text":"<p>Status: Paper A ~70% complete, B &amp; C planned Goal: Extend GRL with operator formalism, scalable algorithms, and applications</p>"},{"location":"ROADMAP/#paper-a-theoretical-foundations-operator-framework","title":"Paper A: Theoretical Foundations (Operator Framework)","text":"<p>Status: \ud83d\udfe2 Draft Complete (~6,500 words), Figures in Progress Progress: 70% \u2014 Theory done, figures 43%, proofs 40%, experiments 0%</p> <p>Core Contribution: - Actions as parametric operators \\(\\hat{O}_\\theta\\) (not just parameter vectors) - Operator algebra: composition, Lie groups, hierarchical skills - Generalized Bellman equation with energy regularization - Unification: classical RL as special case</p> <p>Current State:</p> <p>\u2705 Complete: - Unified draft with all sections - 3 critical figures implemented (paradigm, operator families, unification) - Proof outlines - Figure generation framework</p> <p>\u23f3 Remaining Work: - 4 additional figures (energy landscapes, composition, convergence, policy viz) - Expand proofs (Appendices A) - Operator catalog (Appendix B) - Related work section - Validation experiments</p> <p>Timeline: - January-February 2026: Complete all figures, expand appendices - March 2026: Run experiments, related work, final polish - April 2026: Submit to NeurIPS/ICML 2026</p> <p>Location: <code>dev/papers/paper-a-theory/</code></p>"},{"location":"ROADMAP/#paper-b-algorithms-implementation","title":"Paper B: Algorithms &amp; Implementation","text":"<p>Status: \u23f3 Planned Target: ICML/NeurIPS 2026 (Applied Track)</p> <p>Planned Content: - Operator-Actor-Critic (OAC) algorithm - Neural operator architectures (DeepONet, FNO integration) - Training stability techniques - Benchmark results (continuous control, physics tasks) - Ablation studies</p> <p>Dependencies: - Requires Paper A theory finalized - Requires implementation of <code>src/grl/operators/</code> and <code>src/grl/algorithms/</code></p> <p>Timeline: - February-March 2026: Algorithm development - April-May 2026: Benchmarking - June 2026: Draft and submit</p>"},{"location":"ROADMAP/#paper-c-empirical-applications","title":"Paper C: Empirical Applications","text":"<p>Status: \u23f3 Planned Target: CoRL/IROS 2026 (Robotics/Applications)</p> <p>Planned Content: - Real-world robotic manipulation - Fluid control problems - PDE-governed systems - Physics-based environments - Interpretability analysis (energy landscapes, concept activation)</p> <p>Dependencies: - Requires Paper B algorithms - Requires mature implementation</p> <p>Timeline: - April-June 2026: Application experiments - July 2026: Draft and submit (CoRL deadline)</p>"},{"location":"ROADMAP/#implementation-roadmap","title":"Implementation Roadmap","text":"<p>Status: \ud83d\udd04 Basic structure in place, algorithms pending Goal: Reference implementation of GRL v0, classical RL recovery, and modern applications</p>"},{"location":"ROADMAP/#environment-simulation-package","title":"Environment Simulation Package","text":"<p>Vision: Comprehensive environment package supporting: 1. Validation: Classical RL baselines (CartPole, Pendulum, MuJoCo) 2. Strategy: Modern applications (RLHF, prompt optimization, molecule design) 3. Innovation: GRL-native domains (physics simulation, field control)</p> <p>Package Structure:</p> <pre><code>src/grl/envs/\n\u251c\u2500\u2500 validation/              # Tier 1: Reproduce classical RL\n\u2502   \u251c\u2500\u2500 nav2d.py            # 2D navigation (Priority 7) \u2b50\u2b50\n\u2502   \u251c\u2500\u2500 cartpole.py         # DQN validation\n\u2502   \u251c\u2500\u2500 pendulum.py         # SAC validation\n\u2502   \u2514\u2500\u2500 mujoco_envs.py      # Robotics (Ant, Humanoid)\n\u2502\n\u251c\u2500\u2500 strategic/               # Tier 2: Modern RL applications \ud83d\udd25\n\u2502   \u251c\u2500\u2500 llm_finetuning.py   # RLHF for LLMs \u2b50\u2b50\u2b50 HIGHEST PRIORITY\n\u2502   \u251c\u2500\u2500 prompt_opt.py       # Prompt optimization\n\u2502   \u251c\u2500\u2500 molecule_design.py  # Drug discovery\n\u2502   \u2514\u2500\u2500 nas.py              # Neural Architecture Search\n\u2502\n\u251c\u2500\u2500 novel/                   # Tier 3: GRL-native applications\n\u2502   \u251c\u2500\u2500 physics_sim.py      # Force field control\n\u2502   \u251c\u2500\u2500 fluid_control.py    # PDE-governed systems\n\u2502   \u251c\u2500\u2500 image_editing.py    # Parametric transforms\n\u2502   \u2514\u2500\u2500 multi_robot.py      # Multi-agent coordination\n\u2502\n\u251c\u2500\u2500 wrappers/                # Adapters for existing environments\n\u2502   \u251c\u2500\u2500 gym_wrapper.py      # OpenAI Gym \u2192 GRL\n\u2502   \u251c\u2500\u2500 gymnasium_wrapper.py# Gymnasium \u2192 GRL\n\u2502   \u251c\u2500\u2500 dm_control_wrapper.py # DeepMind Control \u2192 GRL\n\u2502   \u2514\u2500\u2500 rlhf_wrapper.py     # TRL/transformers \u2192 GRL\n\u2502\n\u2514\u2500\u2500 scenarios/               # Predefined configurations\n    \u251c\u2500\u2500 paper_scenarios.py   # Original paper scenarios\n    \u251c\u2500\u2500 benchmark_suite.py   # Standard benchmarks\n    \u2514\u2500\u2500 tutorials.py         # Teaching examples\n</code></pre> <p>Key Design Principles: - Wrappers enable GRL on any existing RL environment - Strategic environments target commercially relevant problems - Scenarios provide reproducible experiments</p>"},{"location":"ROADMAP/#implementation-roadmap_1","title":"Implementation Roadmap","text":""},{"location":"ROADMAP/#phase-1-grl-v0-baseline-current","title":"Phase 1: GRL v0 Baseline (Current)","text":"<p>Target: March 2026</p> <p>Modules:</p> Module Status Description <code>grl/kernels/</code> \u2705 Kernel functions (RBF, Matern, etc.) <code>grl/particles/</code> \ud83d\udd04 Particle memory management <code>grl/fields/</code> \ud83d\udd04 Reinforcement field computation <code>grl/algorithms/memory_update.py</code> \u23f3 MemoryUpdate algorithm <code>grl/algorithms/rf_sarsa.py</code> \u23f3 RF-SARSA algorithm <code>grl/envs/</code> \ud83d\udd04 Test environments <p>Priority Tasks: 1. Complete MemoryUpdate implementation 2. Implement RF-SARSA 3. Add sparse GP variants 4. Create tutorial notebooks</p>"},{"location":"ROADMAP/#phase-2-scalable-learning-march-april-2026","title":"Phase 2: Scalable Learning (March-April 2026)","text":"<p>Goal: Implement alternative learning mechanisms from Chapter 07</p> <p>Tasks: - Kernel ridge regression - Online SGD on weights - Sparse methods (LASSO, inducing points) - Hybrid (neural + particle)</p>"},{"location":"ROADMAP/#phase-3-memory-dynamics-april-may-2026","title":"Phase 3: Memory Dynamics (April-May 2026)","text":"<p>Goal: Implement principled memory consolidation from Chapter 08</p> <p>Tasks: - Soft association (no hard threshold) - Top-k adaptive neighbors - MDL consolidation - Surprise-gated formation - Memory type tags (factual/experiential/working)</p>"},{"location":"ROADMAP/#phase-4-operator-framework-may-july-2026","title":"Phase 4: Operator Framework (May-July 2026)","text":"<p>Goal: Implement Paper A operator formalism</p> <p>Tasks: - Operator base classes - Operator families (affine, field, kernel, neural) - Composition and algebra - Operator-Actor-Critic (OAC)</p>"},{"location":"ROADMAP/#phase-5-applications-august-2026","title":"Phase 5: Applications (August 2026+)","text":"<p>Goal: Benchmarks and real-world demos</p> <p>Tasks: - Continuous control tasks - Physics-based simulations - Robotic manipulation (if hardware available) - Transfer learning experiments</p>"},{"location":"ROADMAP/#documentation-structure","title":"Documentation Structure","text":"<p>Goal: Multi-level documentation for different audiences</p>"},{"location":"ROADMAP/#public-documentation-docs","title":"Public Documentation (<code>docs/</code>)","text":"<p>Tutorial Papers: - <code>docs/GRL0/tutorials/</code> \u2014 Part I: Particle-Based Learning - <code>docs/GRL0/quantum_inspired/</code> \u2014 Advanced topics (QM connections) - <code>docs/GRL0/paper/</code> \u2014 Suggested edits for original paper - <code>docs/GRL0/implementation/</code> \u2014 Implementation notes</p> <p>Future: - <code>docs/theory/</code> \u2014 Operator formalism theory (from Paper A) - <code>docs/algorithms/</code> \u2014 Training algorithms (from Paper B) - <code>docs/tutorials/</code> \u2014 Quick start guides</p>"},{"location":"ROADMAP/#private-development-dev","title":"Private Development (<code>dev/</code>)","text":"<p>Current Work: - <code>dev/GRL0/</code> \u2014 Private notes for GRL v0 development - <code>dev/papers/</code> \u2014 Paper drafts (A, B, C) - <code>dev/GRL_extensions/</code> \u2014 Extension ideas - <code>dev/references/</code> \u2014 Original paper, related papers</p>"},{"location":"ROADMAP/#code-documentation","title":"Code Documentation","text":"<ul> <li>README files in <code>src/grl/</code> subdirectories</li> <li>Docstrings in code (NumPy style)</li> <li>Tutorial notebooks in <code>notebooks/</code></li> <li>API reference (Sphinx, future)</li> </ul>"},{"location":"ROADMAP/#research-themes-connections","title":"Research Themes &amp; Connections","text":""},{"location":"ROADMAP/#theme-1-functional-learning","title":"Theme 1: Functional Learning","text":"<p>Across all work: - State as function \\(Q^+ \\in \\mathcal{H}_k\\) - Operators on function spaces - RKHS as mathematical foundation</p> <p>Papers: GRL v0, Paper A, quantum-inspired extensions</p>"},{"location":"ROADMAP/#theme-2-particle-based-inference","title":"Theme 2: Particle-Based Inference","text":"<p>Key insight: Weighted particles as basis for belief state</p> <p>Papers: GRL v0, memory dynamics (Chapter 08)</p> <p>Extensions: - Sparse approximations - Hierarchical particles - Nonparametric clustering</p>"},{"location":"ROADMAP/#theme-3-energy-based-learning","title":"Theme 3: Energy-Based Learning","text":"<p>Key insight: Energy function \\(E(z) = -Q^+(z)\\) connects to physics</p> <p>Papers: GRL v0 (Chapter 03), Paper A (least action principle)</p> <p>Extensions: - Hamilton-Jacobi-Bellman PDEs - Conservative vector fields - Lagrangian mechanics for policy</p>"},{"location":"ROADMAP/#theme-4-hierarchical-abstraction","title":"Theme 4: Hierarchical Abstraction","text":"<p>Key insight: Concepts as subspaces in function space</p> <p>Papers: GRL v0 Part II, concept projections (Chapter 05), MoE (Chapter 07)</p> <p>Extensions: - Multi-scale representations - Transfer learning via shared basis - Compositional behaviors</p>"},{"location":"ROADMAP/#theme-5-quantum-inspired-probability","title":"Theme 5: Quantum-Inspired Probability","text":"<p>Key insight: Amplitude-based learning with phase and interference</p> <p>Papers: Quantum-inspired chapters (01-04), potential standalone paper</p> <p>Extensions: - Complex RKHS for RL - Born rule for action selection - Spectral methods for concept discovery</p>"},{"location":"ROADMAP/#strategic-applications-demonstrating-grls-generality","title":"Strategic Applications: Demonstrating GRL's Generality","text":"<p>Goal: Show that GRL subsumes classical RL and applies to modern, commercially relevant problems.</p>"},{"location":"ROADMAP/#application-1-recovering-classical-rl-critical-for-adoption","title":"Application 1: Recovering Classical RL \ud83d\udd25 Critical for Adoption","text":"<p>Motivation: Researchers trust frameworks that generalize what they already know.</p> <p>Objective: Demonstrate that Q-learning, DQN, PPO, SAC, RLHF are special cases of GRL.</p> <p>Deliverables: - Document: Recovering Classical RL from GRL \u2705 Complete - Implementation: Wrappers for Gym/Gymnasium environments - Validation: Reproduce classical results (\u00b15% performance)   - Q-learning on GridWorld   - DQN on CartPole   - SAC on Pendulum   - PPO on continuous control</p> <p>Timeline: Q2 2026</p> <p>Impact:  - Convinces classical RL researchers GRL is not alien - Provides clear migration path from classical to GRL - Enables GRL to leverage existing benchmarks</p>"},{"location":"ROADMAP/#application-2-rlhf-for-llms-theoretical-connection-future-direction","title":"Application 2: RLHF for LLMs (Theoretical Connection + Future Direction)","text":"<p>Status: Theoretical connection established, implementation exploratory</p> <p>Why This Matters: - Validation: RLHF (ChatGPT, Claude, Llama) is most commercially important RL application - Familiarity: Most ML researchers understand this problem - Generality: If GRL generalizes RLHF theoretically, it validates framework's breadth</p> <p>Theoretical Formulation: - State: \\(s_t\\) = (prompt, partial response) - Action: \\(\\theta_t\\) = token ID (discrete action space) - Augmented space: \\((s_t, \\theta_t)\\) - Field: \\(Q^+(s_t, \\theta_t)\\) = expected reward for token \\(\\theta_t\\) in context \\(s_t\\) - Key insight: Standard RLHF (PPO) is GRL with discrete actions + neural network approximation</p> <p>Documentation: - Recovering Classical RL from GRL \u2014 Section 6 covers RLHF</p> <p>Potential Advantages (Theoretical): 1. Off-policy learning (replay buffer of human feedback) 2. Kernel generalization (transfer across prompts) 3. Uncertainty quantification (exploration where uncertain) 4. Interpretability (energy landscapes)</p> <p>However: These are speculative without empirical validation.</p> <p>Implementation Reality:</p> <p>Challenges: - Infrastructure complexity (reward model, human feedback data) - Computational cost (expensive even for GPT-2) - Integration with existing tools (TRL, transformers, accelerate) - Validation difficulty (matching PPO requires extensive tuning)</p> <p>Estimated Effort: 6-12 months of focused work with GPU resources</p> <p>When to Pursue: - \u2705 After GRL validated on simpler environments - \u2705 If collaborators or funding available - \u2705 If clear path to demonstrating advantages</p> <p>Realistic Alternative: - Write theoretical articles justifying the connection - Toy RLHF-like problem (not real LLM) as proof-of-concept - Wait for opportunities (industry collaboration, research grant)</p>"},{"location":"ROADMAP/#application-3-additional-modern-rl-domains","title":"Application 3: Additional Modern RL Domains","text":"<p>Prompt Optimization: - Parametric prompt generation (continuous in embedding space) - GRL learns smooth prompt space - Transfer across tasks</p> <p>Molecule Design: - Parametric molecular operators - GRL discovers optimal molecules for drug properties - Physics-informed kernels</p> <p>Neural Architecture Search: - Compositional architecture operators - GRL explores architecture space efficiently - Uncertainty-guided search</p> <p>Timeline: Q4 2026+</p>"},{"location":"ROADMAP/#potential-novel-contributions-publishable","title":"Potential Novel Contributions (Publishable)","text":""},{"location":"ROADMAP/#high-priority-contributions","title":"High-Priority Contributions","text":"<p>1. Amplitude-Based Reinforcement Learning \ud83d\udd25 Top Priority - Novelty: First RL with complex-valued value functions - Venue: NeurIPS/ICML 2026-2027 - Readiness: 30% (theory done, needs implementation)</p> <p>2. Information-Theoretic Memory Consolidation - Novelty: MDL framework for experience replay - Venue: ICML/NeurIPS 2026-2027 - Readiness: 40% (formulation clear, needs experiments)</p> <p>3. Operator-Based GRL (Paper A) - Novelty: Actions as operators, not symbols - Venue: NeurIPS/ICML 2026 - Readiness: 70% (draft complete, figures/experiments needed)</p>"},{"location":"ROADMAP/#medium-priority-contributions","title":"Medium-Priority Contributions","text":"<p>4. Theoretical Articles: Modern RL as Special Cases of GRL - Novelty: Justify that RLHF, prompt optimization, NAS, molecule design are GRL special cases - Venue: Blog posts, workshop papers, or sections in main papers - Readiness: 50% (\"Recovering Classical RL\" document provides template) - Impact: Demonstrates GRL's generality without requiring full implementations</p> <p>5. Concept Subspaces for Hierarchical RL - Novelty: Rigorous RKHS subspace formalism - Venue: ICLR/AISTATS 2027 - Readiness: 50% (math done, algorithms needed)</p> <p>6. Surprise-Modulated Episodic Memory - Novelty: Bio-inspired consolidation - Venue: CogSci/Neural Computation 2027 - Readiness: 60% (theory clear, needs validation)</p> <p>7. Hybrid Neural-Particle RL - Novelty: Combining deep learning with GP memory - Venue: ICLR/ICML 2027 - Readiness: 30% (concept clear, full implementation needed)</p>"},{"location":"ROADMAP/#strategic-applications-future-possibilities-no-timeline","title":"Strategic Applications (Future Possibilities, No Timeline)","text":"<p>8. GRL for LLM Fine-tuning (RLHF) - Novelty: Application of functional RL to most commercially important RL problem - Venue: ICLR/NeurIPS (if pursued) - Readiness: 20% (theoretical connection clear, implementation requires major resources) - Status: Exploratory \u2014 pursue only if collaborators/funding available - Alternative: Write theoretical articles + toy proof-of-concept</p> <p>9. Other Strategic Applications - Prompt optimization, molecule design, neural architecture search - Status: Theoretical connections to be documented - Implementation: Pick 1-2 if resources available - Primary Value: Demonstrate GRL generalizes modern RL methods</p>"},{"location":"ROADMAP/#timeline-summary","title":"Timeline Summary","text":""},{"location":"ROADMAP/#2026-q1-january-march","title":"2026 Q1 (January-March)","text":"<p>Focus: Complete GRL v0 documentation and baseline implementation</p> <ul> <li>\u2705 Finish Part I tutorial chapters (07-10)</li> <li>\u2705 Implement MemoryUpdate and RF-SARSA</li> <li>\ud83d\udd04 Run first experiments</li> <li>\ud83d\udd04 Complete Paper A figures and proofs</li> </ul>"},{"location":"ROADMAP/#2026-q2-april-june","title":"2026 Q2 (April-June)","text":"<p>Focus: Paper A submission, Classical RL recovery, scalable algorithms</p> <ul> <li>Submit Paper A (April deadline)</li> <li>Implement wrappers for Gym/Gymnasium (recover classical RL)</li> <li>Reproduce DQN on CartPole (validation)</li> <li>Reproduce SAC on Pendulum (validation)</li> <li>Document: \"Recovering Classical RL from GRL\" \ud83d\udd25 Strategic</li> <li>Implement alternative learning mechanisms (Chapter 07)</li> <li>Implement memory dynamics (Chapter 08)</li> <li>Draft Paper B algorithms</li> <li>Start benchmark experiments</li> </ul>"},{"location":"ROADMAP/#2026-q3-july-september","title":"2026 Q3 (July-September)","text":"<p>Focus: Paper B submission, novel contributions, extensions</p> <ul> <li>Submit Paper B (June ICML or September NeurIPS)</li> <li>Explore amplitude-based RL (if promising after Part I complete)</li> <li>Implement MDL consolidation (principled memory dynamics)</li> <li>Concept-based MoE (mixture of experts via subspaces)</li> <li>Start operator framework implementation</li> <li>Run application experiments for Paper C</li> <li>Write theoretical articles: How RLHF/prompt-opt/NAS are special cases of GRL</li> </ul>"},{"location":"ROADMAP/#2026-q4-october-december","title":"2026 Q4 (October-December)","text":"<p>Focus: Paper C submission, novel contributions (amplitude/MDL)</p> <ul> <li>Submit Paper C (CoRL deadline ~July)</li> <li>Develop amplitude-based RL fully</li> <li>Implement MDL consolidation</li> <li>Draft standalone papers on extensions</li> </ul>"},{"location":"ROADMAP/#2027","title":"2027+","text":"<p>Focus: Consolidate results, broader impact</p> <ul> <li>Package releases and documentation</li> <li>Workshop papers and tutorials</li> <li>Integration with popular RL libraries</li> <li>Real-world applications</li> </ul>"},{"location":"ROADMAP/#success-metrics","title":"Success Metrics","text":""},{"location":"ROADMAP/#short-term-6-months","title":"Short-Term (6 months)","text":"<ul> <li> Complete GRL v0 tutorial paper (Parts I &amp; II)</li> <li> Reference implementation working on 3+ environments</li> <li> Submit Paper A to top venue</li> <li> At least 10 GitHub stars</li> </ul>"},{"location":"ROADMAP/#medium-term-12-months","title":"Medium-Term (12 months)","text":"<ul> <li> Paper A accepted or under review</li> <li> Papers B &amp; C submitted</li> <li> 2-3 additional papers on extensions (amplitude, MDL, concepts)</li> <li> 50+ GitHub stars, some external users</li> </ul>"},{"location":"ROADMAP/#long-term-24-months","title":"Long-Term (24+ months)","text":"<ul> <li> 3+ papers published at top venues</li> <li> GRL adopted by other researchers</li> <li> Integration with popular libraries (Stable-Baselines3, RLlib)</li> <li> Tutorial at major conference (NeurIPS, ICML)</li> <li> Real-world deployment (robotics, control systems)</li> </ul>"},{"location":"ROADMAP/#open-questions-research-opportunities","title":"Open Questions &amp; Research Opportunities","text":""},{"location":"ROADMAP/#theoretical-questions","title":"Theoretical Questions","text":"<ol> <li>Sample complexity: How does GRL compare to classical RL theoretically?</li> <li>Convergence rates: Can we prove faster convergence in certain settings?</li> <li>Operator algebra: What's the right group structure for operator composition?</li> <li>Phase semantics: What should complex phase represent in amplitude-based RL?</li> </ol>"},{"location":"ROADMAP/#algorithmic-questions","title":"Algorithmic Questions","text":"<ol> <li>Scalability: Best way to handle millions of particles?</li> <li>Consolidation criterion: MDL vs. surprise-gating vs. other?</li> <li>Mixture of experts: How to partition concept subspaces automatically?</li> <li>Transfer learning: Can concept basis enable zero-shot transfer?</li> </ol>"},{"location":"ROADMAP/#application-questions","title":"Application Questions","text":"<ol> <li>Best domains: Where does GRL shine vs. classical RL?</li> <li>Interpretability: Can energy landscapes help explain decisions?</li> <li>Safety: Can concept subspaces encode constraints?</li> <li>Multi-agent: How to extend GRL to multi-agent settings?</li> </ol>"},{"location":"ROADMAP/#resources-references","title":"Resources &amp; References","text":""},{"location":"ROADMAP/#key-papers-original-work","title":"Key Papers (Original Work)","text":"<ul> <li>Chiu &amp; Huber (2022). Generalized Reinforcement Learning. arXiv:2208.04822.</li> </ul>"},{"location":"ROADMAP/#inspirations","title":"Inspirations","text":"<p>Kernel Methods: - Rasmussen &amp; Williams (2006). Gaussian Processes for Machine Learning.</p> <p>Operator Learning: - Lu et al. (2021). Learning Nonlinear Operators via DeepONet. Nature Machine Intelligence. - Li et al. (2021). Fourier Neural Operator. ICLR.</p> <p>Quantum-Inspired ML: - Cheng et al. (2018). Quantum Generative Adversarial Learning. PRL. - Havl\u00ed\u010dek et al. (2019). Supervised Learning with Quantum-Enhanced Feature Spaces. Nature.</p> <p>Memory &amp; Agent Systems: - Cao et al. (2024). Memory in the Age of AI Agents. arXiv:2512.13564.</p>"},{"location":"ROADMAP/#contact-collaboration","title":"Contact &amp; Collaboration","text":"<p>Documentation: docs/ Code: src/grl/ Papers: dev/papers/ Issues: GitHub Issues (coming soon)</p> <p>This roadmap is a living document and will be updated as research progresses.</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/","title":"Tutorial Papers: Reinforcement Fields","text":"<p>Format: Two-Part Tutorial Series + Quantum-Inspired Extensions Status: Part I in progress (8/10 chapters), Extensions (9 chapters complete) Goal: Comprehensive, accessible introduction to particle-based functional reinforcement learning</p> <p>Based on: Chiu &amp; Huber (2022). Generalized Reinforcement Learning: Experience Particles, Action Operator, Reinforcement Field, Memory Association, and Decision Concepts. arXiv:2208.04822</p> <p>See also: Research Roadmap | Quantum-Inspired Extensions</p>"},{"location":"GRL0/#part-i-particle-based-learning","title":"Part I: Particle-Based Learning","text":"<p>Core Topics: - Functional fields over augmented state-action space - Particle memory as belief state in RKHS - MemoryUpdate and RF-SARSA algorithms - Emergent soft state transitions and POMDP interpretation</p>"},{"location":"GRL0/#tutorial-chapters","title":"Tutorial Chapters","text":"Section Chapters Status Topics Foundations 0, 1, 2, 3, 3a \u2705 Complete Augmented space, particles, RKHS, energy, least action principle Field &amp; Memory 4, 4a, 5, 6, 6a \u2705 Complete Functional fields, Riesz theorem, belief representation, MemoryUpdate, advanced memory Algorithms 7 \u2705 Complete RF-SARSA, functional TD, two-layer learning Interpretation 8, 9, 10 \u23f3 Next Soft transitions, POMDP, synthesis <p>Start Here: Chapter 0 \u2192</p>"},{"location":"GRL0/#key-theoretical-innovations","title":"Key Theoretical Innovations","text":""},{"location":"GRL0/#1-quantum-inspired-probability-formulation","title":"1. Quantum-Inspired Probability Formulation","text":"<p>Novel to mainstream ML: GRL introduces probability amplitudes rather than direct probabilities:</p> <ul> <li>RKHS inner products as amplitudes: \\(\\langle \\psi | \\phi \\rangle\\) \u2192 probabilities via \\(|\\langle \\psi | \\phi \\rangle|^2\\)</li> <li>Complex-valued RKHS: Enables interference effects and phase semantics</li> <li>Superposition of particle states: Multi-modal distributions as weighted sums</li> <li>Emergent probabilities: Policy derived from field values, not optimized directly</li> </ul> <p>This formulation\u2014common in quantum mechanics but rare in ML\u2014opens new directions for:</p> <ul> <li>Interference-based learning dynamics</li> <li>Phase-encoded contextual information</li> <li>Richer uncertainty representations</li> <li>Novel spectral methods (Part II)</li> </ul>"},{"location":"GRL0/#2-functional-representation-of-experience","title":"2. Functional Representation of Experience","text":"<p>Experience is not discrete transitions but a continuous field in RKHS:</p> <ul> <li>Particles are basis states in functional space</li> <li>Value functions are kernel superpositions (not neural network outputs)</li> <li>Policy inference from energy landscape navigation (not gradient-based optimization)</li> </ul>"},{"location":"GRL0/#part-ii-emergent-structure-spectral-abstraction","title":"Part II: Emergent Structure &amp; Spectral Abstraction","text":"<p>Status: \ud83d\udccb Planned (begins after Part I)</p> <p>Core Topics: - Functional clustering (clustering functions, not points) - Spectral methods on kernel matrices - Concepts as coherent subspaces of the reinforcement field - Hierarchical policy organization</p>"},{"location":"GRL0/#planned-topics","title":"Planned Topics","text":"Section Chapters Topics Functional Clustering 11 Clustering in RKHS function space Spectral Discovery 12 Spectral methods, eigenspaces Hierarchical Concepts 13 Multi-level abstractions Structured Control 14 Concept-driven policies <p>Based on: Section V of the original paper (Chiu &amp; Huber, 2022)</p>"},{"location":"GRL0/#quantum-inspired-extensions","title":"Quantum-Inspired Extensions","text":"<p>Status: \ud83d\udd2c Advanced topics (9 chapters complete) Goal: Explore mathematical connections to quantum mechanics and novel probability formulations</p> <p>Explore Extensions \u2192</p>"},{"location":"GRL0/#completed-chapters","title":"Completed Chapters","text":"Theme Chapters Topics Foundations 01, 01a, 02 RKHS-QM structural parallel, state vs. wavefunction, amplitude interpretation Complex RKHS 03, 09 Complex-valued kernels, interference effects, Feynman path integrals Projections 04, 05, 06 Action/state fields, concept subspaces (foundation for Part II), belief dynamics Learning &amp; Memory 07, 08 Alternative learning mechanisms, principled memory consolidation"},{"location":"GRL0/#key-novel-contributions","title":"Key Novel Contributions","text":"<p>1. Amplitude-Based Reinforcement Learning - Complex-valued value functions with Born rule policies - Phase semantics for temporal/contextual information - Novel to mainstream ML, potential standalone paper</p> <p>2. Information-Theoretic Memory Consolidation - MDL framework replacing hard threshold \\(\\tau\\) - Surprise-gated formation and consolidation - Principled criteria for what to retain/forget</p> <p>3. Concept-Based Mixture of Experts - Hierarchical RL via concept subspace projections - Gating by concept activation - Multi-scale representation and transfer learning</p>"},{"location":"GRL0/#additional-resources","title":"Additional Resources","text":""},{"location":"GRL0/#implementation","title":"Implementation","text":"<p>Technical specifications and roadmap for the codebase:</p> <ul> <li>System architecture</li> <li>Module specifications</li> <li>Implementation priorities</li> <li>Validation plan</li> </ul>"},{"location":"GRL0/#paper-revisions","title":"Paper Revisions","text":"<p>Suggested edits and improvements for the original GRL-v0 paper.</p>"},{"location":"GRL0/#reading-paths","title":"Reading Paths","text":""},{"location":"GRL0/#quick-start-2-hours","title":"Quick Start (2 hours)","text":"<p>Start here if you want a high-level overview:</p> <ul> <li>Ch. 0: Overview</li> <li>Ch. 1: Core Concepts</li> </ul>"},{"location":"GRL0/#part-i-complete-8-hours","title":"Part I Complete (8 hours)","text":"<p>For full understanding of particle-based learning:</p> <ul> <li>Chapters 0-10 (sequential reading)</li> </ul>"},{"location":"GRL0/#part-ii-complete-4-hours-when-available","title":"Part II Complete (4 hours, when available)","text":"<p>For hierarchical structure and abstraction:</p> <ul> <li>Chapters 11-14 (sequential reading)</li> </ul>"},{"location":"GRL0/#quantum-inspired-extensions-6-hours","title":"Quantum-Inspired Extensions (6 hours)","text":"<p>For advanced mathematical connections:</p> <ul> <li>Quantum-inspired series (Chapters 01-08)</li> <li>Requires: Part I Chapters 2, 4, 5</li> </ul>"},{"location":"GRL0/#implementation-focus","title":"Implementation Focus","text":"<p>If you want to build GRL systems:</p> <ul> <li>Implementation roadmap</li> <li>Chapters 5-7 (algorithms)</li> <li>Quantum-inspired Chapters 07-08 (learning &amp; memory)</li> </ul>"},{"location":"GRL0/#theory-deep-dive","title":"Theory Deep-Dive","text":"<p>If you want mathematical depth:</p> <ul> <li>Chapters 2-3 (RKHS foundations)</li> <li>Chapters 4-5 (field theory)</li> <li>Quantum-inspired Chapters 01-03 (QM connections)</li> <li>Chapters 11-12 (spectral methods, when available)</li> </ul>"},{"location":"GRL0/#why-two-parts","title":"Why Two Parts?","text":"<p>The original GRL paper introduced two major innovations:</p> <ol> <li>Reinforcement Fields (Part I): Replacing discrete experience replay with a continuous particle-based belief state in RKHS</li> <li>Concept-Driven Learning (Part II): Discovering abstract structure through spectral clustering in function space</li> </ol> <p>Each innovation is substantial enough for its own comprehensive treatment, yet they build on shared foundations (RKHS, particles, functional reasoning).</p>"},{"location":"GRL0/#what-makes-grl-different","title":"What Makes GRL Different","text":"Traditional RL Reinforcement Fields (Part I) + Spectral Abstraction (Part II) Experience replay buffer Particle-based belief state + Functional clustering Discrete transitions Continuous energy landscape + Spectral concept discovery Policy optimization Policy inference from field + Hierarchical abstractions Fixed representation Kernel-induced functional space + Emergent structure"},{"location":"GRL0/#key-terminology","title":"Key Terminology","text":"Term Meaning Augmented Space Joint state-action parameter space \\(z = (s, \\theta)\\) Particle Experience point \\((z_i, w_i)\\) with location and weight Reinforcement Field Functional gradient field induced by scalar energy in RKHS Energy Functional Scalar field \\(E: \\mathcal{Z} \\to \\mathbb{R}\\) over augmented space MemoryUpdate Belief-state transition operator RF-SARSA Two-layer TD learning (primitive + GP field) Functional Clustering Clustering in RKHS based on behavior similarity Spectral Concepts Coherent subspaces discovered via eigendecomposition"},{"location":"GRL0/#directory-structure","title":"Directory Structure","text":"<pre><code>docs/GRL0/\n\u251c\u2500\u2500 README.md                 # This file\n\u251c\u2500\u2500 tutorials/                # Tutorial chapters (Parts I &amp; II)\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u251c\u2500\u2500 00-overview.md\n\u2502   \u251c\u2500\u2500 01-core-concepts.md\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 [future chapters 11-14]\n\u251c\u2500\u2500 paper/                    # Paper-ready sections and revisions\n\u2502   \u251c\u2500\u2500 README.md\n\u2502   \u2514\u2500\u2500 [section drafts]\n\u2514\u2500\u2500 implementation/           # Implementation specifications\n    \u251c\u2500\u2500 README.md\n    \u2514\u2500\u2500 [technical specs]\n</code></pre>"},{"location":"GRL0/#contributing","title":"Contributing","text":"<p>When adding content:</p> <ol> <li>Follow the tutorial narrative style \u2014 Build intuition, then formalism</li> <li>Make chapters self-contained \u2014 Readers may skip around</li> <li>Use consistent notation \u2014 See Ch. 0 for conventions</li> <li>Connect to implementation \u2014 Theory serves practice</li> <li>Distinguish Part I vs II \u2014 Part I = particle dynamics, Part II = emergent structure</li> </ol>"},{"location":"GRL0/#original-publication","title":"Original Publication","text":"<p>This tutorial series provides enhanced exposition of the work originally published as:</p> <p>Chiu, P.-H., &amp; Huber, M. (2022). Generalized Reinforcement Learning: Experience Particles, Action Operator, Reinforcement Field, Memory Association, and Decision Concepts. arXiv preprint arXiv:2208.04822.</p> <p>Read on arXiv \u2192 (37 pages, 15 figures)</p> <pre><code>@article{chiu2022generalized,\n  title={Generalized Reinforcement Learning: Experience Particles, Action Operator, \n         Reinforcement Field, Memory Association, and Decision Concepts},\n  author={Chiu, Po-Hsiang and Huber, Manfred},\n  journal={arXiv preprint arXiv:2208.04822},\n  year={2022}\n}\n</code></pre> <p>Last Updated: January 14, 2026 Next: Chapter 7 (RF-SARSA Algorithm) See also: Research Roadmap for comprehensive plan and timeline</p>"},{"location":"GRL0/recovering_classical_rl/","title":"Recovering Classical RL from GRL","text":"<p>Purpose: Demonstrate that traditional RL algorithms are special cases of GRL Audience: Classical RL researchers, practitioners, skeptics Goal: Bridge the gap between familiar methods and the GRL framework</p>"},{"location":"GRL0/recovering_classical_rl/#executive-summary","title":"Executive Summary","text":"<p>Key Claim: Generalized Reinforcement Learning (GRL) is not a replacement for classical RL\u2014it's a unifying framework that recovers existing methods as special cases while enabling new capabilities.</p> <p>Why This Matters:</p> <ul> <li>Adoption: Researchers trust frameworks that subsume what they already know</li> <li>Validation: If GRL recovers DQN/PPO/SAC, it must be correct</li> <li>Innovation: Once the connection is clear, extensions become natural</li> </ul> <p>What You'll Learn:</p> <ol> <li>How Q-learning emerges from GRL with discrete actions</li> <li>How DQN is GRL with neural network approximation</li> <li>How Policy Gradients (REINFORCE) follow from the energy landscape</li> <li>How Actor-Critic (PPO, SAC) naturally arise in GRL</li> <li>How RLHF for LLMs is GRL applied to language modeling</li> </ol>"},{"location":"GRL0/recovering_classical_rl/#table-of-contents","title":"Table of Contents","text":"<ol> <li>The GRL\u2192Classical RL Dictionary</li> <li>Recovery 1: Q-Learning</li> <li>Recovery 2: DQN (Deep Q-Network)</li> <li>Recovery 3: REINFORCE (Policy Gradient)</li> <li>Recovery 4: Actor-Critic (PPO, SAC)</li> <li>Recovery 5: RLHF for LLMs</li> <li>What GRL Adds Beyond Classical RL</li> <li>Implementation: From GRL to Classical</li> </ol>"},{"location":"GRL0/recovering_classical_rl/#1-the-grlclassical-rl-dictionary","title":"1. The GRL\u2192Classical RL Dictionary","text":"Classical RL Concept GRL Equivalent Notes State \\(s\\) State \\(s\\) Same Discrete Action \\(a \\in \\mathcal{A}\\) Fixed parametric mapping \\(\\theta_a\\) One \\(\\theta\\) per discrete action Continuous Action \\(a \\in \\mathbb{R}^d\\) Action parameters \\(\\theta \\in \\mathbb{R}^d\\) Direct correspondence Q-function \\(Q(s, a)\\) Reinforcement field \\(Q^+(s, \\theta)\\) Evaluated at discrete \\(\\theta_a\\) Replay Buffer \\(\\mathcal{D}\\) Particle Memory \\(\\Omega\\) Particles are weighted experiences Experience \\((s, a, r, s')\\) Particle \\((z, w)\\) where \\(z=(s,\\theta)\\), \\(w=r\\) Single transition TD Target \\(y = r + \\gamma \\max_{a'} Q(s', a')\\) MemoryUpdate with TD target Belief transition Policy \\(\\pi(a\\|s)\\) Boltzmann over \\(Q^+(s, \\cdot)\\) Temperature-controlled sampling Value Function \\(V(s)\\) \\(\\max_\\theta Q^+(s, \\theta)\\) Maximum over action parameters Exploration \\(\\epsilon\\)-greedy Temperature \\(\\beta\\) in Boltzmann Smooth instead of discrete <p>Key Insight: Classical RL is GRL with:</p> <ul> <li>Discrete or fixed action spaces</li> <li>Tabular or neural network approximation of the field</li> <li>Specific choices of update rules</li> </ul> <p></p>"},{"location":"GRL0/recovering_classical_rl/#2-recovery-1-q-learning","title":"2. Recovery 1: Q-Learning","text":""},{"location":"GRL0/recovering_classical_rl/#classical-q-learning","title":"Classical Q-Learning","text":"<p>Setup:</p> <ul> <li>State space: \\(\\mathcal{S}\\)</li> <li>Action space: \\(\\mathcal{A} = \\{a_1, \\ldots, a_K\\}\\) (discrete, finite)</li> <li>Q-function: \\(Q(s, a)\\) for each \\((s, a)\\) pair</li> </ul> <p>Update Rule: $\\(Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]\\)$</p> <p>Policy: \\(\\epsilon\\)-greedy or softmax over \\(Q(s, \\cdot)\\)</p>"},{"location":"GRL0/recovering_classical_rl/#grl-version","title":"GRL Version","text":"<p>Setup:</p> <ul> <li>State space: \\(\\mathcal{S}\\) (same)</li> <li>Action space: \\(\\mathcal{A} = \\{a_1, \\ldots, a_K\\}\\) (discrete)</li> <li>Map each discrete action to a parameter: \\(\\theta_1, \\ldots, \\theta_K\\) (fixed)</li> <li>Augmented space: \\(\\mathcal{Z} = \\mathcal{S} \\times \\{\\theta_1, \\ldots, \\theta_K\\}\\)</li> <li>Reinforcement field: \\(Q^+(s, \\theta_i)\\) evaluated only at discrete points \\(\\theta_i\\)</li> </ul> <p>Particle Memory:</p> <ul> <li>Each experience \\((s, a_i, r, s')\\) creates particle \\((z_i, w_i)\\) where \\(z_i = (s, \\theta_i)\\), \\(w_i = r\\)</li> </ul> <p>MemoryUpdate:</p> <ul> <li>Add particle \\((z_t, w_t)\\) where \\(z_t = (s_t, \\theta_{a_t})\\), \\(w_t = r_t\\)</li> <li>With no kernel association (set \\(k(z, z') = \\delta(z, z')\\)), MemoryUpdate reduces to:   $\\(Q^+(s, \\theta_a) \\leftarrow Q^+(s, \\theta_a) + \\alpha [y_t - Q^+(s, \\theta_a)]\\)$   where \\(y_t = r_t + \\gamma \\max_{a'} Q^+(s', \\theta_{a'})\\)</li> </ul> <p>This is exactly Q-learning!</p>"},{"location":"GRL0/recovering_classical_rl/#key-takeaways","title":"Key Takeaways","text":"<p>Q-learning is GRL with:</p> <ol> <li>Discrete action space (finite \\(\\{\\theta_i\\}\\))</li> <li>Delta kernel (no generalization between actions)</li> <li>Tabular representation (store \\(Q\\) for each state-action pair)</li> </ol> <p>What GRL adds:</p> <ul> <li>Generalization via non-trivial kernels: \\(k((s, \\theta_i), (s, \\theta_j)) &gt; 0\\) for \\(i \\neq j\\)</li> <li>Continuous interpolation: \\(Q^+(s, \\theta)\\) defined for all \\(\\theta\\), not just discrete actions</li> <li>Weighted particles: Experience importance via \\(w_i\\)</li> </ul> <p></p>"},{"location":"GRL0/recovering_classical_rl/#3-recovery-2-dqn-deep-q-network","title":"3. Recovery 2: DQN (Deep Q-Network)","text":""},{"location":"GRL0/recovering_classical_rl/#classical-dqn","title":"Classical DQN","text":"<p>Setup:</p> <ul> <li>Q-function approximated by neural network: \\(Q_\\psi(s, a)\\)</li> <li>Experience replay buffer: \\(\\mathcal{D} = \\{(s_i, a_i, r_i, s_i')\\}\\)</li> <li>Target network: \\(Q_{\\psi^-}\\) (delayed copy)</li> </ul> <p>Update Rule: $\\(\\psi \\leftarrow \\psi - \\eta \\nabla_\\psi \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} [(Q_\\psi(s, a) - y)^2]\\)$ where \\(y = r + \\gamma \\max_{a'} Q_{\\psi^-}(s', a')\\)</p>"},{"location":"GRL0/recovering_classical_rl/#grl-version_1","title":"GRL Version","text":"<p>Setup:</p> <ul> <li>Field approximator: Neural network \\(Q_\\psi(s, \\theta)\\) approximates reinforcement field</li> <li>Particle memory: \\(\\Omega = \\{(z_i, w_i)\\}\\) where \\(z_i = (s_i, \\theta_i)\\)</li> <li>Kernel: Implicit kernel induced by neural network architecture</li> </ul> <p>Update Rule: Sample particles from \\(\\Omega\\), compute TD targets: $\\(\\psi \\leftarrow \\psi - \\eta \\nabla_\\psi \\mathbb{E}_{(z,w) \\sim \\Omega} [(Q_\\psi(z) - y)^2]\\)$ where \\(y = w + \\gamma \\max_{\\theta'} Q_\\psi(s', \\theta')\\)</p> <p>Target network: Optional, same as DQN</p>"},{"location":"GRL0/recovering_classical_rl/#key-takeaways_1","title":"Key Takeaways","text":"<p>DQN is GRL with:</p> <ol> <li>Neural network approximation of the reinforcement field</li> <li>Experience replay = particle memory sampling</li> <li>Discrete actions (typically)</li> </ol> <p>What GRL adds:</p> <ul> <li>Explicit particle representation: Particles are not just for replay, they define the field</li> <li>Kernel interpretation: Neural network as implicit kernel</li> <li>Continuous action generalization: \\(Q_\\psi(s, \\theta)\\) for any \\(\\theta\\)</li> </ul> <p></p>"},{"location":"GRL0/recovering_classical_rl/#4-recovery-3-reinforce-policy-gradient","title":"4. Recovery 3: REINFORCE (Policy Gradient)","text":""},{"location":"GRL0/recovering_classical_rl/#classical-reinforce","title":"Classical REINFORCE","text":"<p>Setup:</p> <ul> <li>Policy: \\(\\pi_\\phi(a|s)\\) (parameterized, e.g., neural network)</li> <li>Objective: \\(J(\\phi) = \\mathbb{E}_{\\tau \\sim \\pi_\\phi} [R(\\tau)]\\) (expected return)</li> </ul> <p>Update Rule (score function gradient): $\\(\\nabla_\\phi J(\\phi) = \\mathbb{E}_{\\tau \\sim \\pi_\\phi} [\\sum_t \\nabla_\\phi \\log \\pi_\\phi(a_t | s_t) \\cdot G_t]\\)$</p> <p>where \\(G_t = \\sum_{t'=t}^T \\gamma^{t'-t} r_{t'}\\) (return from time \\(t\\))</p>"},{"location":"GRL0/recovering_classical_rl/#grl-version_2","title":"GRL Version","text":"<p>Setup:</p> <ul> <li>Reinforcement field: \\(Q^+(s, \\theta)\\)</li> <li>Policy: Boltzmann over field   $\\(\\pi(a | s) = \\frac{\\exp(\\beta \\, Q^+(s, \\theta_a))}{\\int \\exp(\\beta \\, Q^+(s, \\theta')) d\\theta'}\\)$</li> </ul> <p>Gradient of Expected Return:</p> <p>The policy gradient in GRL is: $\\(\\nabla_\\theta \\mathbb{E}_{\\pi} [R] = \\mathbb{E}_{\\pi} [\\nabla_\\theta Q^+(s, \\theta) \\cdot \\text{advantage}]\\)$</p> <p>If we parameterize \\(Q^+(s, \\theta) = Q_\\phi(s, \\theta)\\), then: $\\(\\nabla_\\phi J(\\phi) = \\mathbb{E} [\\nabla_\\phi Q_\\phi(s, \\theta) \\cdot G_t]\\)$</p> <p>Connection to REINFORCE:</p> <p>The score function gradient \\(\\nabla_\\phi \\log \\pi_\\phi(a|s)\\) in REINFORCE is equivalent to the field gradient \\(\\nabla_\\phi Q_\\phi(s, \\theta)\\) when policy is Boltzmann.</p> <p>This recovers REINFORCE!</p>"},{"location":"GRL0/recovering_classical_rl/#key-takeaways_2","title":"Key Takeaways","text":"<p>REINFORCE is GRL with:</p> <ol> <li>Boltzmann policy derived from energy landscape</li> <li>Direct parameterization of the field (or policy)</li> <li>Monte Carlo returns (\\(G_t\\)) as targets</li> </ol> <p>What GRL adds:</p> <ul> <li>Energy interpretation: \\(Q^+ = -E\\) provides physics-inspired regularization</li> <li>Particle-based updates: No need for full gradient, use particle approximation</li> <li>Smooth action selection: Temperature \\(\\beta\\) controls exploration naturally</li> </ul> <p></p>"},{"location":"GRL0/recovering_classical_rl/#5-recovery-4-actor-critic-ppo-sac","title":"5. Recovery 4: Actor-Critic (PPO, SAC)","text":""},{"location":"GRL0/recovering_classical_rl/#classical-actor-critic","title":"Classical Actor-Critic","text":"<p>Setup:</p> <ul> <li>Actor: Policy \\(\\pi_\\phi(a|s)\\)</li> <li>Critic: Value function \\(V_\\psi(s)\\) or \\(Q_\\psi(s, a)\\)</li> </ul> <p>Update:</p> <ul> <li> <p>Critic: TD learning   $\\(\\psi \\leftarrow \\psi - \\eta \\nabla_\\psi [Q_\\psi(s, a) - (r + \\gamma V_\\psi(s'))]^2\\)$</p> </li> <li> <p>Actor: Policy gradient with advantage   $\\(\\phi \\leftarrow \\phi + \\eta \\nabla_\\phi \\log \\pi_\\phi(a|s) \\cdot A(s, a)\\)$   where \\(A(s, a) = Q(s, a) - V(s)\\) (advantage)</p> </li> </ul> <p>Variants:</p> <ul> <li>PPO: Clipped objective, KL penalty</li> <li>SAC: Entropy regularization, temperature tuning</li> </ul>"},{"location":"GRL0/recovering_classical_rl/#grl-version_3","title":"GRL Version","text":"<p>Setup:</p> <ul> <li>Critic: Reinforcement field \\(Q^+(s, \\theta)\\) (this is the \"critic\")</li> <li>Actor: Policy inferred from field via Boltzmann   $\\(\\pi(\\theta | s) \\propto \\exp(\\beta \\, Q^+(s, \\theta))\\)$</li> </ul> <p>Update:</p> <ul> <li>Field (Critic): RF-SARSA (two-layer TD system)</li> <li>Primitive layer: TD learning for discrete transitions</li> <li> <p>GP layer: Smooth field over augmented space</p> </li> <li> <p>Policy (Actor): Derived automatically from field</p> </li> <li>No separate actor parameters!</li> <li>Policy gradient = field gradient</li> </ul> <p>Connection:</p> <ul> <li>\\(Q^+(s, \\theta)\\) plays the role of both Q-function and value function</li> <li>Boltzmann policy automatically balances exploitation (high \\(Q^+\\)) and exploration (low \\(\\beta\\))</li> <li>Advantage: \\(A(s, \\theta) = Q^+(s, \\theta) - \\max_{\\theta'} Q^+(s, \\theta')\\)</li> </ul> <p>This recovers Actor-Critic!</p>"},{"location":"GRL0/recovering_classical_rl/#key-takeaways_3","title":"Key Takeaways","text":"<p>Actor-Critic is GRL with:</p> <ol> <li>Reinforcement field as critic</li> <li>Boltzmann policy as actor (no separate parameters)</li> <li>RF-SARSA as update rule</li> </ol> <p>What GRL adds:</p> <ul> <li>Unified representation: No need for separate actor and critic</li> <li>Automatic exploration: Temperature \\(\\beta\\) replaces entropy regularization</li> <li>Particle-based: Memory naturally handles off-policy data</li> </ul> <p>Special Cases:</p> <ul> <li>PPO: GRL with clipped field updates, on-policy sampling</li> <li>SAC: GRL with entropy term in field (equivalent to temperature)</li> </ul> <p></p>"},{"location":"GRL0/recovering_classical_rl/#6-recovery-5-rlhf-for-llms","title":"6. Recovery 5: RLHF for LLMs","text":""},{"location":"GRL0/recovering_classical_rl/#classical-rlhf-reinforcement-learning-from-human-feedback","title":"Classical RLHF (Reinforcement Learning from Human Feedback)","text":"<p>Setup (e.g., for ChatGPT):</p> <ul> <li>LLM: \\(\\pi_\\phi(a_t | s_t)\\) where \\(s_t\\) = (prompt, response so far), \\(a_t\\) = next token</li> <li>Reward Model: \\(r_\\theta(s, a)\\) learned from human preferences</li> <li>Algorithm: PPO or similar policy gradient method</li> </ul> <p>Update (PPO): $\\(\\mathcal{L}(\\phi) = \\mathbb{E}_{(s,a) \\sim \\pi_\\phi} [\\min(r_\\text{clip}, r_\\text{KL})]\\)$</p> <p>where:</p> <ul> <li>\\(r_\\text{clip}\\) = clipped advantage</li> <li>\\(r_\\text{KL}\\) = KL penalty from reference policy</li> </ul>"},{"location":"GRL0/recovering_classical_rl/#grl-version_4","title":"GRL Version","text":"<p>Setup:</p> <ul> <li>State: \\(s_t\\) = (prompt, partial response)</li> <li>Action: \\(\\theta_t\\) = token ID or logit vector (discrete or continuous parameterization)</li> <li>Augmented space: \\((s_t, \\theta_t)\\) in semantic embedding space</li> <li>Reinforcement field: \\(Q^+(s_t, \\theta_t)\\) = expected reward for generating token \\(\\theta_t\\) in context \\(s_t\\)</li> </ul> <p>Formulation:</p> <p>Option 1: Discrete Tokens (Classical RL recovery) - \\(\\theta_t \\in \\{1, \\ldots, V\\}\\) (vocabulary size \\(V\\)) - Field: \\(Q^+(s_t, \\theta_t)\\) for each token - Policy: Softmax over field   $\\(\\pi(\\theta_t | s_t) = \\frac{\\exp(\\beta \\, Q^+(s_t, \\theta_t))}{\\sum_{\\theta'} \\exp(\\beta \\, Q^+(s_t, \\theta'))}\\)$</p> <p>Option 2: Continuous Parameterization (GRL extension) - \\(\\theta_t \\in \\mathbb{R}^d\\) = token embedding or logit vector - Field: \\(Q^+(s_t, \\theta_t)\\) smooth over embedding space - Policy: Sample from continuous distribution over \\(\\theta\\), map to nearest token</p> <p>Update: RF-SARSA with human feedback as rewards - Particle memory stores (prompt, response, reward) tuples - Kernel generalizes across similar prompts/responses - Field learns \\(Q^+(s, \\theta)\\) via TD learning</p>"},{"location":"GRL0/recovering_classical_rl/#advantages-of-grl-for-rlhf","title":"Advantages of GRL for RLHF","text":"<p>1. Off-Policy Learning - Particle memory enables replay - Sample efficiency: learn from all past human feedback - Classical RLHF (PPO) is on-policy only</p> <p>2. Smooth Generalization - Kernel similarity between prompts - Transfer value across related contexts - Fewer human labels needed</p> <p>3. Uncertainty Quantification - Sparse particles = high uncertainty - Exploration naturally targets uncertain regions - Safety: avoid high-stakes decisions with low confidence</p> <p>4. Interpretability - Energy landscape over prompt space - Visualize which responses are preferred - Particle inspection: \"Why did you say that?\"</p>"},{"location":"GRL0/recovering_classical_rl/#implementation-path","title":"Implementation Path","text":"<p>Phase 1: Discrete Tokens (Q1 2026) - Implement GRL on small model (GPT-2) - Reproduce PPO results on standard RLHF benchmarks - Show GRL recovers classical behavior</p> <p>Phase 2: Comparison (Q2 2026) - Compare sample efficiency: GRL vs. PPO - Measure stability: fewer human labels needed? - Quantify uncertainty: does GRL know what it doesn't know?</p> <p>Phase 3: Scale (Q3 2026) - Apply to larger model (LLaMA-7B or Mistral-7B) - Test on real human feedback datasets - Submit paper: \"GRL for LLM Fine-tuning\"</p>"},{"location":"GRL0/recovering_classical_rl/#key-takeaways_4","title":"Key Takeaways","text":"<p>RLHF is GRL with:</p> <ol> <li>Discrete action space (tokens)</li> <li>On-policy updates (PPO)</li> <li>Neural network approximation of field</li> </ol> <p>What GRL adds for RLHF:</p> <ul> <li>Off-policy learning (replay buffer of human feedback)</li> <li>Kernel generalization (transfer across prompts)</li> <li>Uncertainty (exploration where most uncertain)</li> <li>Interpretability (energy landscapes, particle inspection)</li> </ul> <p>Strategic Impact: Demonstrating GRL on RLHF:</p> <ul> <li>Validates GRL on most commercially relevant RL problem</li> <li>Opens door to industry adoption (OpenAI, Anthropic, Meta)</li> <li>Natural bridge to scaling research</li> </ul> <p></p>"},{"location":"GRL0/recovering_classical_rl/#7-what-grl-adds-beyond-classical-rl","title":"7. What GRL Adds Beyond Classical RL","text":"<p>While GRL recovers classical methods, it also enables capabilities that are difficult or impossible in standard RL:</p>"},{"location":"GRL0/recovering_classical_rl/#1-continuous-action-generalization","title":"1. Continuous Action Generalization","text":"<p>Classical RL: Discretize continuous actions or use neural networks GRL: Smooth field \\(Q^+(s, \\theta)\\) over continuous \\(\\theta\\) via kernels</p> <p>Example: Robot grasping - Classical: Sample \\(N\\) discrete grasp poses, learn Q-value for each - GRL: Learn smooth field over continuous grasp space, interpolate between samples</p>"},{"location":"GRL0/recovering_classical_rl/#2-compositional-actions","title":"2. Compositional Actions","text":"<p>Classical RL: Actions are atomic GRL: Actions are operators that can be composed</p> <p>Example: Multi-step manipulation - Classical: Learn separate policies for \"pick\", \"place\", \"push\" - GRL: Learn operators that compose: \\(\\hat{O}_{\\text{place}} \\circ \\hat{O}_{\\text{pick}}\\)</p>"},{"location":"GRL0/recovering_classical_rl/#3-uncertainty-quantification","title":"3. Uncertainty Quantification","text":"<p>Classical RL: Uncertainty requires ensembles or Bayesian NNs GRL: Particle sparsity directly indicates uncertainty</p> <p>Example: Safe exploration - Classical: Ensemble of Q-networks, high variance = uncertain - GRL: Sparse particles = uncertain, avoid or explore based on risk</p>"},{"location":"GRL0/recovering_classical_rl/#4-energy-based-regularization","title":"4. Energy-Based Regularization","text":"<p>Classical RL: Entropy regularization (SAC), KL penalties (PPO) GRL: Energy function \\(E = -Q^+\\) naturally regularizes via least-action principle</p> <p>Example: Smooth, efficient policies - Classical: Add entropy bonus to reward - GRL: Energy naturally prefers smooth, low-energy paths (physics-inspired)</p>"},{"location":"GRL0/recovering_classical_rl/#5-particle-based-interpretability","title":"5. Particle-Based Interpretability","text":"<p>Classical RL: Black-box neural networks GRL: Particles are interpretable experiences</p> <p>Example: Debugging - Classical: \"Why did the policy fail?\" \u2192 Inspect millions of weights - GRL: \"Why did the policy fail?\" \u2192 Inspect nearby particles, visualize energy landscape</p>"},{"location":"GRL0/recovering_classical_rl/#6-hierarchical-abstraction-part-ii","title":"6. Hierarchical Abstraction (Part II)","text":"<p>Classical RL: Hierarchical RL requires careful design GRL: Concepts emerge via spectral clustering</p> <p>Example: Long-horizon tasks - Classical: Manually define options/skills - GRL: Spectral decomposition discovers concepts automatically</p> <p></p>"},{"location":"GRL0/recovering_classical_rl/#8-implementation-from-grl-to-classical","title":"8. Implementation: From GRL to Classical","text":""},{"location":"GRL0/recovering_classical_rl/#code-example-q-learning-from-grl","title":"Code Example: Q-Learning from GRL","text":"<pre><code>from grl.core import ParticleMemory, DeltaKernel\nfrom grl.algorithms import MemoryUpdate\n\n# Classical Q-learning setup\nstate_space = [\"s1\", \"s2\", \"s3\"]\naction_space = [\"a1\", \"a2\"]\n\n# GRL setup: Map discrete actions to fixed parameters\naction_params = {\n    \"a1\": torch.tensor([0.0]),  # \u03b8_1\n    \"a2\": torch.tensor([1.0]),  # \u03b8_2\n}\n\n# Particle memory (GRL)\nmemory = ParticleMemory()\n\n# Delta kernel (no generalization)\nkernel = DeltaKernel()\n\n# Experience: (s, a, r, s')\ns, a, r, s_next = \"s1\", \"a1\", 1.0, \"s2\"\n\n# Convert to GRL format\nz = (s, action_params[a])\nw = r\n\n# MemoryUpdate (GRL) \u2261 Q-learning update (Classical)\nmemory = memory_update(memory, z, w, kernel, alpha=0.1)\n\n# Query field (GRL) \u2261 Q(s, a) (Classical)\nQ_sa = memory.query((s, action_params[a]))\n\n# This is Q-learning!\n</code></pre>"},{"location":"GRL0/recovering_classical_rl/#code-example-dqn-from-grl","title":"Code Example: DQN from GRL","text":"<pre><code>from grl.core import NeuralField\nfrom grl.algorithms import FieldTDLearning\n\n# Neural network approximates reinforcement field\nfield = NeuralField(state_dim=4, action_dim=2, hidden_dim=64)\n\n# Experience replay (GRL particle memory)\nmemory = ParticleMemory()\n\n# Training loop\nfor episode in range(num_episodes):\n    for step in range(max_steps):\n        # Sample from memory (experience replay)\n        batch = memory.sample(batch_size=32)\n\n        # Compute TD targets (same as DQN)\n        td_targets = compute_td_targets(batch, field, gamma=0.99)\n\n        # Update field (GRL) \u2261 Update Q-network (DQN)\n        loss = field.update(batch, td_targets)\n\n# This is DQN!\n</code></pre>"},{"location":"GRL0/recovering_classical_rl/#conclusion","title":"Conclusion","text":"<p>GRL is a unifying framework that:</p> <ol> <li>\u2705 Recovers classical RL (Q-learning, DQN, REINFORCE, PPO, SAC, RLHF)</li> <li>\u2705 Extends to continuous actions (smooth generalization via kernels)</li> <li>\u2705 Enables composition (operator algebra)</li> <li>\u2705 Provides uncertainty (particle sparsity)</li> <li>\u2705 Interprets naturally (energy landscapes, particles)</li> <li>\u2705 Discovers structure (spectral concepts in Part II)</li> </ol> <p>For practitioners: GRL gives you what you already know (classical RL), plus new tools for continuous control, uncertainty, and interpretability.</p> <p>For researchers: GRL provides a principled foundation for understanding why existing methods work and how to extend them.</p> <p>For industry: GRL applies to modern problems (RLHF for LLMs) while offering advantages (off-policy, uncertainty, sample efficiency).</p>"},{"location":"GRL0/recovering_classical_rl/#next-steps","title":"Next Steps","text":"<p>Reproduce Classical Results:</p> <ul> <li> Implement Q-learning recovery on GridWorld</li> <li> Implement DQN recovery on CartPole</li> <li> Implement PPO recovery on continuous control (Pendulum)</li> <li> Implement RLHF recovery on small LLM (GPT-2)</li> </ul> <p>Document Connections:</p> <ul> <li> Add \"Classical RL Recovery\" section to each tutorial chapter</li> <li> Create comparison tables (classical vs. GRL)</li> <li> Write blog post: \"GRL: A Unifying Framework for RL\"</li> </ul> <p>Validate:</p> <ul> <li> Benchmark: GRL vs. DQN on Atari</li> <li> Benchmark: GRL vs. SAC on MuJoCo</li> <li> Benchmark: GRL vs. PPO on RLHF tasks</li> </ul> <p>Scale:</p> <ul> <li> Apply GRL to LLaMA-7B fine-tuning</li> <li> Demonstrate advantages (sample efficiency, uncertainty)</li> <li> Submit paper: \"GRL for LLM Fine-tuning\"</li> </ul> <p>Last Updated: January 14, 2026 See also: Implementation Roadmap | Research Roadmap</p>"},{"location":"GRL0/implementation/","title":"GRL-v0 Implementation Guide","text":"<p>Purpose: Technical specifications for implementing GRL-v0 Audience: Developers ready to implement GRL Prerequisites: Familiarity with tutorial chapters</p>"},{"location":"GRL0/implementation/#overview","title":"Overview","text":"<p>This directory provides implementation specifications for GRL-v0. Each component is documented with:</p> <ul> <li>Theoretical foundation</li> <li>Interface design</li> <li>Implementation details</li> <li>Testing strategy</li> </ul>"},{"location":"GRL0/implementation/#architecture-overview","title":"Architecture Overview","text":"<p>GRL-v0 is organized into four layers spanning both Part I (Reinforcement Fields) and Part II (Emergent Structure):</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Layer 4: Abstraction (Part II: Emergent Structure)      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Spectral Clustering    \u2502  \u2502   Concept Hierarchy         \u2502  \u2502\n\u2502  \u2502  (Functional clusters)  \u2502  \u2502  (Multi-level abstraction)  \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         Layer 3: Inference (Part I: Reinforcement Fields)       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502    Policy Inference     \u2502  \u2502   Soft State Transitions    \u2502  \u2502\n\u2502  \u2502  (Energy minimization)  \u2502  \u2502  (Distributed successors)   \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         Layer 2: Reinforcement (Part I: Reinforcement Fields)   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502      RF-SARSA           \u2502  \u2502      MemoryUpdate           \u2502  \u2502\n\u2502  \u2502  (Two-layer TD system)  \u2502  \u2502  (Belief transition)        \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502         Layer 1: Representation (Part I: Reinforcement Fields)  \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502    Particle Memory      \u2502  \u2502     Kernel Functions        \u2502  \u2502\n\u2502  \u2502   (Belief state \u03a9)      \u2502  \u2502     (RKHS geometry)         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Part I (Layers 1-3): Particle-based learning, reinforcement fields, belief-state inference</p> <p>Part II (Layer 4): Emergent structure discovery, spectral concept formation, hierarchical control</p> <p>Based on: Section V of the original paper (Chiu &amp; Huber, 2022)</p>"},{"location":"GRL0/implementation/#implementation-specifications","title":"Implementation Specifications","text":""},{"location":"GRL0/implementation/#core-infrastructure","title":"Core Infrastructure","text":"Spec Component Priority Status 00 Implementation Overview - \u23f3 Planned 01 Architecture Design - \u23f3 Planned"},{"location":"GRL0/implementation/#layer-1-representation","title":"Layer 1: Representation","text":"Spec Component Priority Status 02 Particle Memory \u2b50 1 \u23f3 Planned 03 Kernel Functions \u2b50 2 \u23f3 Planned"},{"location":"GRL0/implementation/#layer-2-reinforcement","title":"Layer 2: Reinforcement","text":"Spec Component Priority Status 04 MemoryUpdate Algorithm \u2b50 3 \u23f3 Planned 05 RF-SARSA Algorithm \u2b50 4 \u23f3 Planned"},{"location":"GRL0/implementation/#layer-3-inference","title":"Layer 3: Inference","text":"Spec Component Priority Status 06 Policy Inference \u2b50 5 \u23f3 Planned 07 Soft State Transitions \u2b50 6 \u23f3 Planned"},{"location":"GRL0/implementation/#layer-4-abstraction-part-ii","title":"Layer 4: Abstraction (Part II)","text":"Spec Component Priority Status 08 Spectral Clustering \ud83d\udd2c 1 \u23f3 Planned 09 Concept Discovery \ud83d\udd2c 2 \u23f3 Planned 10 Concept Hierarchy \ud83d\udd2c 3 \u23f3 Planned 11 Concept-Conditioned Policies \ud83d\udd2c 4 \u23f3 Planned <p>Note: Part II implementation begins after Part I is validated (see Priority 7 below)</p>"},{"location":"GRL0/implementation/#demonstration-environment","title":"Demonstration Environment","text":"Spec Component Priority Status 12 2D Navigation Domain \u2b50\u2b50 7 \u23f3 Planned <p>Note: This is the primary environment for validating and demonstrating GRL-v0</p>"},{"location":"GRL0/implementation/#supporting-components","title":"Supporting Components","text":"Spec Component Status 13 Environment Interface \u23f3 Planned 14 Visualization Tools \u23f3 Planned 15 Testing Strategy \u23f3 Planned 16 Experiment Protocols \u23f3 Planned"},{"location":"GRL0/implementation/#implementation-priorities","title":"Implementation Priorities","text":""},{"location":"GRL0/implementation/#priority-1-particle-memory","title":"Priority 1: Particle Memory \u2b50","text":"<p>Why first: This IS the agent state. Everything else depends on it.</p> <p>Key Features:</p> <ul> <li>Particle storage: <code>[(z_i, w_i)]</code></li> <li>Energy queries: <code>E(z) = -\u03a3 w_i k(z, z_i)</code></li> <li>Association: Find similar particles</li> <li>Management: Add, merge, prune</li> </ul>"},{"location":"GRL0/implementation/#priority-2-kernel-functions","title":"Priority 2: Kernel Functions","text":"<p>Why second: Defines geometry of augmented space.</p> <p>Key Features:</p> <ul> <li>RBF kernel with ARD</li> <li>Augmented kernel: <code>k((s,\u03b8), (s',\u03b8'))</code></li> <li>Gradient computation</li> <li>Hyperparameter adaptation</li> </ul>"},{"location":"GRL0/implementation/#priority-3-memoryupdate-algorithm-1","title":"Priority 3: MemoryUpdate (Algorithm 1)","text":"<p>Why third: The belief-state transition operator.</p> <p>Key Features:</p> <ul> <li>Particle instantiation</li> <li>Kernel-based association</li> <li>Weight propagation</li> <li>Regularization</li> </ul>"},{"location":"GRL0/implementation/#priority-4-rf-sarsa-algorithm-2","title":"Priority 4: RF-SARSA (Algorithm 2)","text":"<p>Why fourth: Provides reinforcement signals.</p> <p>Key Features:</p> <ul> <li>Primitive SARSA layer</li> <li>Field GP layer</li> <li>Two-layer coupling</li> <li>ARD updates</li> </ul>"},{"location":"GRL0/implementation/#priority-5-policy-inference","title":"Priority 5: Policy Inference","text":"<p>Why fifth: How actions are selected.</p> <p>Key Features:</p> <ul> <li>Energy-based selection</li> <li>Boltzmann sampling</li> <li>Greedy mode</li> <li>Gradient-based optimization (optional)</li> </ul>"},{"location":"GRL0/implementation/#priority-6-soft-state-transitions","title":"Priority 6: Soft State Transitions","text":"<p>Why sixth: Emergent uncertainty from kernel overlap.</p> <p>Key Features:</p> <ul> <li>Distributed successor states</li> <li>Transition probability from kernel</li> <li>Implicit POMDP interpretation</li> <li>Uncertainty quantification</li> </ul>"},{"location":"GRL0/implementation/#priority-7-2d-navigation-domain-critical","title":"Priority 7: 2D Navigation Domain \u2b50\u2b50 Critical","text":"<p>Why seventh: Primary validation and demonstration environment.</p> <p>Purpose:</p> <ol> <li>Reproduce the original paper (Figure 4, Section VI)</li> <li>Validate all Part I components in a controlled setting</li> <li>Demonstrate GRL capabilities professionally</li> </ol> <p>Key Features:</p> <ul> <li>Continuous 2D state space</li> <li>Parametric movement actions (direction, magnitude)</li> <li>Obstacles, walls, and goals</li> <li>Energy landscape visualization</li> <li>Particle memory visualization</li> <li>Trajectory recording</li> </ul> <p>Deployment Goals:</p> <ul> <li>Reproducibility: Match original paper results</li> <li>Professionalism: Publication-quality figures and demos</li> <li> <p>Accessibility: </p> </li> <li> <p>Python API for programmatic use</p> </li> <li>Interactive web interface for exploration</li> <li>Jupyter notebook tutorials</li> <li>Extensibility: Easy to add new scenarios</li> </ul> <p>See: 2D Navigation Specification below</p>"},{"location":"GRL0/implementation/#part-ii-priorities-after-part-i-validated","title":"Part II Priorities (After Part I Validated)","text":""},{"location":"GRL0/implementation/#priority-8-spectral-clustering-part-ii","title":"Priority 8: Spectral Clustering (Part II)","text":"<p>Why first in Part II: Foundation for concept discovery.</p> <p>Key Features:</p> <ul> <li>Kernel matrix construction from particle memory</li> <li>Eigendecomposition</li> <li>Cluster identification</li> <li>Concept subspace projection (from quantum-inspired Chapter 05)</li> </ul>"},{"location":"GRL0/implementation/#priority-9-concept-discovery","title":"Priority 9: Concept Discovery","text":"<p>Why second: Automated structure learning.</p> <p>Key Features:</p> <ul> <li>Functional similarity metrics</li> <li>Automatic concept identification</li> <li>Concept naming/labeling</li> <li>Validation metrics</li> </ul>"},{"location":"GRL0/implementation/#priority-10-concept-hierarchy","title":"Priority 10: Concept Hierarchy","text":"<p>Why third: Multi-level abstraction.</p> <p>Key Features:</p> <ul> <li>Nested subspace structure</li> <li>Hierarchical composition</li> <li>Transfer across concepts</li> <li>Visualization</li> </ul>"},{"location":"GRL0/implementation/#priority-11-concept-conditioned-policies","title":"Priority 11: Concept-Conditioned Policies","text":"<p>Why fourth: Use discovered structure.</p> <p>Key Features:</p> <ul> <li>Policy per concept</li> <li>Concept-gated execution</li> <li>Hierarchical planning</li> <li>Abstract reasoning</li> </ul>"},{"location":"GRL0/implementation/#2d-navigation-domain-specification","title":"2D Navigation Domain Specification","text":""},{"location":"GRL0/implementation/#overview_1","title":"Overview","text":"<p>The 2D Navigation Domain is the primary environment for GRL-v0 validation and demonstration. Originally introduced in the paper (Figure 4, Section VI), we aim to:</p> <ol> <li>Reproduce existing results with high fidelity</li> <li>Enhance the domain to professional standards</li> <li>Deploy as an accessible, interactive demonstration</li> </ol>"},{"location":"GRL0/implementation/#domain-description","title":"Domain Description","text":"<p>State Space: \\(\\mathcal{S} = [0, L_x] \\times [0, L_y]\\) (continuous 2D position)</p> <p>Action Space: \\(\\mathcal{A} = \\{(\\theta, v) : \\theta \\in [0, 2\\pi), v \\in [0, v_{\\max}]\\}\\) - \\(\\theta\\): Direction angle - \\(v\\): Speed magnitude</p> <p>Augmented Space: \\(\\mathcal{Z} = \\mathcal{S} \\times \\mathcal{A}\\) (4D continuous)</p> <p>Dynamics: $\\(s_{t+1} = s_t + v \\cdot (\\cos\\theta, \\sin\\theta) \\cdot \\Delta t\\)$</p> <p>Obstacles: Polygonal or circular regions (configurable)</p> <p>Goals: Target positions with rewards</p>"},{"location":"GRL0/implementation/#scenarios-from-original-paper","title":"Scenarios (From Original Paper)","text":"<p>Scenario 1: Simple goal-reaching - Single goal, no obstacles - Validate basic particle memory and policy inference</p> <p>Scenario 2: Navigation with obstacles - Multiple obstacles (replicating Figure 4) - Demonstrate smooth navigation around barriers - Show energy landscape and particle distribution</p> <p>Scenario 3: Multi-goal task - Multiple goals with different rewards - Demonstrate action-state duality - Show concept emergence (if Part II implemented)</p>"},{"location":"GRL0/implementation/#reproduction-goals","title":"Reproduction Goals","text":"<p>Figure 4 Recreation:</p> <ul> <li>Exact environment setup from paper</li> <li>Energy landscape visualization</li> <li>Particle memory visualization</li> <li>Learned trajectory comparison</li> </ul> <p>Quantitative Metrics:</p> <ul> <li>Success rate (reaching goal)</li> <li>Path efficiency (vs. optimal)</li> <li>Collision rate (with obstacles)</li> <li>Learning curves (episodes to convergence)</li> </ul> <p>Qualitative Assessment:</p> <ul> <li>Smooth, natural trajectories</li> <li>Efficient obstacle avoidance</li> <li>Energy landscape interpretability</li> </ul>"},{"location":"GRL0/implementation/#professional-enhancement","title":"Professional Enhancement","text":"<p>Visual Quality:</p> <ul> <li>Publication-ready figures (vector graphics)</li> <li>Interactive animations (mp4/gif)</li> <li>Real-time rendering (60 FPS)</li> <li> <p>Multiple view modes:</p> </li> <li> <p>Top-down environment view</p> </li> <li>Energy landscape heatmap</li> <li>Particle distribution overlay</li> <li>Trajectory history</li> </ul> <p>Code Quality:</p> <ul> <li>Modular, extensible design</li> <li>Configuration files (YAML/JSON)</li> <li>Logging and metrics</li> <li>Reproducible random seeds</li> </ul> <p>Documentation:</p> <ul> <li>API reference</li> <li>Tutorial notebooks</li> <li>Example scripts</li> <li>Performance benchmarks</li> </ul>"},{"location":"GRL0/implementation/#deployment-plan","title":"Deployment Plan","text":""},{"location":"GRL0/implementation/#phase-1-core-implementation","title":"Phase 1: Core Implementation","text":"<p>Components:</p> <ul> <li>Environment class (<code>Nav2DEnv</code>)</li> <li>Rendering engine</li> <li>Action space handling</li> <li>Reward function</li> </ul> <p>Deliverables:</p> <ul> <li>Python package installable via pip</li> <li>Basic visualization</li> <li>Unit tests</li> </ul>"},{"location":"GRL0/implementation/#phase-2-grl-integration","title":"Phase 2: GRL Integration","text":"<p>Components:</p> <ul> <li>Particle memory integration</li> <li>MemoryUpdate in navigation loop</li> <li>RF-SARSA training</li> <li>Energy landscape computation</li> </ul> <p>Deliverables:</p> <ul> <li>Training scripts</li> <li>Evaluation scripts</li> <li>Experiment configs</li> </ul>"},{"location":"GRL0/implementation/#phase-3-professional-demo","title":"Phase 3: Professional Demo","text":"<p>Components:</p> <ul> <li>Interactive Jupyter notebooks</li> <li>Web-based interface (Flask/FastAPI + React)</li> <li>Video demonstrations</li> <li>Benchmark suite</li> </ul> <p>Deliverables:</p> <ul> <li>Hosted web demo (e.g., Hugging Face Spaces, Streamlit)</li> <li>Tutorial video</li> <li>Blog post</li> </ul>"},{"location":"GRL0/implementation/#web-interface-features","title":"Web Interface Features","text":"<p>Interactive Controls:</p> <ul> <li>Place obstacles (drag-and-drop)</li> <li>Set goal positions</li> <li>Adjust GRL hyperparameters (kernel bandwidth, temperature)</li> <li>Start/stop/reset simulation</li> </ul> <p>Visualizations:</p> <ul> <li>Real-time agent movement</li> <li>Energy landscape evolution</li> <li>Particle memory growth</li> <li>Learning curves</li> </ul> <p>Export:</p> <ul> <li>Save trajectories</li> <li>Download figures</li> <li>Export particle memory</li> </ul> <p>Sharing:</p> <ul> <li>Permalink to configurations</li> <li>Embed in documentation</li> <li>Public gallery of scenarios</li> </ul>"},{"location":"GRL0/implementation/#api-design","title":"API Design","text":"<pre><code>from grl.envs import Nav2DEnv\nfrom grl.agents import GRLAgent\n\n# Create environment\nenv = Nav2DEnv(\n    size=(10, 10),\n    obstacles=[\n        {\"type\": \"circle\", \"center\": (5, 5), \"radius\": 1.5},\n        {\"type\": \"polygon\", \"vertices\": [(2, 2), (3, 2), (3, 3)]},\n    ],\n    goal=(9, 9),\n    goal_reward=10.0,\n)\n\n# Create GRL agent\nagent = GRLAgent(\n    kernel=\"rbf\",\n    lengthscale=1.0,\n    temperature=0.1,\n)\n\n# Training loop\nfor episode in range(num_episodes):\n    state = env.reset()\n    done = False\n\n    while not done:\n        action = agent.act(state)\n        next_state, reward, done, info = env.step(action)\n        agent.update(state, action, reward, next_state)\n        state = next_state\n\n    # Visualize\n    if episode % 10 == 0:\n        env.render(show_particles=True, show_energy=True)\n        agent.save_memory(f\"memory_ep{episode}.pkl\")\n\n# Evaluate\nsuccess_rate, avg_path_length = evaluate(agent, env, num_trials=100)\n</code></pre>"},{"location":"GRL0/implementation/#timeline","title":"Timeline","text":"<p>Week 1-2: Core environment implementation Week 3-4: GRL integration and training Week 5-6: Visualization and demos Week 7-8: Web interface and deployment Week 9-10: Documentation and tutorials</p> <p>Target: Complete professional 2D navigation demo by March 2026</p>"},{"location":"GRL0/implementation/#application-domains-beyond-2d-navigation","title":"Application Domains Beyond 2D Navigation","text":""},{"location":"GRL0/implementation/#philosophy-grl-as-a-generalization","title":"Philosophy: GRL as a Generalization","text":"<p>Key Insight: Traditional RL is a special case of GRL when:</p> <ul> <li>Action space is discrete \u2192 GRL with fixed parametric mappings</li> <li>Action space is finite \u2192 GRL with finite operator set</li> <li>Q-learning \u2192 GRL with trivial augmentation (state only)</li> </ul> <p>Strategic Goal: Demonstrate that GRL subsumes classical RL, including modern applications like:</p> <ul> <li>RLHF for LLMs (Reinforcement Learning from Human Feedback)</li> <li>PPO/SAC for continuous control</li> <li>DQN for discrete actions</li> <li>Actor-critic methods</li> </ul> <p>This positions GRL not as \"another RL algorithm\" but as a unifying framework that recovers existing methods as special cases while enabling new capabilities.</p>"},{"location":"GRL0/implementation/#priority-application-domains","title":"Priority Application Domains","text":""},{"location":"GRL0/implementation/#tier-1-validation-environments-demonstrate-correctness","title":"Tier 1: Validation Environments (Demonstrate Correctness)","text":"<p>Goal: Show GRL recovers classical RL results</p> Domain Type Classical Baseline GRL Advantage Status 2D Navigation Continuous control N/A (novel) Smooth generalization \u23f3 Priority 7 CartPole Discrete control DQN Continuous action variant \ud83d\udccb Planned Pendulum Continuous control DDPG, SAC Parametric torque \ud83d\udccb Planned MuJoCo Ant Robotics PPO, SAC Compositional gaits \ud83d\udccb Planned"},{"location":"GRL0/implementation/#tier-2-strategic-environments-demonstrate-generality","title":"Tier 2: Strategic Environments (Demonstrate Generality)","text":"<p>Goal: Show GRL applies to modern RL problems, including LLMs</p> <p>Note: These are theoretical connections with potential future implementations. Each would require significant engineering effort.</p> Domain Type Why Important Theoretical Connection Implementation LLM Fine-tuning (RLHF) Discrete (tokens) Massive industry relevance Token selection as discrete action, PPO as special case \ud83d\udd2c Exploratory Prompt Optimization Discrete sequences Growing field Parametric prompt generation in embedding space \ud83d\udd2c Exploratory Molecule Design Graph generation Drug discovery Parametric molecule operators \ud83d\udd2c Exploratory Neural Architecture Search Discrete choices AutoML Compositional architecture operators \ud83d\udd2c Exploratory <p>Primary Value: Demonstrating that GRL theoretically generalizes existing methods used in commercially relevant problems (RLHF, prompt tuning, etc.)</p> <p>Implementation Reality: These are massive undertakings comparable to full research projects. They serve as:</p> <ul> <li>Motivation for why GRL matters</li> <li>Future directions if resources/collaborators available</li> <li>Examples in theoretical justification documents</li> </ul>"},{"location":"GRL0/implementation/#tier-3-novel-environments-demonstrate-unique-capabilities","title":"Tier 3: Novel Environments (Demonstrate Unique Capabilities)","text":"<p>Goal: Show what GRL enables that classical RL cannot do easily</p> Domain Type Novel Capability Why GRL Shines Status Physics Simulation Continuous fields Apply force fields, not point forces Operator actions on state space \ud83d\udccb Planned Fluid Control PDE-governed Manipulate flow fields Field operators, neural operators \ud83d\udccb Planned Image Editing High-dim continuous Parametric transformations Smooth action manifolds \ud83d\udccb Planned Multi-Robot Coordination Continuous, multi-agent Compositional team behaviors Operator algebra \ud83d\udccb Planned"},{"location":"GRL0/implementation/#recovering-classical-rl-a-bridge-to-adoption","title":"Recovering Classical RL: A Bridge to Adoption","text":"<p>Document: <code>docs/GRL0/recovering_classical_rl.md</code> (to be created)</p> <p>Purpose: Show step-by-step how classical RL algorithms emerge from GRL as special cases</p> <p>Contents:</p> <ol> <li>Q-learning from GRL</li> <li>Discrete action space as fixed parametric mapping</li> <li>Particle memory as replay buffer</li> <li> <p>TD update as special case of MemoryUpdate</p> </li> <li> <p>DQN from GRL</p> </li> <li>Neural network Q-function as continuous approximation of particle field</li> <li>Experience replay as particle subsampling</li> <li> <p>Target networks as delayed MemoryUpdate</p> </li> <li> <p>Policy Gradient (REINFORCE) from GRL</p> </li> <li>Boltzmann policy from energy landscape</li> <li>Score function gradient as field gradient</li> <li> <p>Baseline as energy normalization</p> </li> <li> <p>Actor-Critic (PPO, SAC) from GRL</p> </li> <li>Actor = policy inference from field</li> <li>Critic = reinforcement field itself</li> <li> <p>Entropy regularization as temperature parameter</p> </li> <li> <p>RLHF for LLMs from GRL</p> </li> <li>Token selection as discrete action</li> <li>Reward model as energy function</li> <li>PPO update as special case of RF-SARSA</li> </ol> <p>Impact: This document becomes the key reference for convincing classical RL researchers that GRL is not alien, but a natural generalization.</p>"},{"location":"GRL0/implementation/#llm-fine-tuning-as-a-grl-application-exploratory","title":"LLM Fine-tuning as a GRL Application (Exploratory)","text":"<p>Status: Theoretical connection established, implementation exploratory</p> <p>Why This Is Interesting: </p> <ul> <li>Relevance: RLHF is used for ChatGPT, Claude, Llama, Gemini</li> <li>Familiarity: Most ML researchers understand this problem</li> <li>Validation: If GRL generalizes RLHF theoretically, it validates the framework's breadth</li> </ul>"},{"location":"GRL0/implementation/#theoretical-formulation","title":"Theoretical Formulation","text":"<p>State: \\(s_t\\) = (prompt, partial response up to token \\(t\\))</p> <p>Action: \\(a_t \\in \\mathcal{V}\\) where \\(\\mathcal{V}\\) = vocabulary (discrete)</p> <p>GRL View: </p> <ul> <li>Augmented space: \\((s_t, \\theta_t)\\) where \\(\\theta_t\\) represents token choice</li> <li>Particle memory: stores (prompt, response, reward) experiences</li> <li>Reinforcement field: \\(Q^+(s_t, \\theta_t)\\) over semantic embedding</li> <li>Policy inference: Sample from Boltzmann over \\(Q^+\\)</li> </ul> <p>Key Insight: Standard RLHF (PPO) is GRL with:</p> <ul> <li>Discrete action space (tokens)</li> <li>Neural network approximation of field</li> <li>On-policy sampling</li> </ul> <p>This theoretical connection is documented in: Recovering Classical RL from GRL</p>"},{"location":"GRL0/implementation/#potential-grl-advantages-theoretical","title":"Potential GRL Advantages (Theoretical)","text":"<ul> <li>Off-policy learning: Particle memory could enable experience replay</li> <li>Smooth generalization: Nearby prompts might share value via kernel</li> <li>Uncertainty: Sparse particles could indicate high uncertainty</li> <li>Interpretability: Energy landscape over prompt space</li> </ul> <p>However: These advantages are speculative without empirical validation.</p>"},{"location":"GRL0/implementation/#implementation-reality","title":"Implementation Reality","text":"<p>Challenges:</p> <ol> <li>Infrastructure: Requires reward model training, human feedback data, preference datasets</li> <li>Computational cost: LLM fine-tuning is expensive (even GPT-2)</li> <li>Comparison difficulty: Matching PPO requires careful hyperparameter tuning</li> <li>Integration: Modern RLHF uses TRL, transformers, accelerate \u2014 non-trivial to integrate</li> <li>Validation: Showing clear advantages requires extensive controlled experiments</li> </ol> <p>Estimated Effort: 6-12 months of focused work with GPU resources</p> <p>When to Pursue:</p> <ul> <li>\u2705 After GRL validated on simpler environments (2D Nav, classical RL)</li> <li>\u2705 If collaborators or funding available</li> <li>\u2705 If clear path to demonstrating advantages</li> <li>\u2705 If access to human feedback datasets</li> </ul> <p>Realistic First Step (if pursued):</p> <ul> <li>Toy RLHF-like problem (small vocabulary, simple preference task)</li> <li>Not real LLM, but demonstrates GRL can handle discrete sequential choices</li> <li>Fast iteration, low compute cost</li> </ul>"},{"location":"GRL0/implementation/#environment-simulation-package-structure","title":"Environment Simulation Package Structure","text":"<p>Given the scope of applications, we'll need a well-organized environment package:</p> <pre><code>src/grl/envs/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base_env.py                 # GRL environment interface\n\u2502\n\u251c\u2500\u2500 validation/                 # Tier 1: Classical RL baselines\n\u2502   \u251c\u2500\u2500 nav2d.py               # 2D navigation (Priority 7)\n\u2502   \u251c\u2500\u2500 cartpole.py            # Discrete control\n\u2502   \u251c\u2500\u2500 pendulum.py            # Continuous control\n\u2502   \u2514\u2500\u2500 mujoco_envs.py         # Robotics (Ant, Humanoid)\n\u2502\n\u251c\u2500\u2500 strategic/                  # Tier 2: Modern RL applications\n\u2502   \u251c\u2500\u2500 llm_finetuning.py      # \ud83d\udd25 RLHF for LLMs (High Priority)\n\u2502   \u251c\u2500\u2500 prompt_optimization.py  # Prompt tuning\n\u2502   \u251c\u2500\u2500 molecule_design.py      # Drug discovery\n\u2502   \u2514\u2500\u2500 nas.py                  # Neural Architecture Search\n\u2502\n\u251c\u2500\u2500 novel/                      # Tier 3: GRL-native applications\n\u2502   \u251c\u2500\u2500 physics_sim.py          # Force field control\n\u2502   \u251c\u2500\u2500 fluid_control.py        # PDE-governed systems\n\u2502   \u251c\u2500\u2500 image_editing.py        # Parametric image transforms\n\u2502   \u2514\u2500\u2500 multi_robot.py          # Multi-agent coordination\n\u2502\n\u251c\u2500\u2500 wrappers/                   # Adapters for existing environments\n\u2502   \u251c\u2500\u2500 gym_wrapper.py          # OpenAI Gym \u2192 GRL\n\u2502   \u251c\u2500\u2500 gymnasium_wrapper.py    # Gymnasium \u2192 GRL\n\u2502   \u251c\u2500\u2500 dm_control_wrapper.py   # DeepMind Control \u2192 GRL\n\u2502   \u2514\u2500\u2500 rlhf_wrapper.py         # TRL/transformers \u2192 GRL\n\u2502\n\u2514\u2500\u2500 scenarios/                  # Predefined configurations\n    \u251c\u2500\u2500 paper_scenarios.py      # Scenarios from original paper\n    \u251c\u2500\u2500 benchmark_suite.py      # Standard benchmarks\n    \u2514\u2500\u2500 tutorials.py            # Teaching examples\n</code></pre> <p>Key Design Principle: </p> <ul> <li>Wrappers allow GRL to be applied to any existing RL environment</li> <li>Native environments showcase GRL's unique capabilities</li> <li>Scenarios provide reproducible experiments</li> </ul>"},{"location":"GRL0/implementation/#strategic-roadmap-update","title":"Strategic Roadmap Update","text":"<p>Phase 1 (Q1 2026): Foundation \u2b50\u2b50\u2b50 - Complete Part I tutorial - Implement core GRL components - \u2705 2D Navigation validated</p> <p>Phase 2 (Q2 2026): Classical RL Recovery \u2b50\u2b50 - Implement wrappers (Gym, Gymnasium) - Reproduce DQN on CartPole - Reproduce SAC on Pendulum - Document: \"Recovering Classical RL from GRL\" \u2705 Complete - Paper A submission</p> <p>Phase 3 (Q3-Q4 2026): Novel Contributions \u2b50 - Amplitude-based RL (if promising) - MDL consolidation - Concept-based mixture of experts - Papers B &amp; C submissions</p> <p>Future Directions (No timeline):</p> <ul> <li>Theoretical articles: Justify how RLHF, prompt optimization, molecule design are special cases</li> <li>Implementation: If resources/collaborators available, pick 1-2 strategic applications</li> <li>Novel applications: Physics simulation, multi-robot coordination (GRL-native capabilities)</li> </ul>"},{"location":"GRL0/implementation/#success-metrics","title":"Success Metrics","text":"<p>Technical (Achievable):</p> <ul> <li> 2D Navigation demo complete with professional web interface</li> <li> GRL recovers DQN/SAC results on classical benchmarks (\u00b15% performance)</li> <li> Classical RL wrappers work with existing environments</li> <li> Documentation complete and accessible</li> </ul> <p>Research (Achievable):</p> <ul> <li> Part I tutorial complete (Chapters 0-10)</li> <li> Part II foundation (concept subspaces formalized)</li> <li> \"Recovering Classical RL\" document demonstrates generality</li> <li> Paper A submitted (operator formalism)</li> <li> 1-2 papers on novel contributions (amplitude-based RL or MDL consolidation)</li> </ul> <p>Adoption (Aspirational):</p> <ul> <li> GitHub stars: 100+ (realistic), 1000+ (stretch)</li> <li> External users beyond our lab</li> <li> Cited in other papers</li> <li> Conference workshop or tutorial (if invited)</li> </ul> <p>Strategic Applications (Aspirational, No Timeline):</p> <ul> <li> Theoretical articles justify RLHF/prompt-opt as special cases</li> <li> If resources available: implement 1-2 strategic applications</li> <li> Industry partnerships (if opportunities arise)</li> </ul>"},{"location":"GRL0/implementation/#code-structure","title":"Code Structure","text":"<pre><code>src/grl/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 particle_memory.py          # Priority 1: Particle state\n\u2502   \u251c\u2500\u2500 kernels.py                  # Priority 2: RKHS geometry\n\u2502   \u2514\u2500\u2500 soft_transitions.py         # Priority 6: Emergent uncertainty\n\u251c\u2500\u2500 algorithms/\n\u2502   \u251c\u2500\u2500 memory_update.py            # Priority 3: Belief transition\n\u2502   \u251c\u2500\u2500 rf_sarsa.py                 # Priority 4: TD learning\n\u2502   \u2514\u2500\u2500 policy_inference.py         # Priority 5: Action selection\n\u251c\u2500\u2500 concepts/                        # Part II: Emergent Structure\n\u2502   \u251c\u2500\u2500 spectral_clustering.py      # Priority 8: Functional clustering\n\u2502   \u251c\u2500\u2500 concept_discovery.py        # Priority 9: Automated structure\n\u2502   \u251c\u2500\u2500 concept_hierarchy.py        # Priority 10: Multi-level abstraction\n\u2502   \u2514\u2500\u2500 concept_policies.py         # Priority 11: Hierarchical control\n\u251c\u2500\u2500 envs/\n\u2502   \u251c\u2500\u2500 nav2d.py                    # Priority 7: 2D Navigation Domain\n\u2502   \u251c\u2500\u2500 scenarios.py                # Predefined scenarios (Figure 4)\n\u2502   \u2514\u2500\u2500 base_env.py                 # Environment interface\n\u251c\u2500\u2500 agents/\n\u2502   \u251c\u2500\u2500 grl_agent.py                # Complete GRL agent\n\u2502   \u2514\u2500\u2500 evaluation.py               # Agent evaluation tools\n\u251c\u2500\u2500 utils/\n\u2502   \u251c\u2500\u2500 config.py                   # Configuration management\n\u2502   \u251c\u2500\u2500 reproducibility.py          # Random seeds, determinism\n\u2502   \u2514\u2500\u2500 metrics.py                  # Performance metrics\n\u251c\u2500\u2500 visualization/\n\u2502   \u251c\u2500\u2500 energy_landscape.py         # Energy field heatmaps\n\u2502   \u251c\u2500\u2500 particle_viz.py             # Particle memory plots\n\u2502   \u251c\u2500\u2500 trajectory_viz.py           # Agent trajectories\n\u2502   \u2514\u2500\u2500 concept_viz.py              # Concept subspace plots (Part II)\n\u2514\u2500\u2500 web/                            # Web deployment (Priority 7)\n    \u251c\u2500\u2500 api.py                      # FastAPI backend\n    \u251c\u2500\u2500 static/                     # Frontend assets\n    \u2514\u2500\u2500 templates/                  # HTML templates\n</code></pre>"},{"location":"GRL0/implementation/#dependencies","title":"Dependencies","text":""},{"location":"GRL0/implementation/#core-dependencies","title":"Core Dependencies","text":"<pre><code>torch &gt;= 2.0              # Neural operators, gradient computation\nnumpy &gt;= 1.24             # Numerical operations\nscipy &gt;= 1.10             # Scientific computing, optimization\ngpytorch &gt;= 1.10          # Gaussian processes (optional)\nscikit-learn &gt;= 1.3       # Spectral clustering (Part II)\n</code></pre>"},{"location":"GRL0/implementation/#visualization","title":"Visualization","text":"<pre><code>matplotlib &gt;= 3.7         # Static plots\nseaborn &gt;= 0.12          # Statistical visualization\nplotly &gt;= 5.14           # Interactive plots\n</code></pre>"},{"location":"GRL0/implementation/#web-deployment-priority-7","title":"Web Deployment (Priority 7)","text":"<pre><code>fastapi &gt;= 0.104         # Backend API\nuvicorn &gt;= 0.24          # ASGI server\npydantic &gt;= 2.4          # Data validation\njinja2 &gt;= 3.1            # Templating\n</code></pre>"},{"location":"GRL0/implementation/#development","title":"Development","text":"<pre><code>pytest &gt;= 7.4            # Testing\nblack &gt;= 23.9            # Code formatting\nmypy &gt;= 1.6              # Type checking\nsphinx &gt;= 7.2            # Documentation\n</code></pre>"},{"location":"GRL0/implementation/#quality-standards","title":"Quality Standards","text":""},{"location":"GRL0/implementation/#code-quality","title":"Code Quality","text":"<ul> <li> All public functions have docstrings (NumPy style)</li> <li> Type hints throughout (Python 3.10+)</li> <li> Unit test coverage &gt; 80%</li> <li> No linting errors (black, mypy, flake8)</li> <li> Examples run without modification</li> <li> Math notation matches paper</li> </ul>"},{"location":"GRL0/implementation/#part-i-validation","title":"Part I Validation","text":"<ul> <li> Reproduce original paper results (Figure 4)</li> <li> MemoryUpdate converges</li> <li> RF-SARSA learns effectively</li> <li> Energy landscapes are smooth</li> <li> Particle memory grows/prunes correctly</li> </ul>"},{"location":"GRL0/implementation/#part-ii-validation-after-part-i","title":"Part II Validation (After Part I)","text":"<ul> <li> Spectral clustering identifies meaningful concepts</li> <li> Concept hierarchy is interpretable</li> <li> Concept-conditioned policies improve performance</li> <li> Transfer learning across concepts works</li> </ul>"},{"location":"GRL0/implementation/#2d-navigation-demo","title":"2D Navigation Demo","text":"<ul> <li> Web interface is responsive and intuitive</li> <li> Visualizations render at 60 FPS</li> <li> All scenarios from paper work</li> <li> Export/sharing functionality works</li> <li> Tutorial notebooks are clear and complete</li> </ul>"},{"location":"GRL0/implementation/#summary","title":"Summary","text":"<p>GRL-v0 Implementation spans:</p> <ul> <li>Part I (Layers 1-3): Particle-based reinforcement fields</li> <li>Part II (Layer 4): Emergent structure and concept discovery</li> <li>Demonstration: Professional 2D navigation domain with web deployment</li> </ul> <p>Priority Order:</p> <ol> <li>Part I foundations (Priorities 1-6)</li> <li>2D Navigation validation (Priority 7) \u2b50\u2b50 Critical milestone</li> <li>Part II extensions (Priorities 8-11)</li> <li>Additional environments and experiments</li> </ol> <p>Target: Complete 2D navigation demo by March 2026</p> <p>See also: Research Roadmap for broader research plan</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/paper/","title":"GRL Tutorial Paper: Publication Sections","text":"<p>Format: Paper-ready sections for the GRL tutorial paper Audience: Academic readers, peer reviewers Goal: Polished, rigorous sections ready for publication</p>"},{"location":"GRL0/paper/#overview","title":"Overview","text":"<p>This directory contains paper-ready sections of the GRL tutorial paper. Each document represents a complete, publishable section with:</p> <ul> <li>Clear mathematical exposition</li> <li>Rigorous notation</li> <li>Complete algorithms</li> <li>Proper citations</li> </ul>"},{"location":"GRL0/paper/#paper-structure","title":"Paper Structure","text":""},{"location":"GRL0/paper/#front-matter","title":"Front Matter","text":"Section Title Status Abstract GRL Tutorial Paper Abstract \u23f3 Planned Introduction Motivation and Overview \u23f3 Planned"},{"location":"GRL0/paper/#main-sections","title":"Main Sections","text":"Section Title Description Status II Preliminaries Background and notation \u23f3 Planned III Reinforcement Field Functional field in RKHS \u23f3 Planned IV Policy Inference Particle-based algorithms \u23f3 Planned V Theoretical Analysis Properties and interpretation \u23f3 Planned VI Implementation Practical considerations \u23f3 Planned VII Experiments Empirical validation \u23f3 Planned VIII Discussion Connections and extensions \u23f3 Planned"},{"location":"GRL0/paper/#supplementary-material","title":"Supplementary Material","text":"Section Title Description Status S1 Energy-Based Interpretation Connection to EBMs \u23f3 Planned S2 Gradient Flow Deterministic limits \u23f3 Planned S3 Convergence Analysis Theoretical guarantees \u23f3 Planned"},{"location":"GRL0/paper/#section-iv-policy-inference-key-section","title":"Section IV: Policy Inference (Key Section)","text":"<p>This is the core algorithmic section, containing:</p>"},{"location":"GRL0/paper/#iv-a-particle-based-belief-update","title":"IV-A: Particle-Based Belief Update","text":"<p>Algorithm 1: MemoryUpdate - Belief-state transition operator - Kernel-based association - Particle management</p>"},{"location":"GRL0/paper/#iv-b-reinforcement-propagation","title":"IV-B: Reinforcement Propagation","text":"<p>Algorithm 2: RF-SARSA - Two-layer architecture - Primitive TD learning - Field-based generalization</p>"},{"location":"GRL0/paper/#iv-c-soft-state-transitions","title":"IV-C: Soft State Transitions","text":"<ul> <li>Emergent uncertainty</li> <li>Distributed successor representation</li> <li>Kernel-induced smoothness</li> </ul>"},{"location":"GRL0/paper/#iv-d-pomdp-interpretation","title":"IV-D: POMDP Interpretation","text":"<ul> <li>Belief-based control</li> <li>Particle ensemble as belief state</li> <li>Inference vs. optimization</li> </ul>"},{"location":"GRL0/paper/#writing-standards","title":"Writing Standards","text":""},{"location":"GRL0/paper/#mathematical-rigor","title":"Mathematical Rigor","text":"<ul> <li>All claims precisely stated</li> <li>Notation consistent throughout</li> <li>Proofs provided or cited</li> </ul>"},{"location":"GRL0/paper/#algorithm-presentation","title":"Algorithm Presentation","text":"<ul> <li>Complete pseudocode</li> <li>Line-by-line explanation</li> <li>Complexity analysis</li> </ul>"},{"location":"GRL0/paper/#clarity","title":"Clarity","text":"<ul> <li>One concept per paragraph</li> <li>Examples for abstract ideas</li> <li>Clear section transitions</li> </ul>"},{"location":"GRL0/paper/#notation-reference","title":"Notation Reference","text":"Symbol Meaning \\(\\mathcal{S}\\) State space \\(\\Theta\\) Action parameter space \\(z = (s, \\theta)\\) Augmented state-action point \\(k: \\mathcal{Z} \\times \\mathcal{Z} \\to \\mathbb{R}\\) Kernel function \\(\\mathcal{H}_k\\) RKHS induced by \\(k\\) \\(Q^+: \\mathcal{Z} \\to \\mathbb{R}\\) Reinforcement field \\(E(z) = -Q^+(z)\\) Energy function \\(\\Omega = \\{(z_i, w_i)\\}\\) Particle memory \\(\\delta_t\\) TD error at time \\(t\\) <p>Last Updated: January 11, 2026</p>"},{"location":"GRL0/quantum_inspired/","title":"Quantum-Inspired Extensions","text":"<p>This section explores GRL's deep mathematical connections to quantum mechanics and the potential for complex-valued RKHS and amplitude-based probability in machine learning.</p>"},{"location":"GRL0/quantum_inspired/#overview","title":"Overview","text":"<p>Status: \ud83d\udd2c Advanced topics (read after Part I)</p> <p>These extensions are novel to mainstream machine learning and represent potential future directions for GRL and probabilistic ML more broadly.</p>"},{"location":"GRL0/quantum_inspired/#documents","title":"Documents","text":""},{"location":"GRL0/quantum_inspired/#1-rkhs-and-quantum-mechanics-a-structural-parallel","title":"1. RKHS and Quantum Mechanics: A Structural Parallel","text":"<p>Topics: - Hilbert space as shared mathematical structure - Inner products and probability amplitudes - Superposition of particle states - Observables and expectation values</p> <p>Key Insight: GRL's RKHS formulation is structurally identical to quantum mechanics' Hilbert space formulation.</p>"},{"location":"GRL0/quantum_inspired/#1a-wavefunction-interpretation-what-does-it-mean-new","title":"1a. Wavefunction Interpretation: What Does It Mean? \u2b50 New","text":"<p>Topics: - State vector vs. wavefunction (coordinate representation) - Probability amplitudes vs. direct probabilities - One state, many representations - Mapping to GRL: \\(Q^+\\) as state, \\(Q^+(z)\\) as wavefunction - Implications for concept discovery</p> <p>Key Insight: The reinforcement field \\(Q^+ \\in \\mathcal{H}_k\\) is a state vector whose projections yield wavefunction-like amplitude fields.</p> <p>Clarifies: What we mean when we say \"the reinforcement field is a wavefunction.\"</p>"},{"location":"GRL0/quantum_inspired/#2-rkhs-basis-kernel-amplitudes-and-energy-based-inference-new","title":"2. RKHS Basis, Kernel Amplitudes, and Energy-Based Inference \u2b50 New","text":"<p>Topics: - What is the \"basis\" in RKHS? (Kernel sections as frame elements) - How choosing \\(z\\) selects a basis element - Kernel amplitudes vs. quantum amplitudes - Why GRL doesn't need normalization (EBM perspective) - Three interpretations: Hilbert state, amplitude field, energy score</p> <p>Key Insight: GRL combines QM's amplitude geometry with EBM's unnormalized inference\u2014no partition function needed!</p> <p>Clarifies: Why \\(Q^+(z)\\) acts like an amplitude but doesn't require Born rule normalization.</p>"},{"location":"GRL0/quantum_inspired/#4-slicing-the-reinforcement-field-action-and-state-projections-new","title":"4. Slicing the Reinforcement Field: Action and State Projections \u2b50 New","text":"<p>Topics: - Action wavefunction \\(\\psi_s(a) = Q^+(s, a)\\): landscape of actions at a state - State wavefunction \\(\\phi_a(s) = Q^+(s, a)\\): applicability of action across states - Concept subspace projections: hierarchical abstractions - Action-state duality in augmented space - From projections to operators</p> <p>Key Insight: One state \\(Q^+\\), multiple projections\u2014action fields, state fields, and concept activations all emerge from the same underlying structure.</p> <p>Enables: Continuous control, implicit precondition learning, affordance maps, hierarchical RL, and natural skill discovery.</p>"},{"location":"GRL0/quantum_inspired/#5-concept-subspaces-projections-and-measurement-theory-new","title":"5. Concept Subspaces, Projections, and Measurement Theory \u2b50 New","text":"<p>Topics: - Concepts as invariant subspaces (not clusters) - Projection operators and their properties - Concept activation observables \\(A_k = \\|P_k Q^+\\|^2\\) - Hierarchical composition via nested subspaces - Spectral discovery algorithms - Connection to quantum measurement theory</p> <p>Key Insight: Concepts are multi-dimensional subspaces with smooth, compositional activations\u2014provides mathematical foundation for Part II (Section V).</p> <p>Enables: Hierarchical RL, concept-conditioned policies, interpretable learning curves, transfer via concept basis.</p>"},{"location":"GRL0/quantum_inspired/#6-the-agents-state-and-belief-evolution-new","title":"6. The Agent's State and Belief Evolution \u2b50 New","text":"<p>Topics: - What is \"the state\" in GRL? (Answer: \\(Q^+\\) = particle memory) - Three distinct operations: fix state, query state, evolve state - Two time scales: learning (MemoryUpdate) vs. inference (queries) - Role of weights: implicit GP-derived coefficients - Experience association as weight propagation operator - Connection to quantum measurement theory</p> <p>Key Insight: The agent's state is the entire field \\(Q^+ \\in \\mathcal{H}_k\\), equivalently the particle memory \\(\\Omega\\). MemoryUpdate is a belief transition operator; queries are projections of a fixed state.</p> <p>Clarifies: What changes when the agent learns vs. what stays fixed during inference\u2014critical for understanding GRL's structure.</p>"},{"location":"GRL0/quantum_inspired/#7-learning-the-reinforcement-field-beyond-gaussian-processes-new","title":"7. Learning the Reinforcement Field \u2014 Beyond Gaussian Processes \u2b50 New","text":"<p>Topics: - Why GP is one choice among many for learning \\(Q^+\\) - Alternative learning mechanisms: kernel ridge, online optimization, sparse methods, deep nets, mixture of experts - Amplitude-based learning from quantum-inspired probability - When to use which approach (trade-offs)</p> <p>Key Insight: The state-as-field formalism is agnostic to the learning mechanism\u2014you can swap the inference engine while preserving GRL's structure.</p> <p>Key Findings: 1. \u2705 QM math and probability amplitudes can be applied to ML/optimization 2. \u2705 Multiple alternatives to GPR exist: online SGD, sparse methods, mixture of experts, deep neural networks</p> <p>Enables: Scalable GRL implementations, hybrid approaches, novel probability formulations.</p>"},{"location":"GRL0/quantum_inspired/#8-memory-dynamics-formation-consolidation-and-retrieval-new","title":"8. Memory Dynamics: Formation, Consolidation, and Retrieval \u2b50 New","text":"<p>Topics: - Three memory functions: factual, experiential, working - Formation operator \\(\\mathcal{E}\\) (how to write memory) - Consolidation operator \\(\\mathcal{C}\\) (what to retain/forget) - Retrieval operator \\(\\mathcal{R}\\) (how to access memory) - Replacing hard threshold \\(\\tau\\) with principled criteria - Preventing agent drift</p> <p>Key Insight: Memory dynamics are operators with learnable criteria\u2014formation, consolidation, retrieval form a complete system.</p> <p>Key Results: 1. \u2705 Principled memory update mechanisms: soft association, top-k adaptive neighbors, MDL consolidation, surprise-gating 2. \u2705 Data-driven retention criteria: based on surprise, novelty, memory type, and compression objectives</p> <p>Enables: Lifelong learning, bounded memory, adaptive forgetting, preventing agent drift.</p>"},{"location":"GRL0/quantum_inspired/#9-path-integrals-and-action-principles-new","title":"9. Path Integrals and Action Principles \u2b50 New","text":"<p>Topics: - Feynman's path integral formulation - Stochastic control as imaginary time QM - GRL's action functional and Boltzmann policy - Complex-valued GRL: enabling true interference - Path integral sampling algorithms (PI\u00b2, Langevin) - Connection to quantum measurement (Chapter 05) - Feynman diagrams and instanton calculus</p> <p>Key Insight: GRL's Boltzmann policy emerges from the principle of least action via path integrals\u2014not an analogy, a mathematical equivalence. Complex extensions enable quantum interference effects.</p> <p>Enables: Physics-grounded policy optimization, complex-valued fields, tunneling-like exploration, principled action discovery.</p>"},{"location":"GRL0/quantum_inspired/#10-complex-valued-rkhs-and-interference-effects","title":"10. Complex-Valued RKHS and Interference Effects","text":"<p>Topics: - Complex-valued kernels and feature maps - Interference: constructive and destructive - Phase semantics (temporal, contextual, directional) - Complex spectral clustering - Connections to quantum kernel methods</p> <p>Key Insight: Complex-valued RKHS enables richer dynamics and multi-modal representations through interference effects.</p>"},{"location":"GRL0/quantum_inspired/#why-this-matters-for-ml","title":"Why This Matters for ML","text":""},{"location":"GRL0/quantum_inspired/#novel-probability-formulation","title":"Novel Probability Formulation","text":"<p>Traditional ML uses direct probabilities: \\(p(x)\\)</p> <p>GRL (quantum-inspired) uses probability amplitudes: \\(\\langle \\psi | \\phi \\rangle\\) \u2192 \\(|\\langle \\psi | \\phi \\rangle|^2\\)</p> Aspect Traditional ML Quantum-Inspired GRL Representation Real-valued probabilities Complex amplitudes Multi-modality Mixture models Superposition Dynamics Direct optimization Interference effects Phase Not represented Encodes context/time"},{"location":"GRL0/quantum_inspired/#potential-applications","title":"Potential Applications","text":"<ol> <li>Interference-based learning: Constructive/destructive updates to value functions</li> <li>Phase-encoded context: Temporal or directional information in complex phase</li> <li>Spectral concept discovery: Eigenmodes of complex kernels reveal structure</li> <li>Quantum-inspired algorithms: New optimization and sampling methods</li> </ol>"},{"location":"GRL0/quantum_inspired/#reading-order","title":"Reading Order","text":"<p>Recommended sequence:</p> <p>Foundation (Chapters 1-2): 1. Start with 01-rkhs-quantum-parallel.md for the high-level structural parallel 2. Read 01a-wavefunction-interpretation.md for precise conceptual grounding (state vs. wavefunction) 3. Continue with 02-rkhs-basis-and-amplitudes.md to understand RKHS basis and why normalization isn't needed</p> <p>Applications (Chapters 4-6): 4. New: Read 04-action-and-state-fields.md to see how one field \\(Q^+\\) gives multiple projections (action/state/concept) 5. New: Read 05-concept-projections-and-measurements.md for rigorous formalism of concepts as subspaces (foundation for Part II) 6. New: Read 06-agent-state-and-belief-evolution.md to understand what \"the state\" is and how it evolves via MemoryUpdate</p> <p>Learning &amp; Memory (Chapters 7-8): 7. New: Read 07-learning-the-field-beyond-gp.md for learning mechanisms beyond GP\u2014scalability, amplitude-based learning, mixture of experts 8. New: Read 08-memory-dynamics-formation-consolidation-retrieval.md for principled memory dynamics\u2014what to retain/forget, preventing agent drift</p> <p>Advanced (Chapters 9-10): 9. New: Read 09-path-integrals-and-action-principles.md for Feynman path integrals, imaginary time QM, complex-valued GRL, and connection to Tutorial Chapter 03a 10. Explore 03-complex-rkhs.md for complex-valued extensions (interference, phase semantics)</p>"},{"location":"GRL0/quantum_inspired/#connection-to-tutorial-paper","title":"Connection to Tutorial Paper","text":"<p>Part I (Particle-Based Learning): - Chapters 1-2 provide mathematical grounding - Show RKHS-QM structural parallel - Justify amplitude interpretation</p> <p>Part II (Emergent Structure &amp; Spectral Abstraction): - Chapter 5 provides the formal foundation for Section V - Concepts as subspaces (not clusters) - Projection operators and observables - Hierarchical composition framework</p> <p>Extensions (Papers A/B/C): - Chapter 4 (action/state fields) enables novel algorithms - Chapter 5 (concept projections) enables transfer learning - Chapter 6 (complex RKHS) enables interference-based dynamics</p>"},{"location":"GRL0/quantum_inspired/#implementation-notes","title":"Implementation Notes","text":"<p>Current Status: Theoretical foundations established</p> <p>Implemented: - RKHS framework (standard kernels) - Particle-based field representation - Spectral clustering for concept discovery</p> <p>To Implement: - Projection operators for concept activation - Concept-conditioned policies - Hierarchical composition algorithms - Complex-valued kernels (Chapter 6) - Interference-based updates</p>"},{"location":"GRL0/quantum_inspired/#prerequisites","title":"Prerequisites","text":"<p>Before reading these documents, you should understand:</p> <ul> <li>Part I, Chapter 2: RKHS Foundations</li> <li>Part I, Chapter 4: Reinforcement Field</li> <li>Part I, Chapter 5: Particle Memory</li> </ul>"},{"location":"GRL0/quantum_inspired/#references","title":"References","text":"<p>Original Paper: Chiu &amp; Huber (2022), Section V. arXiv:2208.04822</p> <p>Quantum Kernel Methods: - Havl\u00ed\u010dek et al. (2019). Supervised learning with quantum-enhanced feature spaces. Nature. - Schuld &amp; Killoran (2019). Quantum machine learning in feature Hilbert spaces. Physical Review Letters.</p> <p>Complex-Valued Neural Networks: - Trabelsi et al. (2018). Deep complex networks. ICLR. - Hirose (2012). Complex-valued neural networks: Advances and applications. Wiley.</p> <p>Quantum Mechanics Foundations: - Dirac, P. A. M. (1930). The Principles of Quantum Mechanics. Oxford. - Ballentine, L. E. (1998). Quantum Mechanics: A Modern Development. World Scientific.</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/","title":"RKHS and Quantum Mechanics: A Structural Parallel","text":""},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#introduction","title":"Introduction","text":"<p>GRL's reinforcement field framework exhibits a deep structural similarity to quantum mechanics\u2014not as a loose analogy, but as a mathematical identity. Both frameworks are built on the same underlying structure:</p> <p>Hilbert space + inner product + superposition</p> <p>This document explores this connection and its implications for probabilistic machine learning.</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#the-core-parallel","title":"The Core Parallel","text":""},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#grls-formulation","title":"GRL's Formulation","text":"<p>In GRL (Section V of the original paper):</p> <p>Each experience particle defines a basis function in a reproducing kernel Hilbert space, and the field is expressed as a superposition of these functions.</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#quantum-mechanics-formulation","title":"Quantum Mechanics' Formulation","text":"<p>In quantum mechanics:</p> <p>Each eigenstate defines a basis vector in Hilbert space, and the wavefunction is expressed as a superposition of these states.</p> <p>This is not analogy\u2014it is structural identity.</p> <p>In both cases:</p> <ul> <li>The state of the system is a vector in a Hilbert space (not a point)</li> <li>What we \"observe\" or \"infer\" comes from inner products</li> <li>Meaning arises from overlap, not identity</li> <li>Probabilities are derived from amplitudes, not primitive</li> </ul>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#precise-mathematical-correspondence","title":"Precise Mathematical Correspondence","text":""},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#1-particles-basis-states","title":"1. Particles \u2194 Basis States","text":"<p>In GRL:</p> <p>Each particle \\(z_i\\) induces a function\u2014a vector in RKHS:</p> \\[z_i \\mapsto k(z_i, \\cdot) \\in \\mathcal{H}_k\\] <p>In Quantum Mechanics:</p> <p>Each basis state is a vector in Hilbert space:</p> \\[|i\\rangle \\in \\mathcal{H}\\] <p>Neither is a \"thing in the world\"\u2014both are representational primitives.</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#2-reinforcement-field-wavefunction","title":"2. Reinforcement Field \u2194 Wavefunction","text":"<p>In GRL:</p> \\[Q^+(\\cdot) = \\sum_i w_i \\, k(z_i, \\cdot)\\] <p>In Quantum Mechanics:</p> \\[|\\psi\\rangle = \\sum_i c_i \\, |i\\rangle\\] <p>The parallel is exact:</p> <ul> <li>Coefficients: \\(w_i \\leftrightarrow c_i\\)</li> <li>Basis vectors: \\(k(z_i, \\cdot) \\leftrightarrow |i\\rangle\\)</li> <li>The system state is the superposition itself</li> </ul> <p>Interpretation: The reinforcement field is a wavefunction over augmented state-action space. More specifically, the reinforcement field is a state vector in RKHS, whose projections onto kernel-induced bases yield wavefunction-like amplitude fields over augmented state-action space.</p> <p>(See 01a-wavefunction-interpretation.md for detailed clarification of this distinction.)</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#3-kernel-inner-product-probability-amplitude","title":"3. Kernel Inner Product \u2194 Probability Amplitude","text":"<p>In RKHS:</p> \\[\\langle k(z_i, \\cdot), k(z_j, \\cdot) \\rangle_{\\mathcal{H}_k} = k(z_i, z_j)\\] <p>In Quantum Mechanics:</p> \\[\\langle \\phi | \\psi \\rangle\\] <p>In both cases:</p> <ul> <li>Inner product = overlap amplitude</li> <li>Large overlap = strong compatibility</li> <li>Orthogonality = conceptual independence</li> </ul> <p>Why spectral clustering works: It decomposes the space by overlap structure\u2014exactly what eigendecomposition does in quantum mechanics.</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#probability-amplitudes-vs-direct-probabilities","title":"Probability Amplitudes vs. Direct Probabilities","text":""},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#in-quantum-mechanics","title":"In Quantum Mechanics","text":"<ul> <li>The wavefunction \\(\\psi(x)\\) is not a probability</li> <li>\\(|\\psi(x)|^2\\) is the probability density</li> <li>Probability is derived from amplitude, not primitive</li> </ul>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#in-grl","title":"In GRL","text":"<p>Similarly:</p> <ul> <li>The reinforcement field \\(Q^+(z)\\) is not a probability</li> <li>Policy \\(\\pi(a|s) \\propto \\exp(\\beta \\, Q^+(s, a))\\) is derived from the field</li> <li>Inner products \\(k(z_i, z_j)\\) measure compatibility (amplitude overlap)</li> </ul>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#why-this-matters","title":"Why This Matters","text":"<p>Traditional ML: Uses probabilities directly \\(p(x)\\)</p> <p>GRL (Quantum-Inspired): Uses amplitudes \\(\\langle \\psi | \\phi \\rangle\\), then derives probabilities via \\(|\\langle \\psi | \\phi \\rangle|^2\\)</p> <p>This formulation enables:</p> <ol> <li>Superposition: Represent multi-modal distributions naturally</li> <li>Interference: Amplitudes can add constructively or destructively</li> <li>Phase information: (In complex RKHS) Encode temporal/contextual relationships</li> <li>Spectral methods: Eigendecomposition reveals structure</li> </ol>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#observables-and-expectation-values","title":"Observables and Expectation Values","text":""},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#in-quantum-mechanics_1","title":"In Quantum Mechanics","text":"<p>Observables are Hermitian operators \\(\\hat{O}\\):</p> \\[\\langle O \\rangle = \\langle \\psi | \\hat{O} | \\psi \\rangle\\]"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#in-grl_1","title":"In GRL","text":"<p>The expected value at a configuration:</p> \\[V(z) = \\sum_i w_i \\, k(z_i, z) = \\langle Q^+, k(z, \\cdot) \\rangle\\] <p>The value function is an expectation over the particle distribution, weighted by kernel overlap.</p> <p>Parallel: Both frameworks compute expectations as inner products in Hilbert space.</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#implications-for-machine-learning","title":"Implications for Machine Learning","text":""},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#1-novel-probability-formulation","title":"1. Novel Probability Formulation","text":"<p>This amplitude-based formulation is not yet mainstream in ML:</p> Traditional ML Quantum-Inspired (GRL) Direct probabilities \\(p(x)\\) Amplitudes \\(\\langle \\psi \\| \\phi \\rangle\\) Single-valued Superposition of states Real-valued Complex-valued possible No interference Interference effects"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#2-spectral-structure-is-natural","title":"2. Spectral Structure is Natural","text":"<p>Because the system state is a superposition in Hilbert space:</p> <ul> <li>Eigendecomposition naturally reveals coherent subspaces</li> <li>Spectral clustering groups by amplitude overlap</li> <li>Concepts emerge as eigenmodes of the kernel matrix</li> </ul> <p>This is why Part II (Emergent Structure &amp; Spectral Abstraction) uses spectral methods\u2014they're the natural tool for analyzing Hilbert space structure.</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#3-richer-dynamics","title":"3. Richer Dynamics","text":"<p>With complex-valued RKHS (see next document):</p> <ul> <li>Interference effects can guide learning</li> <li>Phase evolution provides temporal dynamics</li> <li>Partial overlaps enable nuanced similarity</li> </ul>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#what-this-is-and-isnt","title":"What This Is and Isn't","text":""},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#this-is","title":"This IS:","text":"<ul> <li>\u2705 A mathematical identity in structure (Hilbert space + inner product)</li> <li>\u2705 A principled way to think about multi-modal distributions</li> <li>\u2705 Justification for amplitude-based probability in ML</li> <li>\u2705 Foundation for spectral methods in concept discovery</li> </ul>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#this-is-not","title":"This IS NOT:","text":"<ul> <li>\u274c Claiming GRL involves physical quantum effects</li> <li>\u274c Requiring quantum computers</li> <li>\u274c Just a metaphor or analogy</li> </ul> <p>The mathematics is literally the same\u2014but applied to learning, not physics.</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#connection-to-part-i-and-part-ii","title":"Connection to Part I and Part II","text":""},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#part-i-particle-based-learning","title":"Part I: Particle-Based Learning","text":"<p>Uses real-valued RKHS:</p> <ul> <li>Particles as basis functions</li> <li>Reinforcement field as superposition</li> <li>Inner products for similarity</li> <li>GP-based energy landscape</li> </ul> <p>Already leverages the Hilbert space structure!</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#part-ii-emergent-structure-spectral-abstraction","title":"Part II: Emergent Structure &amp; Spectral Abstraction","text":"<p>Exploits this structure explicitly:</p> <ul> <li>Spectral clustering on kernel matrix</li> <li>Eigenspaces as concept subspaces</li> <li>Hierarchical structure from spectral decomposition</li> </ul> <p>The quantum-inspired view explains why spectral methods work for concept discovery.</p>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#within-this-tutorial","title":"Within This Tutorial","text":"<ul> <li>Part I, Chapter 2: RKHS Foundations</li> <li>Part I, Chapter 4: Reinforcement Field</li> <li>Next: Complex-Valued RKHS</li> </ul>"},{"location":"GRL0/quantum_inspired/01-rkhs-quantum-parallel/#external-references","title":"External References","text":"<p>Quantum Kernel Methods: - Havl\u00ed\u010dek et al. (2019). Supervised learning with quantum-enhanced feature spaces. Nature 567, 209-212. - Schuld &amp; Killoran (2019). Quantum machine learning in feature Hilbert spaces. Physical Review Letters 122, 040504.</p> <p>RKHS Theory: - Berlinet &amp; Thomas-Agnan (2004). Reproducing Kernel Hilbert Spaces in Probability and Statistics. Springer. - Steinwart &amp; Christmann (2008). Support Vector Machines. Springer.</p> <p>GRL Original Paper: - Chiu &amp; Huber (2022). Generalized Reinforcement Learning. arXiv:2208.04822</p> <p>Last Updated: January 12, 2026</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/","title":"Wavefunction Interpretation: What Does It Mean for the Reinforcement Field?","text":""},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#motivation","title":"Motivation","text":"<p>In the previous document, we stated:</p> <p>\"The reinforcement field is a wavefunction over augmented state-action space.\"</p> <p>This raises important questions:</p> <ul> <li>What exactly is a wavefunction in quantum mechanics?</li> <li>What does it represent and predict?</li> <li>How should we interpret this claim for the reinforcement field?</li> </ul> <p>This document provides the precise conceptual grounding.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#1-what-is-the-wavefunction-in-quantum-mechanics","title":"1. What Is the Wavefunction in Quantum Mechanics?","text":"<p>In standard (non-relativistic) quantum mechanics, the wavefunction \\(\\psi(x,t)\\) is:</p> <p>A complete mathematical representation of the physical state of a system, expressed in a particular basis (the position basis).</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#simple-analogy-first-3d-vectors-and-coordinates","title":"Simple Analogy First: 3D Vectors and Coordinates","text":"<p>Before the formal definition, let's build intuition with a familiar example.</p> <p>Consider a vector in 3D space, like a velocity: \\(\\mathbf{v}\\).</p> <p>The vector itself is a geometric object\u2014an arrow with direction and magnitude. This exists independent of any coordinate system.</p> <p>But to work with it numerically, we express it in coordinates:</p> <p>In Cartesian coordinates \\((x, y, z)\\):</p> \\[\\mathbf{v} = \\begin{bmatrix} 3 \\\\ 4 \\\\ 0 \\end{bmatrix}\\] <p>This means: \"3 units in the \\(x\\) direction, 4 in \\(y\\), 0 in \\(z\\).\"</p> <p>In polar coordinates \\((r, \\theta, z)\\):</p> \\[\\mathbf{v} = \\begin{bmatrix} 5 \\\\ 53.1\u00b0 \\\\ 0 \\end{bmatrix}\\] <p>Key insight: The vector \\(\\mathbf{v}\\) is the same geometric object in both cases. Only its coordinate representation changed.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#the-quantum-version-state-vector-vs-wavefunction","title":"The Quantum Version: State Vector vs. Wavefunction","text":"<p>The same idea applies in quantum mechanics:</p> <p>Formal definition:</p> <ul> <li>The state of a quantum system is a vector \\(|\\psi\\rangle\\) in a complex Hilbert space \\(\\mathcal{H}\\) (like the geometric vector \\(\\mathbf{v}\\))</li> <li>The wavefunction \\(\\psi(x)\\) is the coordinate representation of that vector in the position basis \\(\\{|x\\rangle\\}\\)</li> </ul> <p>The relationship is given by an inner product (projection):</p> \\[\\psi(x) = \\langle x | \\psi \\rangle\\] <p>What this means in plain English:</p> <p>The wavefunction \\(\\psi(x)\\) tells you \"how much\" of the state \\(|\\psi\\rangle\\) \"points in the direction\" of position \\(x\\).</p> <p>It's exactly like asking: \"What is the \\(x\\)-component of velocity \\(\\mathbf{v}\\)?\" Answer: 3.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#concrete-example-two-level-system-qubit","title":"Concrete Example: Two-Level System (Qubit)","text":"<p>Let's work through this with actual numbers.</p> <p>Setup: A qubit has a 2-dimensional Hilbert space with computational basis:</p> \\[|0\\rangle = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad |1\\rangle = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\] <p>These are basis vectors, analogous to \\(\\mathbf{e}_x = [1, 0]\\) and \\(\\mathbf{e}_y = [0, 1]\\) in 2D Cartesian coordinates.</p> <p>State vector (the quantum system itself):</p> \\[|\\psi\\rangle = \\frac{1}{\\sqrt{2}} |0\\rangle + \\frac{1}{\\sqrt{2}} |1\\rangle = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\] <p>Analogy: Just like \\(\\mathbf{v} = 3\\mathbf{e}_x + 4\\mathbf{e}_y = [3, 4]\\), here \\(|\\psi\\rangle\\) is a linear combination of basis vectors \\(|0\\rangle\\) and \\(|1\\rangle\\).</p> <p>Question: What are the \"wavefunction values\" (coordinates) in the \\(\\{|0\\rangle, |1\\rangle\\}\\) basis?</p> <p>Answer: Compute the inner products (projections)!</p> \\[\\psi_0 = \\langle 0 | \\psi \\rangle = \\begin{bmatrix} 1 &amp; 0 \\end{bmatrix} \\cdot \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\] \\[\\psi_1 = \\langle 1 | \\psi \\rangle = \\begin{bmatrix} 0 &amp; 1 \\end{bmatrix} \\cdot \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{\\sqrt{2}}\\] <p>What we computed: Project the state \\(|\\psi\\rangle\\) onto each basis vector.</p> <p>Result: The wavefunction in this basis is \\(\\left[\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}\\right]\\).</p> <p>Interpretation: - \"How much of \\(|\\psi\\rangle\\) points in the \\(|0\\rangle\\) direction?\" \u2192 \\(\\frac{1}{\\sqrt{2}}\\) - \"How much of \\(|\\psi\\rangle\\) points in the \\(|1\\rangle\\) direction?\" \u2192 \\(\\frac{1}{\\sqrt{2}}\\)</p> <p>Analogy: If \\(\\mathbf{v} = [3, 4]\\), the \"\\(x\\)-coordinate\" is 3 (how much of \\(\\mathbf{v}\\) is in the \\(\\mathbf{e}_x\\) direction).</p> <p>Now let's use a DIFFERENT coordinate system:</p> <p>So far, we've been working in the computational basis \\(\\{|0\\rangle, |1\\rangle\\}\\)\u2014think of this as our \"Cartesian coordinates.\"</p> <p>Now let's define a second coordinate system, the Hadamard basis:</p> \\[|+\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle + |1\\rangle) = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\] \\[|-\\rangle = \\frac{1}{\\sqrt{2}}(|0\\rangle - |1\\rangle) = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}\\] <p>Important: These coordinates \\([1/\\sqrt{2}, 1/\\sqrt{2}]\\) are expressed in the computational basis. We're defining new basis vectors by specifying their coordinates in the old basis.</p> <p>Analogy: Like defining polar coordinates by saying: \\(\\hat{r} = \\cos\\theta \\, \\mathbf{e}_x + \\sin\\theta \\, \\mathbf{e}_y\\) (new basis vectors in terms of old).</p> <p>Key observation: Our state \\(|\\psi\\rangle\\) and the basis vector \\(|+\\rangle\\) have the same coordinates in the computational basis:</p> \\[|\\psi\\rangle = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = |+\\rangle\\] <p>Wait, so they're equal? Yes! As vectors, \\(|\\psi\\rangle = |+\\rangle\\). But they play different roles in our current discussion:</p> Symbol Role in This Example \\(\\|\\psi\\rangle\\) The state we're analyzing (the \"subject\") \\(\\|+\\rangle\\) A basis vector in the Hadamard coordinate system (the \"axis\") <p>Analogy: The vector \\([1, 0]\\) could be:</p> <ul> <li>\"The velocity we're analyzing\" \u2190 role: subject</li> <li>\"The x-axis of our coordinate system\" \u2190 role: reference axis</li> </ul> <p>Same vector, different roles!</p> <p>Now the question: What is the wavefunction of \\(|\\psi\\rangle\\) in the Hadamard basis \\(\\{|+\\rangle, |-\\rangle\\}\\)?</p> <p>What we're computing: Express our state \\(|\\psi\\rangle\\) using the Hadamard basis vectors \\(|+\\rangle\\) and \\(|-\\rangle\\) as coordinates.</p> <p>Component in the \\(|+\\rangle\\) direction:</p> \\[\\psi_+ = \\langle + | \\psi \\rangle = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 &amp; 1 \\end{bmatrix} \\cdot \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{2}(1 + 1) = 1\\] <p>Why is this 1? Because \\(|\\psi\\rangle = |+\\rangle\\)! The state is perfectly aligned with the \\(|+\\rangle\\) basis vector. It's like asking: \"How much of \\(\\mathbf{e}_x\\) is in the \\(\\mathbf{e}_x\\) direction?\" Answer: 1 (all of it).</p> <p>Component in the \\(|-\\rangle\\) direction:</p> \\[\\psi_- = \\langle - | \\psi \\rangle = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 &amp; -1 \\end{bmatrix} \\cdot \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} = \\frac{1}{2}(1 - 1) = 0\\] <p>Why is this 0? Because \\(|\\psi\\rangle\\) is orthogonal to \\(|-\\rangle\\). It has zero component in that direction.</p> <p>Result: In the Hadamard basis, the wavefunction is \\([1, 0]\\).</p> <p>What this means:</p> Basis Used Coordinates of \\(\\|\\psi\\rangle\\) Interpretation Computational \\(\\{\\|0\\rangle, \\|1\\rangle\\}\\) \\([\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}]\\) Equal mix of both directions Hadamard \\(\\{\\|+\\rangle, \\|-\\rangle\\}\\) \\([1, 0]\\) Fully in \\(\\|+\\rangle\\) direction, nothing in \\(\\|-\\rangle\\) <p>Same state, different coordinates! Like how \\(\\mathbf{v} = [3, 4]\\) in Cartesian is \\([5, 53.1\u00b0]\\) in polar.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#common-confusion-which-direction-is-the-projection","title":"Common Confusion: Which Direction Is the Projection?","text":"<p>Question: \"Isn't \\(\\langle + | \\psi \\rangle\\) just the projection of the basis vector \\(|+\\rangle\\) onto the state \\(|\\psi\\rangle\\)?\"</p> <p>Answer: No! The notation \\(\\langle + | \\psi \\rangle\\) means:</p> <p>\"Project the state \\(|\\psi\\rangle\\) onto the basis vector \\(|+\\rangle\\)\"</p> <p>Analogy: In 3D, if you have:</p> <ul> <li>Vector: \\(\\mathbf{v} = [3, 4, 0]\\)</li> <li>Basis vector: \\(\\mathbf{e}_x = [1, 0, 0]\\)</li> </ul> <p>The \"\\(x\\)-coordinate\" is: $\\(x = \\mathbf{e}_x \\cdot \\mathbf{v} = [1, 0, 0] \\cdot [3, 4, 0] = 3\\)$</p> <p>You're asking: \"How much of \\(\\mathbf{v}\\) is in the \\(\\mathbf{e}_x\\) direction?\"</p> <p>Same in QM: \\(\\langle + | \\psi \\rangle\\) asks: \"How much of \\(|\\psi\\rangle\\) is in the \\(|+\\rangle\\) direction?\"</p> <p>Why the bra-ket notation?</p> <p>The notation \\(\\langle + |\\) is the \"bra\" (row vector), \\(|\\psi\\rangle\\) is the \"ket\" (column vector):</p> \\[\\langle + | \\psi \\rangle = \\text{row vector} \\times \\text{column vector} = \\text{scalar}\\] <p>This is the inner product that gives you the component!</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#summary-two-bases-one-state","title":"Summary: Two Bases, One State","text":"<p>Let's be crystal clear about what just happened:</p> <p>1. We have ONE state (the thing we're analyzing):</p> \\[|\\psi\\rangle = \\frac{1}{\\sqrt{2}} \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}_{\\text{computational}}\\] <p>2. We expressed it in TWO different coordinate systems:</p> <p>Coordinate System 1: Computational Basis \\(\\{|0\\rangle, |1\\rangle\\}\\) - Basis vectors (the \"axes\"): \\(|0\\rangle = [1, 0]\\), \\(|1\\rangle = [0, 1]\\) - State coordinates: \\(|\\psi\\rangle\\) has wavefunction \\([\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}]\\)</p> <p>Coordinate System 2: Hadamard Basis \\(\\{|+\\rangle, |-\\rangle\\}\\) - Basis vectors (the \"axes\"): \\(|+\\rangle = [\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}]_{\\text{computational}}\\), \\(|-\\rangle = [\\frac{1}{\\sqrt{2}}, -\\frac{1}{\\sqrt{2}}]_{\\text{computational}}\\) - State coordinates: \\(|\\psi\\rangle\\) has wavefunction \\([1, 0]\\)</p> <p>3. Key observation:</p> <p>In computational coordinates, \\(|\\psi\\rangle\\) and \\(|+\\rangle\\) are the same vector: \\([\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}]\\).</p> <p>But they play different roles:</p> <ul> <li>\\(|\\psi\\rangle\\) = the state (subject of analysis)</li> <li>\\(|+\\rangle\\) = one of the axes in the Hadamard coordinate system</li> </ul> <p>Analogy: If your velocity is \\(\\mathbf{v} = [1, 0]\\) m/s (heading east), you could also use that same vector \\([1, 0]\\) as the x-axis of a new coordinate system. Same vector, different roles!</p> <p>The key insight:</p> <p>The state \\(|\\psi\\rangle\\) is the same geometric object in both cases\u2014it's the same quantum system!</p> <p>But its wavefunction (coordinate representation) is different:</p> Coordinate System (Basis) State \\(\\|\\psi\\rangle\\) Coordinates Computational \\(\\{\\lvert 0\\rangle, \\lvert 1\\rangle\\}\\) \\(\\left[\\frac{1}{\\sqrt{2}}, \\frac{1}{\\sqrt{2}}\\right]\\) Hadamard \\(\\{\\lvert +\\rangle, \\lvert -\\rangle\\}\\) \\([1, 0]\\) <p>Analogy: Same vector \\(\\mathbf{v}\\), different coordinate systems:</p> Coordinate System Coordinates Cartesian \\((x, y)\\) \\([3, 4]\\) Polar \\((r, \\theta)\\) \\([5, 53.1\u00b0]\\) <p>Same object, different numbers!</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#infinite-dimensional-case-position-basis","title":"Infinite-Dimensional Case: Position Basis","text":"<p>In standard quantum mechanics, position can be any real number, so the Hilbert space is infinite-dimensional.</p> <p>State: \\(|\\psi\\rangle\\) (abstract vector in infinite-dimensional Hilbert space)</p> <p>Position basis: \\(\\{|x\\rangle : x \\in \\mathbb{R}\\}\\)</p> <p>What is \\(|x\\rangle\\)?</p> <p>The symbol \\(|x\\rangle\\) is a basis vector representing \"the state where the particle is definitely at position \\(x\\).\"</p> <p>Analogy to finite case:</p> Finite (Qubit) Infinite (Position) 2 basis vectors: \\(\\|0\\rangle, \\|1\\rangle\\) Infinite basis vectors: \\(\\|x\\rangle\\) for every \\(x \\in \\mathbb{R}\\) \\(\\|0\\rangle\\) = \"definitely in state 0\" \\(\\|x\\rangle\\) = \"definitely at position \\(x\\)\" Discrete index: 0, 1 Continuous index: \\(x \\in \\mathbb{R}\\) <p>Critical distinction: \\(|x\\rangle\\) is NOT a position vector like \\(\\mathbf{r} = [x, y, z]\\) in classical mechanics. It's a basis vector in Hilbert space that represents a particular position eigenstate.</p> <p>Important: \\(|\\psi\\rangle\\) is NOT a basis vector\u2014it's the state (the thing we're analyzing), expressed as a combination of basis vectors!</p> <p>Wavefunction: For each position \\(x\\), compute the projection:</p> \\[\\psi(x) = \\langle x | \\psi \\rangle\\] <p>What this means:</p> <p>\"Project the state \\(|\\psi\\rangle\\) onto the basis vector \\(|x\\rangle\\) (the state 'definitely at position \\(x\\)')\"</p> <p>Result: This gives a function \\(\\psi: \\mathbb{R} \\to \\mathbb{C}\\) that tells you the \"component\" of \\(|\\psi\\rangle\\) in each position direction.</p> <p>Analogy: Just like we computed:</p> <ul> <li>\\(\\psi_0 = \\langle 0 | \\psi \\rangle\\) for the \\(|0\\rangle\\) direction</li> <li>\\(\\psi_1 = \\langle 1 | \\psi \\rangle\\) for the \\(|1\\rangle\\) direction</li> </ul> <p>Now we compute:</p> <ul> <li>\\(\\psi(x=0) = \\langle 0 | \\psi \\rangle\\) for position \\(x=0\\)</li> <li>\\(\\psi(x=1) = \\langle 1 | \\psi \\rangle\\) for position \\(x=1\\)</li> <li>\\(\\psi(x=2.5) = \\langle 2.5 | \\psi \\rangle\\) for position \\(x=2.5\\)</li> <li>... and so on for every real number \\(x\\)</li> </ul> <p>Since \\(x\\) is continuous, we get a continuous function \\(\\psi(x)\\)!</p> <p>Concrete Example: Gaussian Wavepacket</p> <p>Suppose our state \\(|\\psi\\rangle\\) has the following wavefunction in the position basis:</p> \\[\\psi(x) = \\langle x | \\psi \\rangle = \\frac{1}{(\\pi \\sigma^2)^{1/4}} \\exp\\left(-\\frac{x^2}{2\\sigma^2}\\right)\\] <p>This is a bell curve! Let's evaluate it at specific positions (say \\(\\sigma = 1\\)):</p> Position \\(x\\) Basis Vector Component \\(\\psi(x) = \\langle x \\| \\psi \\rangle\\) \\(x = 0\\) \\(\\|0\\rangle\\) \\(\\psi(0) = \\frac{1}{(\\pi)^{1/4}} \\approx 0.75\\) (maximum) \\(x = 1\\) \\(\\|1\\rangle\\) \\(\\psi(1) = \\frac{1}{(\\pi)^{1/4}} e^{-1/2} \\approx 0.46\\) \\(x = 3\\) \\(\\|3\\rangle\\) \\(\\psi(3) = \\frac{1}{(\\pi)^{1/4}} e^{-9/2} \\approx 0.008\\) (nearly zero) <p>Interpretation: - The state \\(|\\psi\\rangle\\) has large component in the \\(|x=0\\rangle\\) direction (particle likely near origin) - Medium component in the \\(|x=1\\rangle\\) direction - Tiny component in the \\(|x=3\\rangle\\) direction (particle unlikely far from origin)</p> <p>Visual:</p> <pre><code>\u03c8(x)\n  ^\n  |     *\n  |    ***\n  |   *****\n  |  *******\n  | *********\n  |***********\n  +---------------&gt; x\n -3  -1  0  1  3\n</code></pre> <p>The wavefunction \\(\\psi(x)\\) tells you how much of \\(|\\psi\\rangle\\) \"points in the direction\" of each position basis vector \\(|x\\rangle\\).</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#clarification-psirangle-vs-xrangle-which-is-the-basis","title":"Clarification: \\(|\\psi\\rangle\\) vs. \\(|x\\rangle\\) - Which Is the Basis?","text":"<p>Common confusion: \"Are both \\(|\\psi\\rangle\\) and \\(|x\\rangle\\) basis vectors?\"</p> <p>Answer: NO! This is a critical distinction:</p> Symbol What It Is Role \\(\\|\\psi\\rangle\\) The state (thing being analyzed) Like the vector \\(\\mathbf{v} = [3, 4]\\) you're analyzing \\(\\|x\\rangle\\) A basis vector (coordinate axis) Like \\(\\mathbf{e}_x = [1, 0]\\) or \\(\\mathbf{e}_y = [0, 1]\\) <p>\\(|\\psi\\rangle\\) is expressed AS A COMBINATION of basis vectors \\(|x\\rangle\\):</p> \\[|\\psi\\rangle = \\int_{-\\infty}^{\\infty} \\psi(x) \\, |x\\rangle \\, dx\\] <p>This is like writing: \\(\\mathbf{v} = 3\\mathbf{e}_x + 4\\mathbf{e}_y\\)</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#your-classical-intuition-is-rightbut-quantum-is-different","title":"Your Classical Intuition Is Right\u2014But Quantum Is Different!","text":"<p>You said: \"When I hear 'basis vector', it gives me the impression of [0, 0, 1], [0, 1, 0], [1, 0, 0] - linearly independent vectors.\"</p> <p>You're absolutely right! That's exactly what basis vectors are in classical mechanics.</p> <p>The quantum twist:</p> <p>In quantum mechanics, \\(|x\\rangle\\) plays a role analogous to \\(\\mathbf{e}_x, \\mathbf{e}_y, \\mathbf{e}_z\\), but there's a crucial difference:</p> Classical Position Quantum State Position vector: \\(\\mathbf{r} = [x, y, z]\\) State vector: \\(\\|\\psi\\rangle\\) Expressed using basis: \\(\\mathbf{r} = x\\mathbf{e}_x + y\\mathbf{e}_y + z\\mathbf{e}_z\\) Expressed using basis: \\(\\|\\psi\\rangle = \\int \\psi(x) \\|x\\rangle dx\\) Basis vectors: \\(\\mathbf{e}_x, \\mathbf{e}_y, \\mathbf{e}_z\\) (3 of them) Basis vectors: \\(\\|x\\rangle\\) (one for each \\(x \\in \\mathbb{R}\\), infinitely many) Coordinates: \\(x, y, z\\) (numbers) Coordinates: \\(\\psi(x)\\) (the wavefunction!)"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#the-big-difference-what-were-representing","title":"The Big Difference: What We're Representing","text":"<p>Classical mechanics: - Thing: Position of a particle - Representation: \\(\\mathbf{r} = x\\mathbf{e}_x + y\\mathbf{e}_y + z\\mathbf{e}_z\\) - Basis vectors: \\(\\mathbf{e}_x, \\mathbf{e}_y, \\mathbf{e}_z\\) (spatial directions)</p> <p>Quantum mechanics: - Thing: State of a particle - Representation: \\(|\\psi\\rangle = \\int \\psi(x) |x\\rangle dx\\) - Basis vectors: \\(|x\\rangle\\) (not spatial directions, but quantum states!)</p> <p>Key insight: In QM, \\(|x\\rangle\\) doesn't represent \"the direction x\" in space. It represents the quantum state \"particle is at position x\".</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#concrete-example-finite-case-first","title":"Concrete Example: Finite Case First","text":"<p>Let's make this super concrete with a qubit:</p> <p>Basis vectors (the \"axes\"): - \\(|0\\rangle = [1, 0]\\) - \\(|1\\rangle = [0, 1]\\)</p> <p>These are linearly independent, just like \\(\\mathbf{e}_x\\) and \\(\\mathbf{e}_y\\)!</p> <p>State (the thing we're analyzing): $\\(|\\psi\\rangle = \\frac{1}{\\sqrt{2}}|0\\rangle + \\frac{1}{\\sqrt{2}}|1\\rangle = \\frac{1}{\\sqrt{2}}\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}\\)$</p> <p>Is \\(|\\psi\\rangle\\) a basis vector? NO! It's a combination of basis vectors, like \\(\\mathbf{v} = 3\\mathbf{e}_x + 4\\mathbf{e}_y\\).</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#now-the-infinite-case","title":"Now the Infinite Case","text":"<p>Basis vectors (the \"axes\"): - \\(|x\\rangle\\) for every \\(x \\in \\mathbb{R}\\)</p> <p>State (the thing we're analyzing): $\\(|\\psi\\rangle = \\int_{-\\infty}^{\\infty} \\psi(x) \\, |x\\rangle \\, dx\\)$</p> <p>where \\(\\psi(x) = \\langle x | \\psi \\rangle\\) are the coordinates (wavefunction).</p> <p>Is \\(|\\psi\\rangle\\) a basis vector? NO! It's a continuous combination of basis vectors \\(|x\\rangle\\).</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#classical-vs-quantum-interpretation","title":"Classical vs. Quantum Interpretation","text":"<p>Common expectation: \"An arbitrary position x should be expressible in terms of chosen basis vectors.\"</p> <p>Classical mechanics (YES): Position \\(\\mathbf{r}\\) is expressed as \\(\\mathbf{r} = x\\mathbf{e}_x + y\\mathbf{e}_y + z\\mathbf{e}_z\\).</p> <p>Quantum mechanics (DIFFERENT!): We're not expressing positions\u2014we're expressing states!</p> <p>The state \\(|\\psi\\rangle\\) is expressed in the position basis as: $\\(|\\psi\\rangle = \\int \\psi(x) |x\\rangle dx\\)$</p> <p>The basis vectors \\(|x\\rangle\\) are NOT expressing positions\u2014they ARE quantum states (states of \"definitely at position x\").</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#why-this-is-confusing","title":"Why This Is Confusing","text":"<p>The notation \\(|x\\rangle\\) looks like it should mean \"position x\", but it actually means:</p> <p>\"The quantum state where the particle is definitely located at position x\"</p> <p>So \\(|x\\rangle\\) is not a point in space\u2014it's a vector in Hilbert space representing a specific quantum state.</p> <p>Better notation (if we could redesign QM): - \\(|x\\rangle\\) \u2192 \\(|\\text{definitely at } x\\rangle\\) (clearer!)</p> <p>But physicists use \\(|x\\rangle\\) as shorthand.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#summary-classical-vs-quantum-representation","title":"Summary: Classical vs. Quantum Representation","text":"Aspect Classical Mechanics Quantum Mechanics What we represent Position of particle State of particle The thing \\(\\mathbf{r}\\) (position vector) \\(\\|\\psi\\rangle\\) (state vector) Basis vectors \\(\\mathbf{e}_x, \\mathbf{e}_y, \\mathbf{e}_z\\) \\(\\|x\\rangle\\) for each \\(x \\in \\mathbb{R}\\) What basis vectors mean Spatial directions Quantum states (\"at position x\") Number of basis vectors 3 (in 3D space) Infinite (one per position) Representation \\(\\mathbf{r} = x\\mathbf{e}_x + y\\mathbf{e}_y + z\\mathbf{e}_z\\) \\(\\|\\psi\\rangle = \\int \\psi(x) \\|x\\rangle dx\\) Coordinates \\(x, y, z\\) (position components) \\(\\psi(x)\\) (wavefunction) \"Is position a basis vector?\" No, position uses basis vectors No, \\(\\|x\\rangle\\) IS a basis vector <p>Key takeaway: - Classical: Position \\(\\mathbf{r}\\) is expressed using basis vectors \\(\\mathbf{e}_x, \\mathbf{e}_y, \\mathbf{e}_z\\) - Quantum: Basis vector \\(|x\\rangle\\) represents the state \"particle at position x\"</p> <p>This is why quantum mechanics feels weird! We're not representing positions anymore\u2014we're representing states, and the basis vectors themselves encode \"where the particle is.\"</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#key-distinction-now-clear","title":"Key Distinction (Now Clear!)","text":"<p>State \\(|\\psi\\rangle\\) = The quantum system itself (basis-independent) Wavefunction \\(\\psi(x)\\) = Coordinate representation in position basis</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#summary-table","title":"Summary Table","text":"Concept What It Is Analogy State vector \\(\\lvert\\psi\\rangle\\) The quantum system (abstract) The geometric vector \\(\\mathbf{v}\\) Wavefunction \\(\\psi(x)\\) Coordinate representation Cartesian coordinates \\([3, 4, 0]\\) Inner product \\(\\langle x \\lvert \\psi \\rangle\\) Component in direction \\(\\lvert x\\rangle\\) How much of \\(\\mathbf{v}\\) is in \\(x\\)-direction? Different basis Different coordinate system Cartesian vs. polar Same state, different wavefunction Same \\(\\lvert\\psi\\rangle\\), different basis Same \\(\\mathbf{v}\\), different coordinates <p>Why this matters for GRL:</p> <p>In GRL, the reinforcement field \\(Q^+(z)\\) is like a wavefunction\u2014it's the coordinate representation of a state vector in RKHS, expressed in the kernel-induced basis \\(\\{k(z_i, \\cdot)\\}\\).</p> <p>This maps cleanly onto RKHS language, as we'll see in Section 5.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#2-what-does-the-wavefunction-represent","title":"2. What Does the Wavefunction Represent?","text":"<p>The wavefunction does not represent:</p> <ul> <li>\u274c A probability</li> <li>\u274c A physical wave in space</li> <li>\u274c Ignorance in the Bayesian sense</li> </ul> <p>Instead, it represents probability amplitudes.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#the-born-rule","title":"The Born Rule","text":"<p>The Born rule tells us how to extract observable predictions:</p> \\[p(x) = |\\psi(x)|^2\\] <p>Key properties: - \\(\\psi(x)\\) can be positive, negative, or complex - Interference arises because amplitudes add before squaring - Probabilities are derived, not primitive</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#the-fundamental-move","title":"The Fundamental Move","text":"<p>This is the single most important structural move quantum mechanics makes:</p> <p>QM is not a probabilistic theory. It is an amplitude theory from which probabilities are derived.</p> <p>This alone justifies:</p> <ul> <li>Spectral methods</li> <li>Interference-like effects</li> <li>Superposition-based reasoning</li> </ul> <p>\u2014without invoking physics mysticism.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#3-why-one-state-many-wavefunctions","title":"3. Why \"One State, Many Wavefunctions\"?","text":"<p>From the examples above, you've seen that one state \\(|\\psi\\rangle\\) can have different coordinate representations depending on the basis.</p> <p>This is why physicists sometimes say \"the wavefunction\" (singular) and sometimes \"wavefunctions\" (plural):</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#singular-the-wavefunction","title":"Singular: \"The Wavefunction\"","text":"<p>When we say \"the wavefunction,\" we usually mean:</p> <p>The position-basis representation \\(\\psi(x) = \\langle x | \\psi \\rangle\\)</p> <p>This is the most common choice because position is directly measurable.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#plural-different-wavefunctions","title":"Plural: \"Different Wavefunctions\"","text":"<p>When we say \"different wavefunctions,\" we mean different basis representations of the same state:</p> <p>Position basis: $\\(\\psi(x) = \\langle x | \\psi \\rangle \\quad \\text{(position wavefunction)}\\)$</p> <p>Momentum basis: $\\(\\tilde{\\psi}(p) = \\langle p | \\psi \\rangle \\quad \\text{(momentum wavefunction)}\\)$</p> <p>Energy basis: $\\(c_n = \\langle E_n | \\psi \\rangle \\quad \\text{(energy amplitudes)}\\)$</p> <p>These are not different physical states\u2014they are different coordinate charts on the same object, like Cartesian vs. polar coordinates for the same vector.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#connection-to-grl","title":"Connection to GRL","text":"<p>In GRL, when we talk about \"wavefunction-like amplitude fields,\" we mean:</p> <p>The reinforcement field \\(Q^+(z)\\) is one representation of the state in RKHS, specifically the representation in the kernel-induced basis \\(\\{k(z_i, \\cdot)\\}\\).</p> <p>We could also express the same state in different bases (e.g., Fourier basis, wavelet basis), just like quantum states have position and momentum representations.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#4-operators-observables-and-prediction","title":"4. Operators, Observables, and Prediction","text":"<p>In quantum mechanics, nothing observable comes directly from the wavefunction.</p> <p>All predictions are mediated by operators.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#observables-as-hermitian-operators","title":"Observables as Hermitian Operators","text":"<ul> <li>Observables are Hermitian operators \\(\\hat{O}\\)</li> <li>Expected value:</li> </ul> \\[\\langle O \\rangle = \\langle \\psi | \\hat{O} | \\psi \\rangle\\]"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#measurement-probabilities","title":"Measurement Probabilities","text":"<p>Measurement probabilities arise from projection operators:</p> \\[p(o) = |\\hat{P}_o |\\psi\\rangle|^2\\]"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#the-workflow","title":"The Workflow","text":"<p>Quantum mechanics follows this structure:</p> <p>state \u2192 operator \u2192 expectation / distribution</p> <p>Not:</p> <p>state \u2192 probability</p> <p>This is a deep conceptual alignment with GRL's formulation.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#5-mapping-back-to-grl-state-vs-representation","title":"5. Mapping Back to GRL: State vs. Representation","text":"<p>Let's translate each component with discipline.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#a-what-corresponds-to-the-quantum-state","title":"(a) What Corresponds to the Quantum State?","text":"<p>In GRL, the reinforcement field is:</p> \\[Q^+(\\cdot) = \\sum_i w_i \\, k(z_i, \\cdot)\\] <p>This is best interpreted as:</p> <p>The GRL state is the entire reinforcement field as an element of RKHS</p> <p>That is:</p> \\[Q^+ \\in \\mathcal{H}_k\\] <p>This is the analogue of \\(|\\psi\\rangle\\), not of \\(\\psi(x)\\).</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#b-what-corresponds-to-the-wavefunction","title":"(b) What Corresponds to the Wavefunction?","text":"<p>The wavefunction analogue appears only after choosing a query point.</p> <p>Given a \"query configuration\" \\(z = (s, \\theta)\\), the scalar:</p> \\[Q^+(z) = \\langle Q^+, k(z,\\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>is exactly analogous to:</p> \\[\\psi(x) = \\langle x | \\psi \\rangle\\]"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#summary-of-the-mapping","title":"Summary of the Mapping","text":"Quantum Mechanics GRL State vector \\(\\|\\psi\\rangle \\in \\mathcal{H}\\) Reinforcement field \\(Q^+ \\in \\mathcal{H}_k\\) Wavefunction \\(\\psi(x) = \\langle x \\| \\psi \\rangle\\) Value at query \\(Q^+(z) = \\langle Q^+, k(z,\\cdot) \\rangle\\) Position basis \\(\\|x\\rangle\\) Kernel basis \\(k(z, \\cdot)\\) Probability \\(p(x) = \\|\\psi(x)\\|^2\\) Policy \\(\\pi(a\\|s) \\propto \\exp(\\beta Q^+(s,a))\\) <p>Key insight: - \\(Q^+\\) is the state - \\(Q^+(z)\\) is the coordinate representation of that state in the kernel-induced basis</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#6-one-reinforcement-field-or-many","title":"6. One Reinforcement Field or Many?","text":"<p>Now we can answer this precisely.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#strict-answer","title":"Strict Answer","text":"<p>There is one reinforcement field state \\(Q^+\\).</p> <p>But there are many induced wavefunction-like representations, depending on:</p> <ul> <li>Which subspace you project onto</li> <li>Which action slice you fix</li> <li>Which kernel basis you query</li> <li>Which abstraction level you operate at</li> </ul>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#examples-of-different-representations","title":"Examples of Different Representations","text":"<p>Fixing state \\(s\\): \\(Q^+(s, \\cdot)\\) \u2192 action-amplitude field</p> <p>Fixing action parameters \\(\\theta\\): \\(Q^+(\\cdot, \\theta)\\) \u2192 state-amplitude field</p> <p>Projecting onto a concept subspace: \u2192 concept-level amplitude field</p> <p>Marginalizing over actions: \\(V(s) = \\mathbb{E}_\\theta[Q^+(s, \\theta)]\\) \u2192 state value function</p> <p>All of these are representations, not distinct states.</p> <p>This mirrors quantum mechanics exactly.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#7-implications-for-concept-discovery-section-v","title":"7. Implications for Concept Discovery (Section V)","text":"<p>This interpretation does important conceptual work:</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#what-concepts-are","title":"What Concepts Are","text":"<ul> <li>Functional clusters are not mixtures of policies</li> <li>They are coherent subspaces of a single state</li> <li>Spectral clustering identifies approximate eigenstates</li> <li>Hierarchies correspond to coarse-graining of observables</li> </ul>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#concept-formation-as-spectral-decomposition","title":"Concept Formation as Spectral Decomposition","text":"<p>Concept formation becomes:</p> <p>Identifying stable subspaces under the action of GRL's implicit operators</p> <p>This is far stronger than \"kernel clustering\" in the usual ML sense.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#connection-to-part-ii","title":"Connection to Part II","text":"<p>Part II (Emergent Structure &amp; Spectral Abstraction) leverages this:</p> <ul> <li>Spectral methods reveal the natural decomposition of \\(Q^+\\)</li> <li>Eigenmodes of the kernel matrix are \"concept basis states\"</li> <li>Hierarchical structure emerges from nested spectral decompositions</li> </ul>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#8-refined-terminology","title":"8. Refined Terminology","text":"<p>Based on this analysis, we should refine our language.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#instead-of","title":"Instead of:","text":"<p>\"The reinforcement field is a wavefunction over augmented state-action space.\"</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#use","title":"Use:","text":"<p>\"The reinforcement field is a state vector in RKHS, whose projections onto kernel-induced bases yield wavefunction-like amplitude fields over augmented state-action space.\"</p> <p>Why this is better: - Distinguishes state (abstract) from representation (coordinate) - Prevents over-interpretation - Preserves the structural claim - Aligns precisely with quantum mechanics terminology</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#9-what-this-opens-up","title":"9. What This Opens Up","text":"<p>Once this is conceptually clean, several things become almost unavoidable:</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#for-theory-part-ii","title":"For Theory (Part II)","text":"<ul> <li>Section V-C: Frame concepts as approximately invariant subspaces</li> <li>Hierarchies: Nested spectral decompositions</li> <li>World models: Operators acting on the GRL state</li> <li>Complex RKHS: Introduces phase (not just probability)</li> <li>Interference: Meaningful without metaphysics</li> </ul>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#for-implementation","title":"For Implementation","text":"<ul> <li>Query the reinforcement field at different points \u2192 different \"wavefunctions\"</li> <li>Spectral decomposition reveals concept structure</li> <li>Projections onto subspaces enable hierarchical reasoning</li> <li>Phase relationships (in complex RKHS) encode temporal/contextual structure</li> </ul>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#for-understanding","title":"For Understanding","text":"<p>Nothing here requires claiming GRL is quantum mechanics.</p> <p>Only that it lives in the same mathematical universe.</p> <p>That's not mysticism\u2014it's functional analysis doing what it always does.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#10-next-steps-what-operators-does-grl-define","title":"10. Next Steps: What Operators Does GRL Define?","text":"<p>The natural next move is to formalize which operators GRL implicitly defines, because that's where the analogy becomes productive rather than decorative.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#candidate-operators-in-grl","title":"Candidate Operators in GRL","text":"<p>1. Value Functional $\\(\\hat{V}: Q^+ \\mapsto \\mathbb{E}_\\theta[Q^+(\\cdot, \\theta)]\\)$</p> <p>2. MemoryUpdate as State Transition $\\(\\hat{M}: Q^+_t \\mapsto Q^+_{t+1}\\)$</p> <p>3. Concept Projection $\\(\\hat{P}_c: Q^+ \\mapsto \\text{proj}_{\\text{concept}_c}(Q^+)\\)$</p> <p>4. Action Selection $\\(\\hat{A}: (Q^+, s) \\mapsto \\theta^* = \\arg\\max_\\theta Q^+(s, \\theta)\\)$</p> <p>Each of these operators acts on the reinforcement field state, producing either:</p> <ul> <li>Another state (state transition)</li> <li>An expectation value (observable)</li> <li>A projection (reduced representation)</li> </ul> <p>This is exactly how observables work in quantum mechanics.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#11-summary","title":"11. Summary","text":"Question Answer What is the wavefunction? Coordinate representation of a state vector in a chosen basis What does it represent? Probability amplitudes (not probabilities directly) Why \"one\" wavefunction? One state, many representations (different bases) What is the GRL state? The reinforcement field \\(Q^+ \\in \\mathcal{H}_k\\) What is the GRL \"wavefunction\"? The value \\(Q^+(z)\\) at a query point (coordinate representation) One field or many? One state, many projections (action-fields, state-fields, concept-fields) What does this enable? Spectral methods, interference, hierarchical concepts, operator formalism"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#the-core-insight","title":"The Core Insight","text":"<p>Quantum mechanics and GRL share the same mathematical structure:</p> <ul> <li>State = vector in Hilbert space</li> <li>Observations = inner products</li> <li>Probabilities = derived from amplitudes</li> <li>Dynamics = operators on the state</li> <li>Structure = revealed by spectral decomposition</li> </ul> <p>This is not analogy\u2014it is mathematical identity.</p>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#within-this-tutorial","title":"Within This Tutorial","text":"<ul> <li>Previous: RKHS and Quantum Mechanics: A Structural Parallel</li> <li>Next: Complex-Valued RKHS</li> <li>Part II (forthcoming): Spectral methods and concept discovery</li> </ul>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#quantum-mechanics-foundations","title":"Quantum Mechanics Foundations","text":"<ul> <li>Dirac, P. A. M. (1930). The Principles of Quantum Mechanics. Oxford.</li> <li>Ballentine, L. E. (1998). Quantum Mechanics: A Modern Development. World Scientific.</li> <li>Nielsen &amp; Chuang (2010). Quantum Computation and Quantum Information. Cambridge.</li> </ul>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#rkhs-and-functional-analysis","title":"RKHS and Functional Analysis","text":"<ul> <li>Reed &amp; Simon (1980). Functional Analysis. Academic Press.</li> <li>Berlinet &amp; Thomas-Agnan (2004). Reproducing Kernel Hilbert Spaces in Probability and Statistics. Springer.</li> </ul>"},{"location":"GRL0/quantum_inspired/01a-wavefunction-interpretation/#grl-original-paper","title":"GRL Original Paper","text":"<ul> <li>Chiu &amp; Huber (2022). Generalized Reinforcement Learning. arXiv:2208.04822</li> </ul> <p>Last Updated: January 12, 2026</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/","title":"Chapter 2: RKHS Basis, Kernel Amplitudes, and Why GRL Doesn't Need Normalization","text":""},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#motivation","title":"Motivation","text":"<p>In Chapter 1a, we learned that quantum mechanics represents states in Hilbert space, with wavefunctions as coordinate representations in a chosen basis.</p> <p>In Chapter 1, we stated that GRL's reinforcement field \\(Q^+ \\in \\mathcal{H}_k\\) is analogous to a quantum state, with the field value \\(Q^+(z)\\) analogous to a wavefunction \\(\\psi(x)\\).</p> <p>This raises three fundamental questions:</p> <ol> <li>Basis: What is the \"basis\" in RKHS, and how is it chosen?</li> <li>Amplitudes: How do kernel evaluations relate to probability amplitudes?</li> <li>Probabilities: Does GRL need normalized probabilities like quantum mechanics?</li> </ol> <p>This chapter answers all three and reveals a surprising insight: GRL combines QM's amplitude geometry with energy-based models' unnormalized inference.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#1-the-rkhs-basis-kernel-sections","title":"1. The RKHS Basis: Kernel Sections","text":""},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#quantum-mechanics-review","title":"Quantum Mechanics Review","text":"<p>In QM, the position basis is:</p> \\[\\{|x\\rangle : x \\in \\mathbb{R}\\}\\] <p>Each \\(|x\\rangle\\) is a basis vector representing \"particle definitely at position \\(x\\).\"</p> <p>To get coordinates: Project the state \\(|\\psi\\rangle\\) onto a basis vector:</p> \\[\\psi(x) = \\langle x | \\psi \\rangle\\] <p>Key insight: Choosing a specific \\(x\\) doesn't define the whole basis\u2014it selects one basis vector from the continuum.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#rkhs-analogue","title":"RKHS Analogue","text":"<p>In GRL, the RKHS \\(\\mathcal{H}_k\\) has an implicit basis (technically, a frame) given by kernel sections:</p> \\[\\{k(z, \\cdot) : z \\in \\mathcal{Z}\\}\\] <p>where \\(\\mathcal{Z} = \\mathcal{S} \\times \\Theta\\) is the augmented state-action space.</p> <p>What is \\(k(z, \\cdot)\\)?</p> <p>For each point \\(z \\in \\mathcal{Z}\\), the function \\(k(z, \\cdot): \\mathcal{Z} \\to \\mathbb{R}\\) is an element of \\(\\mathcal{H}_k\\).</p> <p>Analogy:</p> Quantum Mechanics GRL (RKHS) \\(\\|x\\rangle\\) = basis vector for position \\(x\\) \\(k(z, \\cdot)\\) = basis vector (frame element) for point \\(z\\) Position basis: \\(\\{\\|x\\rangle : x \\in \\mathbb{R}\\}\\) Kernel basis: \\(\\{k(z, \\cdot) : z \\in \\mathcal{Z}\\}\\) State: \\(\\|\\psi\\rangle \\in \\mathcal{H}\\) State: \\(Q^+ \\in \\mathcal{H}_k\\) Wavefunction: \\(\\psi(x) = \\langle x \\| \\psi \\rangle\\) Field value: \\(Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle_{\\mathcal{H}_k}\\)"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#to-get-coordinates-evaluate-the-inner-product","title":"To Get Coordinates: Evaluate the Inner Product","text":"<p>Given the state \\(Q^+ \\in \\mathcal{H}_k\\) (determined by particles), to find its \"coordinate\" at query point \\(z\\):</p> \\[Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>What's happening:</p> <ol> <li>The state \\(Q^+\\) is already fixed (by the particle memory)</li> <li>Choosing \\(z\\) selects one frame element \\(k(z, \\cdot)\\)</li> <li>The inner product gives the amplitude of \\(Q^+\\) along that direction</li> </ol> <p>This is structurally identical to \\(\\psi(x) = \\langle x | \\psi \\rangle\\)!</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#so-is-the-basis-determined-by-choosing-z","title":"So Is the Basis Determined by Choosing \\(z\\)?","text":"<p>Short answer: Yes\u2014but locally, not globally.</p> <p>Long answer:</p> <p>The kernel-induced basis \\(\\{k(z, \\cdot)\\}\\) exists for all \\(z \\in \\mathcal{Z}\\), just like the position basis exists for all \\(x \\in \\mathbb{R}\\).</p> <p>When you choose a specific \\(z\\):</p> <ul> <li>You're not \"defining the basis\"</li> <li>You're selecting which basis element to project onto</li> </ul> <p>Analogy: Asking \"What is \\(\\psi(3.2)\\)?\" doesn't define the position basis\u2014it just asks for the amplitude along the \\(|3.2\\rangle\\) direction.</p> <p>Same here: \\(Q^+(z_0)\\) asks for the amplitude along the \\(k(z_0, \\cdot)\\) direction.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#2-kernel-amplitudes-overlap-without-probabilities","title":"2. Kernel Amplitudes: Overlap Without Probabilities","text":""},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#quantum-mechanics-amplitudes-probabilities","title":"Quantum Mechanics: Amplitudes \u2192 Probabilities","text":"<p>In QM:</p> <p>Primitive: Complex amplitude \\(\\psi(x)\\)</p> <p>Derived: Probability via Born rule</p> \\[p(x) = |\\psi(x)|^2\\] <p>Normalization:</p> \\[\\int_{-\\infty}^{\\infty} |\\psi(x)|^2 dx = 1\\] <p>Key point: Amplitude is fundamental; probability is derived by squaring and normalizing.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#rkhs-amplitudes-without-probabilities","title":"RKHS: Amplitudes Without Probabilities","text":"<p>In RKHS, a kernel evaluation:</p> \\[k(z_i, z) = \\langle k(z_i, \\cdot), k(z, \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>already looks like an overlap amplitude!</p> <p>But important differences:</p> Property Quantum Mechanics RKHS / GRL Values Complex: \\(\\psi(x) \\in \\mathbb{C}\\) Real: \\(Q^+(z) \\in \\mathbb{R}\\) Normalization Required: \\(\\int \\|\\psi(x)\\|^2 dx = 1\\) Not required Interpretation Probability amplitude Score / value / energy <p>Could you normalize? Yes! You could define:</p> \\[p(z) = \\frac{|Q^+(z)|^2}{\\int_{\\mathcal{Z}} |Q^+(z')|^2 dz'}\\] <p>This is mathematically valid\u2014and in fact, Born machines and quantum-inspired generative models do exactly this.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#but-grl-doesnt-need-this-step","title":"But GRL Doesn't Need This Step","text":"<p>Why not?</p> <p>Because GRL uses \\(Q^+(z)\\) for decision-making and control, not for sampling or probability estimation.</p> <p>Policy inference:</p> \\[\\pi(\\theta | s) \\propto \\exp(\\beta \\, Q^+(s, \\theta))\\] <p>Action selection:</p> \\[\\theta^* = \\arg\\max_\\theta Q^+(s, \\theta)\\] <p>Both use relative values, not normalized probabilities.</p> <p>This brings us to the third question...</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#3-energy-based-models-unnormalized-scores-are-enough","title":"3. Energy-Based Models: Unnormalized Scores Are Enough","text":""},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#the-ebm-perspective","title":"The EBM Perspective","text":"<p>Energy-based models (EBMs) define a probability distribution:</p> \\[p(x) = \\frac{1}{Z} \\exp(-E(x))\\] <p>where \\(Z = \\int \\exp(-E(x')) dx'\\) is the partition function.</p> <p>The key insight:</p> <p>In practice, we never compute \\(Z\\)!</p> <p>Why? Because inference, optimization, and control only require relative energies:</p> <p>Comparing two options:</p> \\[\\frac{p(x_1)}{p(x_2)} = \\frac{\\exp(-E(x_1))}{\\exp(-E(x_2))} = \\exp(E(x_2) - E(x_1))\\] <p>The partition function \\(Z\\) cancels out!</p> <p>Optimization:</p> \\[x^* = \\arg\\min_x E(x)\\] <p>No normalization needed\u2014just find the minimum.</p> <p>Control:</p> \\[u^* = \\arg\\min_u E(x, u)\\] <p>Again, relative values are sufficient.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#grls-position-in-the-landscape","title":"GRL's Position in the Landscape","text":"<p>GRL does the same thing:</p> <p>The field \\(Q^+(z)\\) acts as an unnormalized score / negative energy:</p> \\[E(z) = -Q^+(z)\\] <p>Policy (Boltzmann distribution):</p> \\[\\pi(\\theta | s) \\propto \\exp(\\beta \\, Q^+(s, \\theta))\\] <p>No explicit normalization is computed\u2014the policy is normalized implicitly when actions are sampled or probabilities are queried.</p> <p>Optimization (greedy action):</p> \\[\\theta^* = \\arg\\max_\\theta Q^+(s, \\theta)\\] <p>Only relative values matter.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#where-does-grl-sit","title":"Where Does GRL Sit?","text":"Framework Primitive Normalized? Use Case Quantum Mechanics Amplitude \\(\\psi(x)\\) Yes (Born rule) Predictions about measurements Born Machines Amplitude \\(\\psi(x)\\) Yes Generative modeling Energy-Based Models Energy \\(E(x)\\) No Optimization, inference GRL Field \\(Q^+(z)\\) No Control, decision-making <p>The key insight:</p> <p>GRL borrows the amplitude geometry from quantum mechanics and the unnormalized inference logic from energy-based models.</p> <p>This combination keeps GRL mathematically clean without forcing probability normalization.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#4-three-interpretations-one-object","title":"4. Three Interpretations, One Object","text":"<p>The reinforcement field \\(Q^+\\) can be viewed from three complementary perspectives:</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#perspective-1-hilbert-space-state","title":"Perspective 1: Hilbert Space State","text":"\\[Q^+ \\in \\mathcal{H}_k\\] <p>A vector in the RKHS, determined by particle memory:</p> \\[Q^+ = \\sum_{i=1}^N w_i \\, k(z_i, \\cdot)\\] <p>This is the abstract object\u2014basis-independent.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#perspective-2-amplitude-field-qm-like","title":"Perspective 2: Amplitude Field (QM-like)","text":"\\[Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>The coordinate representation in the kernel-induced basis.</p> <p>This gives the field value at each point\u2014the \"wavefunction\" of the field.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#perspective-3-energy-score-function-ebm-like","title":"Perspective 3: Energy / Score Function (EBM-like)","text":"\\[E(z) = -Q^+(z)\\] <p>An unnormalized score used for inference and control.</p> <p>This enables decision-making without normalization.</p> <p>Nothing is inconsistent here\u2014they're just different readouts of the same state, like:</p> <ul> <li>Abstract vector \\(\\mathbf{v}\\)</li> <li>Cartesian coordinates \\([v_x, v_y, v_z]\\)</li> <li>Potential energy \\(U(\\mathbf{v})\\)</li> </ul> <p>All describe the same object from different angles.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#5-what-grl-does-and-doesnt-claim","title":"5. What GRL Does (and Doesn't) Claim","text":""},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#grl-does-not-claim","title":"GRL Does NOT Claim:","text":"<ul> <li>\u274c \\(Q^+(z)\\) is a probability</li> <li>\u274c Probabilities are derived via Born's rule \\(p(z) \\propto |Q^+(z)|^2\\)</li> <li>\u274c Normalization is required for inference</li> </ul>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#grl-does-claim","title":"GRL DOES Claim:","text":"<ul> <li>\u2705 \\(Q^+\\) is an element of a Hilbert space (RKHS)</li> <li>\u2705 Kernel evaluations act as overlap amplitudes</li> <li>\u2705 Action selection uses unnormalized scores (like EBMs)</li> <li>\u2705 This unifies RKHS geometry, QM-style state representation, and EBM inference</li> </ul>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#a-precise-statement","title":"A Precise Statement","text":"<p>Although kernel evaluations resemble probability amplitudes, GRL does not require normalized probabilities. Inference and control are performed via unnormalized energy-like scores, consistent with modern energy-based models.</p> <p>This keeps the framework mathematically clean and modern.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#6-could-we-define-probabilities-optional-extension","title":"6. Could We Define Probabilities? (Optional Extension)","text":"<p>Natural question: Since kernels have inner product structure like QM, can we define probabilities from them?</p> <p>Answer: Yes! Probabilities could be defined in RKHS. There are multiple options:</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#option-1-born-rule-on-kernel-amplitudes","title":"Option 1: Born Rule on Kernel Amplitudes","text":"\\[p(z) = \\frac{|Q^+(z)|^2}{\\int_{\\mathcal{Z}} |Q^+(z')|^2 dz'}\\] <p>This works! Born machines use this approach.</p> <p>For GRL: - Pro: Direct QM analogy - Con: Requires computing the normalization integral - Con: Real-valued amplitudes don't give interference effects</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#option-2-boltzmann-distribution-what-grl-uses","title":"Option 2: Boltzmann Distribution (What GRL Uses)","text":"\\[\\pi(\\theta | s) = \\frac{\\exp(\\beta Q^+(s, \\theta))}{\\int_{\\Theta} \\exp(\\beta Q^+(s, \\theta')) d\\theta'}\\] <p>This is what GRL implicitly uses for policy!</p> <p>Advantages: - Standard in RL (Boltzmann exploration) - Temperature parameter \\(\\beta\\) controls exploration - No need to compute normalization explicitly (ratio trick)</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#option-3-complex-valued-rkhs-advanced","title":"Option 3: Complex-Valued RKHS (Advanced)","text":"<p>If you extend RKHS to complex values, you get:</p> <ul> <li>Probability amplitudes: \\(\\psi(z) \\in \\mathbb{C}\\)</li> <li>Born rule: \\(p(z) = |\\psi(z)|^2\\)</li> <li>Interference effects from phase!</li> </ul> <p>This is exactly what you explore in <code>02-complex-rkhs.md</code>!</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#7-three-roles-of-choosing-a-point","title":"7. Three Roles of \"Choosing a Point\"","text":"<p>Let's unify the three concepts by understanding what \"choosing \\(z\\)\" means in each context:</p> Concept Question Answered Example Basis selection \"Along which direction am I looking?\" \\(k(z, \\cdot)\\) is the direction Amplitude \"How much does the state align with that direction?\" \\(Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle\\) Probability \"How often would I observe this if I sampled?\" \\(p(z) \\propto \\exp(\\beta Q^+(z))\\) (optional) <p>GRL needs the first two, not the third.</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#8-why-this-matters-for-your-framework","title":"8. Why This Matters for Your Framework","text":""},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#conceptual-clarity","title":"Conceptual Clarity","text":"<p>Understanding that GRL works with unnormalized amplitudes (like EBMs) rather than normalized probabilities (like QM):</p> <ol> <li>Explains why no partition function is needed</li> <li>Justifies the QM analogy (geometry) without overcommitting (Born rule)</li> <li>Positions GRL correctly in the modern ML landscape (alongside EBMs, score-based models)</li> </ol>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#future-extensions","title":"Future Extensions","text":"<p>If you want to develop normalized probability formulations:</p> <p>Option A: Born rule approach (for generative modeling)</p> \\[p(z) \\propto |Q^+(z)|^2\\] <p>Option B: Complex RKHS (for interference and phase semantics)</p> \\[\\psi(z) \\in \\mathbb{C}, \\quad p(z) = |\\psi(z)|^2\\] <p>Both are natural extensions explored in later chapters!</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#practical-implications","title":"Practical Implications","text":"<p>For implementation: - No need to compute partition functions - Use relative values for action selection - Policy normalization handled implicitly</p> <p>For theory: - Clean mathematical framework - Rigorous Hilbert space structure - Flexible enough for future probability extensions</p>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#summary","title":"Summary","text":""},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#key-insights","title":"Key Insights","text":"<ol> <li>RKHS Basis:</li> <li>Kernel sections \\(\\{k(z, \\cdot)\\}\\) form a continuous frame</li> <li>Choosing \\(z\\) selects one frame element, like choosing \\(x\\) selects \\(|x\\rangle\\) in QM</li> <li> <p>The state \\(Q^+\\) exists independently; \\(Q^+(z)\\) is its projection onto \\(k(z, \\cdot)\\)</p> </li> <li> <p>Kernel Amplitudes:</p> </li> <li>\\(Q^+(z)\\) acts like a wavefunction (coordinate representation)</li> <li>But GRL doesn't require normalization (unlike QM)</li> <li> <p>Similar to EBMs working with unnormalized energies</p> </li> <li> <p>Three Interpretations:</p> </li> <li>Abstract: \\(Q^+ \\in \\mathcal{H}_k\\) (Hilbert space state)</li> <li>Coordinate: \\(Q^+(z)\\) (amplitude field)</li> <li> <p>Inference: \\(-Q^+(z)\\) (energy score)</p> </li> <li> <p>Why No Normalization:</p> </li> <li>Decision-making uses relative values</li> <li>Partition function cancels in ratios</li> <li> <p>Same principle as modern EBMs</p> </li> <li> <p>Could Define Probabilities:</p> </li> <li>Born rule: \\(p(z) \\propto |Q^+(z)|^2\\) (possible, not required)</li> <li>Boltzmann: \\(\\pi(\\theta|s) \\propto \\exp(\\beta Q^+(s,\\theta))\\) (what GRL uses)</li> <li>Complex RKHS: \\(p(z) = |\\psi(z)|^2\\) with interference (future extension)</li> </ol>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#key-equations","title":"Key Equations","text":"<p>RKHS basis (frame):</p> \\[\\{k(z, \\cdot) : z \\in \\mathcal{Z}\\} \\subset \\mathcal{H}_k\\] <p>State in RKHS:</p> \\[Q^+ = \\sum_{i=1}^N w_i \\, k(z_i, \\cdot) \\in \\mathcal{H}_k\\] <p>Coordinate representation (amplitude):</p> \\[Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>Energy interpretation:</p> \\[E(z) = -Q^+(z)\\] <p>Policy (Boltzmann):</p> \\[\\pi(\\theta | s) \\propto \\exp(\\beta \\, Q^+(s, \\theta))\\] <p>Analogy to QM:</p> Quantum RKHS \\(\\|\\psi\\rangle \\in \\mathcal{H}\\) \\(Q^+ \\in \\mathcal{H}_k\\) \\(\\psi(x) = \\langle x \\| \\psi \\rangle\\) \\(Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle\\) Basis: \\(\\{\\|x\\rangle\\}\\) Frame: \\(\\{k(z, \\cdot)\\}\\)"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#within-this-tutorial-series","title":"Within This Tutorial Series","text":"<ul> <li>Chapter 1: RKHS-Quantum Structural Parallel</li> <li>Chapter 1a: What Is a Wavefunction?</li> <li>Chapter 3 (complex): Complex-Valued RKHS and Probability Amplitudes</li> </ul>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#back-to-grl-tutorials","title":"Back to GRL Tutorials","text":"<ul> <li>Tutorial Chapter 2: RKHS Foundations</li> <li>Tutorial Chapter 4: Reinforcement Field</li> </ul>"},{"location":"GRL0/quantum_inspired/02-rkhs-basis-and-amplitudes/#related-concepts","title":"Related Concepts","text":"<ul> <li>Energy-Based Models: LeCun et al. (2006). \"A Tutorial on Energy-Based Learning\"</li> <li>Born Machines: Cheng et al. (2018). \"Quantum Generative Adversarial Learning\"</li> <li>RKHS and Kernel Methods: Berlinet &amp; Thomas-Agnan (2004). \"Reproducing Kernel Hilbert Spaces\"</li> </ul> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/","title":"Complex-Valued RKHS and Interference Effects","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#introduction","title":"Introduction","text":"<p>The previous document established that GRL's reinforcement field shares deep structural similarities with quantum mechanical wavefunctions. This document explores what happens when we extend GRL to complex-valued reproducing kernel Hilbert spaces.</p> <p>This is where the analogy becomes mathematically literal\u2014and where entirely new learning dynamics become possible.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#motivation-why-go-complex","title":"Motivation: Why Go Complex?","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#limitations-of-real-valued-rkhs","title":"Limitations of Real-Valued RKHS","text":"<p>In standard GRL, the reinforcement field is real-valued:</p> \\[Q^+(z) \\in \\mathbb{R}\\] <p>This means:</p> <ul> <li>Particles can only reinforce or cancel each other (positive/negative weights)</li> <li>There is no notion of phase</li> <li>Interference is limited to constructive/destructive along a single axis</li> </ul>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#what-complex-numbers-enable","title":"What Complex Numbers Enable","text":"<p>In quantum mechanics, amplitudes are complex:</p> \\[\\psi(x) \\in \\mathbb{C}\\] <p>Probability is \\(|\\psi(x)|^2 = \\psi^*(x) \\cdot \\psi(x)\\), where \\(\\psi^*\\) is the complex conjugate.</p> <p>Complex amplitudes enable:</p> <ul> <li>Phase relationships: Two particles can have the same magnitude but different phases</li> <li>Rich interference: Constructive, destructive, and partial interference depending on phase alignment</li> <li>Rotation in state space: Phase evolution provides natural dynamics</li> <li>Richer representations: Multi-modal distributions with phase structure</li> </ul>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#complex-valued-rkhs-mathematical-foundation","title":"Complex-Valued RKHS: Mathematical Foundation","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#definition","title":"Definition","text":"<p>A complex RKHS is a Hilbert space \\(\\mathcal{H}\\) over the complex field \\(\\mathbb{C}\\), consisting of functions \\(f: \\mathcal{X} \\to \\mathbb{C}\\), equipped with a reproducing kernel \\(k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{C}\\) satisfying:</p> <ol> <li>Hermitian symmetry: \\(k(x, y) = \\overline{k(y, x)}\\) (complex conjugate)</li> <li>Positive semi-definiteness: For any \\(\\{x_i\\}\\) and \\(\\{c_i \\in \\mathbb{C}\\}\\):    $\\(\\sum_{i,j} \\bar{c}_i \\, c_j \\, k(x_i, x_j) \\geq 0\\)$</li> <li>Reproducing property: \\(f(x) = \\langle f, k(x, \\cdot) \\rangle_{\\mathcal{H}}\\)</li> </ol> <p>The inner product is now sesquilinear (conjugate-linear in first argument):</p> \\[\\langle f, g \\rangle = \\int \\overline{f(x)} \\, g(x) \\, dx\\]"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#example-complex-gaussian-kernel","title":"Example: Complex Gaussian Kernel","text":"<p>The Gaussian kernel can be augmented with a phase factor:</p> \\[k_{\\mathbb{C}}(x, y) = \\exp\\left(-\\frac{\\|x-y\\|^2}{2\\sigma^2}\\right) \\cdot e^{i\\phi(x,y)}\\] <p>where \\(\\phi(x,y)\\) encodes directional or temporal relationships.</p> <p>Properties: - Magnitude decreases with distance (as usual) - Phase encodes additional structure - Hermitian: \\(k_{\\mathbb{C}}(x, y) = \\overline{k_{\\mathbb{C}}(y, x)}\\)</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#complex-grl-the-extended-framework","title":"Complex GRL: The Extended Framework","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#1-complex-experience-particles","title":"1. Complex Experience Particles","text":"<p>Each particle carries a position, magnitude, and phase:</p> \\[(z_i, w_i, \\phi_i)\\] <p>where:</p> <ul> <li>\\(z_i = (s_i, \\theta_i)\\): augmented state-action location</li> <li>\\(w_i \\in \\mathbb{R}^+\\): magnitude (importance)</li> <li>\\(\\phi_i \\in [0, 2\\pi)\\): phase angle</li> </ul> <p>The complex weight is:</p> \\[c_i = w_i \\cdot e^{i\\phi_i}\\]"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#2-complex-reinforcement-field","title":"2. Complex Reinforcement Field","text":"<p>The reinforcement field becomes complex-valued:</p> \\[\\Psi(z) = \\sum_i c_i \\, k(z_i, z) \\in \\mathbb{C}\\] <p>This is now literally a wavefunction over augmented state-action space.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#3-energy-from-squared-magnitude","title":"3. Energy from Squared Magnitude","text":"<p>The value (or negative energy) at a point is the squared magnitude:</p> \\[V(z) = |\\Psi(z)|^2 = \\Psi^*(z) \\cdot \\Psi(z)\\] <p>This is the Born rule applied to reinforcement learning.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#interference-effects","title":"Interference Effects","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#constructive-interference","title":"Constructive Interference","text":"<p>Two particles at \\(z_1, z_2\\) with aligned phases (\\(\\phi_1 \\approx \\phi_2\\)):</p> \\[|\\Psi(z)|^2 = |c_1 k(z_1, z) + c_2 k(z_2, z)|^2 \\approx (|c_1| + |c_2|)^2 k(z_1, z)^2\\] <p>The combined effect is greater than the sum of individual contributions.</p> <p>Use case: Reinforce confidence when multiple experiences agree in phase (temporal coherence).</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#destructive-interference","title":"Destructive Interference","text":"<p>Two particles with opposite phases (\\(\\phi_1 = \\phi_2 + \\pi\\)):</p> \\[|\\Psi(z)|^2 \\approx (|c_1| - |c_2|)^2 k(z_1, z)^2\\] <p>The particles partially or fully cancel each other.</p> <p>Use case: Suppress value when experiences disagree in temporal or contextual phase.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#partial-interference","title":"Partial Interference","text":"<p>For general phase difference \\(\\Delta\\phi = \\phi_2 - \\phi_1\\):</p> \\[|\\Psi(z)|^2 = |c_1|^2 + |c_2|^2 + 2|c_1||c_2| \\cos(\\Delta\\phi) \\, k(z_1, z) k(z_2, z)\\] <p>The interference term \\(\\cos(\\Delta\\phi)\\) modulates the interaction:</p> <ul> <li>\\(\\Delta\\phi = 0\\): Constructive (\\(\\cos = +1\\))</li> <li>\\(\\Delta\\phi = \\pi/2\\): No interference (\\(\\cos = 0\\))</li> <li>\\(\\Delta\\phi = \\pi\\): Destructive (\\(\\cos = -1\\))</li> </ul>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#phase-semantics-what-does-phase-represent","title":"Phase Semantics: What Does Phase Represent?","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#1-temporal-phase","title":"1. Temporal Phase","text":"<p>Idea: Encode when an experience occurred.</p> \\[\\phi_i = \\omega \\cdot t_i\\] <p>where \\(t_i\\) is the time of experience \\(i\\), and \\(\\omega\\) is a frequency.</p> <p>Effect: Recent experiences (similar \\(t\\)) interfere constructively. Old experiences may cancel new ones if out of phase.</p> <p>Use case: Temporal credit assignment, recency weighting.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#2-contextual-phase","title":"2. Contextual Phase","text":"<p>Idea: Encode context (e.g., episode ID, environment variant).</p> \\[\\phi_i = 2\\pi \\cdot \\frac{\\text{context}_i}{\\text{num\\_contexts}}\\] <p>Effect: Experiences from the same context interfere constructively; different contexts may interfere destructively.</p> <p>Use case: Multi-task learning, domain adaptation.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#3-directional-phase","title":"3. Directional Phase","text":"<p>Idea: Encode directionality in state-action space.</p> \\[\\phi(x, y) = \\text{angle}(y - x)\\] <p>Effect: Particles pointing in similar directions reinforce; opposite directions cancel.</p> <p>Use case: Vector field learning, flow-based control.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#4-learned-phase","title":"4. Learned Phase","text":"<p>Idea: Let a neural network predict phase:</p> \\[\\phi_i = \\text{NN}_\\phi(z_i, \\text{context})\\] <p>Effect: The network learns what phase relationships are useful.</p> <p>Use case: Discover latent temporal or contextual structure.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#complex-spectral-clustering","title":"Complex Spectral Clustering","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#motivation","title":"Motivation","text":"<p>Part II (Emergent Structure &amp; Spectral Abstraction) uses spectral clustering on the kernel matrix to discover concepts. With complex kernels, this becomes even more powerful.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#complex-kernel-matrix","title":"Complex Kernel Matrix","text":"<p>The Gram matrix is now complex Hermitian:</p> \\[K_{ij} = k_{\\mathbb{C}}(z_i, z_j) \\in \\mathbb{C}\\] <p>Properties:</p> <ul> <li>\\(K^\\dagger = K\\) (Hermitian)</li> <li>Eigenvalues are real</li> <li>Eigenvectors are complex</li> </ul>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#complex-eigenmodes-as-concepts","title":"Complex Eigenmodes as Concepts","text":"<p>Eigendecomposition:</p> \\[K = V \\Lambda V^\\dagger\\] <p>where \\(V\\) contains complex eigenvectors.</p> <p>Interpretation: - Each eigenvector \\(v_j\\) is a concept in function space - Magnitude \\(|v_{ij}|\\) indicates particle \\(i\\)'s contribution to concept \\(j\\) - Phase \\(\\arg(v_{ij})\\) indicates particle \\(i\\)'s phase alignment within concept \\(j\\)</p> <p>Result: Concepts can have internal phase structure, enabling richer hierarchical organization.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#implementation-sketch","title":"Implementation Sketch","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#complex-particle-memory","title":"Complex Particle Memory","text":"<pre><code>class ComplexParticleMemory:\n    def __init__(self):\n        self.positions = []     # z_i: (state, action_params)\n        self.magnitudes = []    # w_i: real weights\n        self.phases = []        # \u03c6_i: angles in [0, 2\u03c0)\n\n    def add(self, z, w, phi):\n        self.positions.append(z)\n        self.magnitudes.append(w)\n        self.phases.append(phi)\n\n    def complex_weights(self):\n        return [w * np.exp(1j * phi) \n                for w, phi in zip(self.magnitudes, self.phases)]\n</code></pre>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#complex-reinforcement-field","title":"Complex Reinforcement Field","text":"<pre><code>def complex_field(z_query, memory, kernel):\n    \"\"\"\n    \u03a8(z) = \u03a3_i c_i k(z_i, z)\n    \"\"\"\n    psi = 0.0 + 0.0j  # Complex accumulator\n    for z_i, c_i in zip(memory.positions, memory.complex_weights()):\n        psi += c_i * kernel(z_i, z_query)\n    return psi\n\ndef value(z_query, memory, kernel):\n    \"\"\"\n    V(z) = |\u03a8(z)|\u00b2\n    \"\"\"\n    psi = complex_field(z_query, memory, kernel)\n    return np.abs(psi)**2\n</code></pre>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#complex-kernel","title":"Complex Kernel","text":"<pre><code>def complex_gaussian_kernel(x, y, sigma=1.0, phase_fn=None):\n    \"\"\"\n    k(x,y) = exp(-||x-y||\u00b2/2\u03c3\u00b2) \u00b7 exp(i\u03c6(x,y))\n    \"\"\"\n    dist_sq = np.sum((x - y)**2)\n    magnitude = np.exp(-dist_sq / (2 * sigma**2))\n\n    if phase_fn is None:\n        phase = 0.0  # Real-valued by default\n    else:\n        phase = phase_fn(x, y)\n\n    return magnitude * np.exp(1j * phase)\n</code></pre>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#potential-applications","title":"Potential Applications","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#1-temporal-credit-assignment","title":"1. Temporal Credit Assignment","text":"<p>Problem: Delayed rewards make credit assignment hard.</p> <p>Complex GRL Solution: - Encode time in phase: \\(\\phi_i = \\omega \\cdot t_i\\) - Recent experiences interfere constructively - Automatic temporal discounting via phase decoherence</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#2-multi-task-learning","title":"2. Multi-Task Learning","text":"<p>Problem: Experiences from different tasks may conflict.</p> <p>Complex GRL Solution: - Encode task ID in phase - Same-task experiences reinforce - Cross-task experiences may cancel (if interference is destructive)</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#3-directional-value-fields","title":"3. Directional Value Fields","text":"<p>Problem: In navigation, direction matters.</p> <p>Complex GRL Solution: - Encode movement direction in phase - Forward-consistent trajectories reinforce - Conflicting directions cancel</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#4-concept-discovery-with-phase-structure","title":"4. Concept Discovery with Phase Structure","text":"<p>Problem: Concepts may have temporal or contextual organization.</p> <p>Complex GRL Solution: - Complex spectral clustering reveals concepts with phase structure - Hierarchical concepts organized by magnitude and phase</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#challenges-and-open-questions","title":"Challenges and Open Questions","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#implementation-challenges","title":"Implementation Challenges","text":"<ol> <li>Complex neural networks: Most ML frameworks assume real-valued parameters</li> <li> <p>Solution: Use complex-valued layers (Trabelsi et al., 2018) or split real/imaginary</p> </li> <li> <p>Interpretability: Complex weights are harder to visualize</p> </li> <li> <p>Solution: Visualize magnitude and phase separately</p> </li> <li> <p>Computational cost: Complex arithmetic is more expensive</p> </li> <li>Solution: Implement in efficient backends (JAX, PyTorch with complex dtypes)</li> </ol>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#theoretical-questions","title":"Theoretical Questions","text":"<ol> <li>What phase functions are useful? Requires empirical investigation</li> <li>How to initialize phases? Random, learned, or hand-designed?</li> <li>When does complex RKHS outperform real RKHS? Depends on problem structure</li> </ol>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#connection-to-quantum-machine-learning","title":"Connection to Quantum Machine Learning","text":"<p>This framework connects GRL to the emerging field of quantum kernel methods:</p> <ul> <li>Havl\u00ed\u010dek et al. (2019): Quantum feature maps naturally produce complex kernels</li> <li>Schuld &amp; Killoran (2019): Quantum models as complex RKHS</li> <li>Lloyd et al. (2020): Quantum algorithms for spectral clustering</li> </ul> <p>Key insight: Even without quantum hardware, complex-valued RKHS can capture structure that real-valued methods miss.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#summary","title":"Summary","text":"Aspect Real RKHS (Part I) Complex RKHS (This Doc) Field \\(Q^+(z) \\in \\mathbb{R}\\) \\(\\Psi(z) \\in \\mathbb{C}\\) Particles \\((z_i, w_i)\\) \\((z_i, w_i, \\phi_i)\\) Interference Additive only Constructive/destructive/partial Phase None Temporal, contextual, directional Spectral Real eigenmodes Complex eigenmodes with phase <p>Key Takeaway: Complex-valued RKHS is a natural extension of GRL that enables richer dynamics, phase-based reasoning, and structured concept discovery.</p>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#next-steps","title":"Next Steps","text":""},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#for-researchers","title":"For Researchers","text":"<ul> <li>Implement complex particle memory and kernels</li> <li>Test on problems with temporal or contextual structure</li> <li>Compare complex vs. real spectral clustering</li> </ul>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#for-theorists","title":"For Theorists","text":"<ul> <li>Prove convergence guarantees for complex RF-SARSA</li> <li>Characterize when complex kernels outperform real kernels</li> <li>Connect to quantum information theory</li> </ul>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#for-part-ii-spectral-abstraction","title":"For Part II (Spectral Abstraction)","text":"<ul> <li>Apply complex spectral clustering to concept discovery</li> <li>Investigate phase structure within concepts</li> <li>Develop hierarchical methods leveraging phase</li> </ul>"},{"location":"GRL0/quantum_inspired/03-complex-rkhs/#references","title":"References","text":"<p>Complex-Valued Neural Networks: - Trabelsi et al. (2018). Deep complex networks. ICLR. - Hirose (2012). Complex-valued neural networks: Advances and applications. Wiley.</p> <p>Quantum Kernel Methods: - Havl\u00ed\u010dek et al. (2019). Supervised learning with quantum-enhanced feature spaces. Nature 567, 209-212. - Schuld &amp; Killoran (2019). Quantum machine learning in feature Hilbert spaces. Physical Review Letters 122, 040504. - Lloyd et al. (2020). Quantum embeddings for machine learning. arXiv:2001.03622.</p> <p>RKHS Theory: - Steinwart &amp; Christmann (2008). Support Vector Machines. Springer (Chapter on complex RKHS). - Sch\u00f6lkopf &amp; Smola (2002). Learning with Kernels. MIT Press.</p> <p>GRL Original Paper: - Chiu &amp; Huber (2022). Generalized Reinforcement Learning. arXiv:2208.04822</p> <p>Last Updated: January 12, 2026</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/","title":"Chapter 4: Slicing the Reinforcement Field \u2014 Action, State, and Concept Projections","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#motivation","title":"Motivation","text":"<p>In Chapter 2, we learned that the reinforcement field \\(Q^+\\) is a state in RKHS, and \\(Q^+(z)\\) is its coordinate representation at point \\(z = (s, a)\\).</p> <p>But \\(z\\) has two components: state \\(s\\) and action \\(a\\).</p> <p>This raises a natural question:</p> <p>Can we slice \\(Q^+\\) along just one dimension\u2014fixing either state or action\u2014to get different \"views\" of the same field?</p> <p>Answer: Yes! And this reveals a powerful structure that enables:</p> <ul> <li>Action wavefunctions: The landscape of possible actions at a given state</li> <li>State wavefunctions: The applicability of an action across states</li> <li>Concept projections: Hierarchical abstractions via subspace projections</li> </ul> <p>This chapter shows how one state \\(Q^+\\) gives rise to multiple coordinate representations, each useful for different purposes.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#1-the-foundation-one-state-many-projections","title":"1. The Foundation: One State, Many Projections","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#quantum-mechanics-parallel","title":"Quantum Mechanics Parallel","text":"<p>In QM, you have one state \\(|\\psi\\rangle\\), but you can view it in different bases:</p> <p>Position representation:</p> \\[\\psi(x) = \\langle x | \\psi \\rangle\\] <p>Momentum representation:</p> \\[\\tilde{\\psi}(p) = \\langle p | \\psi \\rangle\\] <p>Key point: Same state, different coordinate systems.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#grl-analogue","title":"GRL Analogue","text":"<p>In GRL, you have one field \\(Q^+ \\in \\mathcal{H}_k\\), but you can project it onto different subspaces:</p> <p>Full augmented space:</p> \\[Q^+(s, a) = \\langle Q^+, k((s,a), \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>Action slice (fix state):</p> \\[\\psi_s(a) = Q^+(s, a) \\quad \\text{for fixed } s\\] <p>State slice (fix action):</p> \\[\\phi_a(s) = Q^+(s, a) \\quad \\text{for fixed } a\\] <p>Concept projection (subspace):</p> \\[Q^+_{\\text{concept}} = P_k Q^+\\] <p>Key insight: These are not different states\u2014they're different projections of the same state \\(Q^+\\).</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#the-critical-distinction","title":"The Critical Distinction","text":"<p>Before we proceed, let's be absolutely clear about what's what:</p> Object Type Role \\(Q^+ \\in \\mathcal{H}_k\\) State (the thing) The agent's knowledge, encoded as RKHS element \\(k(z, \\cdot)\\) Basis element (frame) Representational scaffolding, fixed \\(Q^+(z)\\) Coordinate (view) Value of state in direction \\(k(z, \\cdot)\\) <p>Crucial:</p> <p>Basis vectors are fixed questions; the state is the system's answer.</p> <p>When you compute \\(Q^+(s, a)\\), you're not creating a new state\u2014you're asking: \"What does the current state look like from this point of view?\"</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#2-action-wavefunction-psi_sa-qs-a","title":"2. Action Wavefunction: \\(\\psi_s(a) = Q^+(s, a)\\)","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#definition","title":"Definition","text":"<p>Fix the state \\(s\\) and let action \\(a\\) vary:</p> \\[\\psi_s: \\Theta \\to \\mathbb{R}$$ $$\\psi_s(a) := Q^+(s, a)\\] <p>This gives you a function over the action space for a specific state.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#what-it-represents","title":"What It Represents","text":"<p>\\(\\psi_s(a)\\) is an amplitude field over actions, not a policy yet.</p> <p>It encodes: - Compatibility between past experience and candidate actions - Multi-modality (multiple plausible actions can have high amplitude) - Smooth generalization across actions via kernel overlap</p> <p>Visual intuition:</p> <pre><code>\u03c8_s(a)\n  ^\n  |        *         *\n  |       ***       ***\n  |      *****     *****\n  |     *******   *******\n  +--------------------------&gt; a\n      a\u2081        a\u2082\n</code></pre> <p>Two peaks suggest two good actions at state \\(s\\).</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#from-amplitude-to-policy","title":"From Amplitude to Policy","text":"<p>To get a policy, apply the Boltzmann transformation:</p> \\[\\pi(a | s) = \\frac{\\exp(\\beta \\, \\psi_s(a))}{\\int_{\\Theta} \\exp(\\beta \\, \\psi_s(a')) da'}\\] <p>Analogy to QM: - Wavefunction \\(\\psi_s(a)\\) \u2192 amplitude landscape - Boltzmann rule \u2192 measurement process (like Born rule)</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#practical-use-cases","title":"Practical Use Cases","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#1-continuous-control","title":"1. Continuous Control","text":"<p>Standard RL: Pick \\(a^* = \\arg\\max_a Q(s, a)\\) (one point)</p> <p>With action wavefunction: See the full landscape - Multiple peaks \u2192 multiple strategies - Smooth ridges \u2192 continuous manifolds of good actions - Natural for robotics and motor control</p> <p>Example: Robot grasping - \\(\\psi_s(a)\\) shows all viable grips - Controller can smoothly interpolate between them</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#2-action-substitution","title":"2. Action Substitution","text":"<p>If two actions have high kernel overlap, they appear as neighboring peaks in \\(\\psi_s(a)\\).</p> <p>This gives graded replaceability: - Action \\(a_1\\) unavailable? Use nearby \\(a_2\\) with similar amplitude - No need for discrete \"backup plans\"\u2014structure is inherent</p> <p>Example: Tool use - \"Hammer\" unavailable \u2192 \"mallet\" is nearby in action space - Amplitude field shows compatibility automatically</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#3-exploration-strategies","title":"3. Exploration Strategies","text":"<p>Standard: \\(\\epsilon\\)-greedy (uniform noise) or Boltzmann (temperature)</p> <p>With action wavefunction: - Sample from high-amplitude regions (not just max) - Preserves structure (won't explore clearly bad actions) - Natural multi-modal exploration</p> <p>Example: Game playing - Multiple openings have high amplitude - Explore all of them proportional to their support</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#4-option-discovery","title":"4. Option Discovery","text":"<p>Observation: Peaks in \\(\\psi_s(a)\\) correspond to natural options</p> <p>Algorithm: 1. Compute \\(\\psi_s(a)\\) for visited states \\(s\\) 2. Cluster peaks across states 3. Each cluster = a discovered option</p> <p>Why this works: Options are actions that \"make sense together\" across states\u2014exactly what amplitude clustering captures.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#3-state-wavefunction-phi_as-qs-a","title":"3. State Wavefunction: \\(\\phi_a(s) = Q^+(s, a)\\)","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#definition_1","title":"Definition","text":"<p>Now reverse the roles: fix the action \\(a\\) and let state \\(s\\) vary:</p> \\[\\phi_a: \\mathcal{S} \\to \\mathbb{R}$$ $$\\phi_a(s) := Q^+(s, a)\\] <p>This gives you a function over the state space for a specific action.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#what-it-represents_1","title":"What It Represents","text":"<p>\\(\\phi_a(s)\\) is an applicability amplitude\u2014it answers:</p> <p>\"Where in state space does this action make sense?\"</p> <p>High values mean: - This action has historically aligned well with similar states - Kernel overlap supports generalization here - Preconditions (implicit) are satisfied</p> <p>Low values mean: - Action is structurally incompatible with this region - Either never tried here, or tried and failed</p> <p>Visual intuition:</p> <pre><code>State space:\n\n  s\u2082 |  \u00b7\u00b7\u00b7  \u2588\u2588\u2588\u2588\u2588\u2588\u2588  \u00b7\u00b7\u00b7   \u2190 \u03c6_a(s) high\n     |       \u2588\u2588\u2588\u2588\u2588\u2588\u2588\n  s\u2081 |       \u2588\u2588\u2588\u2588\u2588\u2588\u2588\n     +------------------------\n         Region where action a works\n</code></pre>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#why-this-is-novel","title":"Why This Is Novel","text":"<p>Standard RL: Actions are arbitrary choices\u2014any action can be taken anywhere (modulo physical constraints).</p> <p>GRL with state wavefunction: Actions have applicability regions\u2014some actions \"belong\" in certain states.</p> <p>This is closer to human reasoning:</p> <p>\"This tool works here, not there.\"</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#practical-use-cases_1","title":"Practical Use Cases","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#1-implicit-precondition-learning","title":"1. Implicit Precondition Learning","text":"<p>Without symbolic rules, learn where actions apply.</p> <p>Example: Robotics - \"Open door\" action: \\(\\phi_{\\text{open}}(s)\\) high when door is closed and reachable - \"Pour liquid\" action: \\(\\phi_{\\text{pour}}(s)\\) high when container is held upright</p> <p>No hand-coded logic\u2014just learned structure!</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#2-affordance-maps","title":"2. Affordance Maps","text":"<p>In robotics, \\(\\phi_a(s)\\) is literally an affordance map.</p> <p>Affordance: \"What can I do here?\"</p> <p>GRL answer: For each action \\(a\\), \\(\\phi_a(s)\\) shows where it affords interaction.</p> <p>Example: Navigation - \\(\\phi_{\\text{walk}}(s)\\): high on floors, low on walls - \\(\\phi_{\\text{grasp}}(s)\\): high near objects, low in empty space</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#3-safety-constraints","title":"3. Safety Constraints","text":"<p>Define: An action \\(a\\) is unsafe at state \\(s\\) if \\(\\phi_a(s) &lt; \\tau\\) (threshold).</p> <p>This gives implicit safety without explicit rules: - Low amplitude \u2192 historically bad outcomes - Safe policy: only consider actions with \\(\\phi_a(s) &gt; \\tau\\)</p> <p>Example: Autonomous driving - \\(\\phi_{\\text{accelerate}}(s)\\): low when obstacle ahead - Learned from experience, not programmed</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#4-world-model-filtering","title":"4. World Model Filtering","text":"<p>Setup: Model-based RL with a world model predicting future states</p> <p>Challenge: Model might predict physically possible but practically infeasible states</p> <p>Solution: Filter by action applicability - World model: \"You could be in state \\(s'\\)\" - GRL: \"But action \\(a\\) doesn't work there\" (low \\(\\phi_a(s')\\)) - Keep only reachable states</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#5-skill-discovery","title":"5. Skill Discovery","text":"<p>Observation: Actions with compact support (high \\(\\phi_a\\) in small region) are natural skills.</p> <p>Algorithm: 1. Compute \\(\\phi_a(s)\\) for all actions 2. Find actions with localized high-amplitude regions 3. Each compact region defines a skill</p> <p>Why: Skills are actions that work in specific contexts\u2014exactly what state wavefunctions capture.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#4-concept-subspace-projections-p_k-q","title":"4. Concept Subspace Projections: \\(P_k Q^+\\)","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#beyond-pointwise-projections","title":"Beyond Pointwise Projections","text":"<p>So far, we've projected onto pointwise bases \\(k(z, \\cdot)\\).</p> <p>Now let's project onto subspaces discovered by spectral clustering (Section V of the original paper).</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#definition_2","title":"Definition","text":"<p>Suppose spectral clustering identifies an eigenspace:</p> \\[\\mathcal{C}_k \\subset \\mathcal{H}_k\\] <p>spanned by eigenfunctions \\(\\{\\phi_1, \\ldots, \\phi_m\\}\\) associated with a functional cluster.</p> <p>Project the field onto this subspace:</p> \\[P_k Q^+ = \\sum_{i=1}^m \\langle Q^+, \\phi_i \\rangle \\phi_i\\] <p>This is still the same state, just viewed through a coarse lens.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#what-this-represents","title":"What This Represents","text":"<p>A concept-level amplitude field answers:</p> <p>\"How strongly does the current situation activate this concept?\"</p> <p>Not: - \u274c A discrete label - \u274c A hard cluster assignment - \u274c A symbolic rule</p> <p>But: - \u2705 A graded, geometric activation - \u2705 Smooth interpolation between concepts - \u2705 Hierarchical structure</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#connection-to-concepts","title":"Connection to Concepts","text":"<p>Traditional clustering: Assign data point to one cluster</p> <p>GRL concepts: Compute activation of each concept subspace</p> \\[\\text{activation}_k = \\|P_k Q^+\\|_{\\mathcal{H}_k}\\] <p>This is a functional, not discrete, view of concepts.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#practical-use-cases_2","title":"Practical Use Cases","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#1-hierarchical-decision-making","title":"1. Hierarchical Decision Making","text":"<p>High-level: Concept activations determine strategy</p> <p>Low-level: Action wavefunction \\(\\psi_s(a)\\) implements strategy</p> <p>Example: Navigation - High-level concept: \"In doorway\" (concept subspace) - Low-level action: \"Turn left\" (action wavefunction) - Concept modulates action field</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#2-interpretability","title":"2. Interpretability","text":"<p>Concept activation curves over time are interpretable trajectories:</p> <pre><code>Concept activation\n  ^\n  | C\u2081 \u2500\u2500\u2500\u2500\u2500\u2500\u256e\n  |          \u2570\u2500 C\u2082 \u2500\u2500\u2500\u2500\u2500\u256e\n  |                     \u2570\u2500 C\u2083\n  +-------------------------&gt; time\n</code></pre> <p>\"Agent transitioned from concept C\u2081 to C\u2082 to C\u2083\"</p> <p>No symbolic labels needed\u2014just geometric structure!</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#3-transfer-learning","title":"3. Transfer Learning","text":"<p>Observation: Concept subspaces are more stable than raw particles.</p> <p>Why: Concepts capture abstract structure, not specific experiences.</p> <p>Transfer algorithm: 1. Learn concept subspaces in source task 2. Transfer subspaces to target task 3. Re-learn particle weights within subspaces</p> <p>This is natural compositional transfer.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#5-unifying-view-one-state-multiple-projections","title":"5. Unifying View: One State, Multiple Projections","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#the-common-structure","title":"The Common Structure","text":"<p>All three cases share the same mathematical structure:</p> <ol> <li>There is one GRL state \\(Q^+ \\in \\mathcal{H}_k\\)</li> <li>You never clone it or create variants</li> <li>You only project, slice, or coarse-grain it</li> </ol> <p>This mirrors quantum mechanics exactly:</p> Quantum Mechanics GRL (RKHS) State vector \\(\\|\\psi\\rangle\\) Reinforcement field \\(Q^+\\) Basis \\(\\|x\\rangle\\) Kernel basis \\(k(z, \\cdot)\\) Wavefunction \\(\\psi(x) = \\langle x \\| \\psi \\rangle\\) Field value \\(Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle\\) Observable (position, momentum) Projection (action, state, concept)"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#projection-operations-summary","title":"Projection Operations Summary","text":"Projection Type Operation Result Use Case Full field \\(Q^+(s, a)\\) Value at point \\((s, a)\\) Standard RL value Action slice \\(\\psi_s(a) = Q^+(s, a)\\) Action landscape at \\(s\\) Continuous control, exploration State slice \\(\\phi_a(s) = Q^+(s, a)\\) Applicability of \\(a\\) across states Preconditions, affordances, safety Concept subspace \\(P_k Q^+\\) Concept activation Hierarchical RL, interpretability, transfer"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#why-this-matters","title":"Why This Matters","text":"<p>Once you see this structure, the QM analogy stops being decorative and starts being **constraining\u2014in a good way.**</p> <p>It tells you: - How to define new projections (other subspaces?) - Why actions and states are dual under projection - How to compose operations (project, then project again?)</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#6-action-state-duality","title":"6. Action-State Duality","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#the-symmetry","title":"The Symmetry","text":"<p>Notice the beautiful symmetry:</p> <p>Action wavefunction:</p> \\[\\psi_s(a) = Q^+(s, a) \\quad \\text{(fix } s\\text{, vary } a\\text{)}\\] <p>State wavefunction:</p> \\[\\phi_a(s) = Q^+(s, a) \\quad \\text{(fix } a\\text{, vary } s\\text{)}\\] <p>These are the same function, just indexed differently!</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#deeper-implications","title":"Deeper Implications","text":"<p>This suggests actions and states might be more symmetric than traditional RL assumes.</p> <p>Standard RL: - States = where you are - Actions = what you do - Asymmetric roles</p> <p>GRL: - States and actions are coordinates in augmented space \\(\\mathcal{Z} = \\mathcal{S} \\times \\Theta\\) - Projections along either dimension are equally valid - Symmetric structure</p> <p>This could enable: - Dual learning: Learn about actions by exploring states, and vice versa - Compositional policies: Combine state-based and action-based representations - Transfer: Actions discovered in one state space transfer to another</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#7-from-projections-to-operators-preview","title":"7. From Projections to Operators (Preview)","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#the-next-level","title":"The Next Level","text":"<p>So far, we've discussed projections\u2014static views of \\(Q^+\\).</p> <p>Natural question: What transforms \\(Q^+\\)?</p> <p>Answer: Operators!</p> <p>Examples: - MemoryUpdate: Operator that updates \\(Q^+\\) given new experience - Action conditioning: Operator that shifts field based on action - Abstraction: Operator that projects onto concept subspaces</p> <p>This is where dynamics live\u2014not in the state, but in the operators that transform it.</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#operator-formalism-preview","title":"Operator Formalism (Preview)","text":"<p>In QM, observables are Hermitian operators \\(\\hat{O}\\).</p> <p>In GRL, we can define:</p> <p>Memory update operator:</p> \\[\\hat{M}(e): \\mathcal{H}_k \\to \\mathcal{H}_k$$ $$Q^+ \\mapsto Q^+ + w_{new} k(z_{new}, \\cdot)\\] <p>Policy operator:</p> \\[\\hat{\\Pi}_s: \\mathcal{H}_k \\to L^2(\\Theta)$$ $$Q^+ \\mapsto \\psi_s(a) = Q^+(s, a)\\] <p>This will be explored in future chapters!</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#summary","title":"Summary","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#key-concepts","title":"Key Concepts","text":"<ol> <li>One State, Many Views</li> <li>\\(Q^+ \\in \\mathcal{H}_k\\) is the state</li> <li> <p>\\(Q^+(z), \\psi_s(a), \\phi_a(s), P_k Q^+\\) are projections</p> </li> <li> <p>Action Wavefunction \\(\\psi_s(a) = Q^+(s, a)\\)</p> </li> <li>Amplitude field over actions at state \\(s\\)</li> <li> <p>Use: continuous control, exploration, option discovery</p> </li> <li> <p>State Wavefunction \\(\\phi_a(s) = Q^+(s, a)\\)</p> </li> <li>Applicability of action \\(a\\) across states</li> <li> <p>Use: preconditions, affordances, safety, skills</p> </li> <li> <p>Concept Projections \\(P_k Q^+\\)</p> </li> <li>Subspace projections onto concept eigenspaces</li> <li> <p>Use: hierarchical RL, interpretability, transfer</p> </li> <li> <p>Action-State Duality</p> </li> <li>Symmetric roles in augmented space</li> <li>Enables dual learning and compositional policies</li> </ol>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#key-equations","title":"Key Equations","text":"<p>Action wavefunction:</p> \\[\\psi_s(a) = Q^+(s, a) = \\sum_i w_i k((s_i, a_i), (s, a))\\] <p>State wavefunction:</p> \\[\\phi_a(s) = Q^+(s, a) = \\sum_i w_i k((s_i, a_i), (s, a))\\] <p>Concept projection:</p> \\[P_k Q^+ = \\sum_{j=1}^m \\langle Q^+, \\phi_j \\rangle_{\\mathcal{H}_k} \\phi_j\\] <p>Policy from action wavefunction:</p> \\[\\pi(a|s) = \\frac{\\exp(\\beta \\, \\psi_s(a))}{\\int_{\\Theta} \\exp(\\beta \\, \\psi_s(a')) da'}\\]"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#what-this-enables","title":"What This Enables","text":"<p>Theoretical: - Rigorous projection formalism - Action-state duality - Operator-based dynamics</p> <p>Practical: - Continuous control with full action landscapes - Implicit precondition and affordance learning - Hierarchical RL via concept activations - Natural skill and option discovery - Safety via applicability constraints</p>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#within-this-series","title":"Within This Series","text":"<ul> <li>Chapter 1a: State Vector vs. Wavefunction</li> <li>Chapter 2: RKHS Basis and Amplitudes</li> <li>Chapter 5 (future): Operators on the Reinforcement Field</li> </ul>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#grl-tutorials","title":"GRL Tutorials","text":"<ul> <li>Tutorial Chapter 4: Reinforcement Field Basics</li> <li>Tutorial Chapter 5: Particle Memory</li> </ul>"},{"location":"GRL0/quantum_inspired/04-action-and-state-fields/#related-work","title":"Related Work","text":"<p>Eigenoptions: - Machado et al. (2017). \"A Laplacian Framework for Option Discovery.\" ICML.</p> <p>Affordances in RL: - Khetarpal et al. (2020). \"What can I do here? A theory of affordances in reinforcement learning.\" ICML.</p> <p>Hierarchical RL: - Sutton et al. (1999). \"Between MDPs and semi-MDPs: A framework for temporal abstraction in RL.\"</p> <p>Operator Formalism: - Barreto et al. (2017). \"Successor Features for Transfer in Reinforcement Learning.\" NIPS.</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/","title":"Chapter 5: Concept Subspaces, Projections, and Measurement Theory","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#motivation","title":"Motivation","text":"<p>In Chapter 4, we saw how one state \\(Q^+\\) gives rise to multiple coordinate representations by projecting onto different subspaces (action slices, state slices).</p> <p>But all those projections were pointwise\u2014we projected onto individual basis elements \\(k(z, \\cdot)\\).</p> <p>Natural question: Can we project onto multi-dimensional subspaces discovered by spectral analysis?</p> <p>Answer: Yes! And this gives us a rigorous framework for concepts in reinforcement learning.</p> <p>This chapter develops:</p> <ul> <li>Concepts as invariant subspaces (not clusters)</li> <li>Projection operators for concept activation</li> <li>Measurement theory connecting to quantum mechanics</li> <li>Hierarchical composition via nested subspaces</li> <li>Practical algorithms for concept-driven learning</li> </ul> <p>This formalizes Part II (Emergent Structure &amp; Spectral Abstraction) of the GRL tutorial paper.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#1-from-spectral-clustering-to-concept-subspaces","title":"1. From Spectral Clustering to Concept Subspaces","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#the-problem-with-clusters","title":"The Problem with Clusters","text":"<p>Traditional clustering (k-means, hierarchical, spectral) produces:</p> <ul> <li>Discrete assignments: Point \\(x\\) belongs to cluster \\(k\\)</li> <li>Hard boundaries: Sharp transitions between clusters</li> <li>No smooth interpolation: Can't blend concepts</li> </ul> <p>This doesn't match how concepts work in cognition or RL!</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#the-grl-approach-functional-clustering","title":"The GRL Approach: Functional Clustering","text":"<p>Section V of the original paper introduces spectral clustering in RKHS:</p> <ol> <li>Compute kernel matrix \\(K_{ij} = k(z_i, z_j)\\)</li> <li>Eigendecomposition: \\(K = \\Phi \\Lambda \\Phi^T\\)</li> <li>Cluster eigenvectors by similarity</li> <li>Eigenmodes = \"concepts\"</li> </ol> <p>But what does this mean mathematically?</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#concepts-as-subspaces","title":"Concepts as Subspaces","text":"<p>Key insight: Each cluster of eigenvectors defines a subspace in RKHS.</p> <p>Formal definition:</p> <p>Let \\(\\{\\phi_{k,1}, \\phi_{k,2}, \\ldots, \\phi_{k,m_k}\\}\\) be eigenvectors in cluster \\(k\\).</p> <p>Concept \\(k\\) is the subspace:</p> \\[\\mathcal{C}_k = \\text{span}\\{\\phi_{k,1}, \\phi_{k,2}, \\ldots, \\phi_{k,m_k}\\} \\subset \\mathcal{H}_k\\] <p>Properties:</p> <ul> <li>\\(\\mathcal{C}_k\\) is a linear subspace of the RKHS</li> <li>Dimension: \\(\\dim(\\mathcal{C}_k) = m_k\\)</li> <li>Orthogonal decomposition: \\(\\mathcal{H}_k = \\bigoplus_k \\mathcal{C}_k \\oplus \\mathcal{C}_{\\perp}\\)</li> </ul>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#why-subspaces-not-clusters","title":"Why Subspaces, Not Clusters?","text":"<p>Subspaces give you:</p> <ol> <li>Smooth activation: Degree of membership, not binary</li> <li>Compositionality: Combine multiple concepts</li> <li>Interpolation: Blend between concepts</li> <li>Hierarchy: Nested subspaces = hierarchical concepts</li> <li>Operators: Well-defined projection and measurement</li> </ol> <p>Clusters only give you: Hard assignments.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#2-projection-operators","title":"2. Projection Operators","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#definition","title":"Definition","text":"<p>For concept subspace \\(\\mathcal{C}_k\\) with orthonormal basis \\(\\{\\phi_{k,i}\\}_{i=1}^{m_k}\\):</p> <p>Projection operator:</p> \\[P_k: \\mathcal{H}_k \\to \\mathcal{C}_k\\] \\[P_k f = \\sum_{i=1}^{m_k} \\langle f, \\phi_{k,i} \\rangle_{\\mathcal{H}_k} \\phi_{k,i}\\] <p>For the reinforcement field:</p> \\[P_k Q^+ = \\sum_{i=1}^{m_k} \\langle Q^+, \\phi_{k,i} \\rangle_{\\mathcal{H}_k} \\phi_{k,i}\\]"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#properties","title":"Properties","text":"<p>1. Idempotence:</p> \\[P_k^2 = P_k\\] <p>(Projecting twice = projecting once)</p> <p>2. Orthogonality:</p> \\[P_k P_\\ell = 0 \\quad \\text{for } k \\neq \\ell\\] <p>(Different concepts are orthogonal)</p> <p>3. Completeness:</p> \\[\\sum_k P_k + P_{\\perp} = I\\] <p>(Concepts span the full space)</p> <p>4. Self-adjoint:</p> \\[\\langle P_k f, g \\rangle = \\langle f, P_k g \\rangle\\] <p>(Symmetric inner product)</p> <p>These are exactly the properties of quantum mechanical projection operators!</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#visual-intuition","title":"Visual Intuition","text":"<p>In 3D, projecting onto a plane:</p> <pre><code>         Q\u207a (state)\n          \u2022\n         /|\\\n        / | \\\n       /  |  \\\n      /   |   \\  \u2190 projection\n     /____|____\\ \n    plane (concept subspace)\n</code></pre> <p>\\(P_k Q^+\\) is the \"shadow\" of \\(Q^+\\) on the concept subspace.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#computational-form","title":"Computational Form","text":"<p>Given: - Reinforcement field: \\(Q^+ = \\sum_i w_i k(z_i, \\cdot)\\) - Concept basis: \\(\\{\\phi_{k,1}, \\ldots, \\phi_{k,m_k}\\}\\)</p> <p>Compute projection:</p> \\[P_k Q^+ = \\sum_{j=1}^{m_k} \\underbrace{\\left(\\sum_i w_i \\langle k(z_i, \\cdot), \\phi_{k,j} \\rangle\\right)}_{\\text{coefficient } c_{k,j}} \\phi_{k,j}\\] <p>In matrix form: \\(\\mathbf{c}_k = \\mathbf{K} \\mathbf{w}\\) where:</p> <ul> <li>\\(K_{ji} = \\langle k(z_i, \\cdot), \\phi_{k,j} \\rangle\\)</li> <li>\\(\\mathbf{w} = [w_1, \\ldots, w_N]^T\\)</li> </ul>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#3-concept-activation-observables","title":"3. Concept Activation: Observables","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#the-measurement-question","title":"The Measurement Question","text":"<p>Given field \\(Q^+\\) and concept \\(k\\), we want to measure:</p> <p>\"How strongly does the current field activate this concept?\"</p> <p>This is exactly the quantum measurement problem!</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#concept-activation-observable","title":"Concept Activation Observable","text":"<p>Definition:</p> \\[A_k = \\|P_k Q^+\\|_{\\mathcal{H}_k}^2\\] <p>Expanded form:</p> \\[A_k = \\langle P_k Q^+, P_k Q^+ \\rangle_{\\mathcal{H}_k} = \\sum_{i=1}^{m_k} |\\langle Q^+, \\phi_{k,i} \\rangle|^2\\] <p>Interpretation: Sum of squared projections onto concept basis vectors.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#properties_1","title":"Properties","text":"<p>1. Non-negativity:</p> \\[A_k \\geq 0\\] <p>2. Boundedness:</p> \\[\\sum_k A_k \\leq \\|Q^+\\|^2\\] <p>(Total activation bounded by field strength)</p> <p>3. Normalized activation:</p> \\[\\tilde{A}_k = \\frac{A_k}{\\sum_\\ell A_\\ell + \\|P_{\\perp} Q^+\\|^2}\\] <p>gives a probability-like distribution over concepts.</p> <p>4. Continuity:</p> <p>\\(A_k\\) varies smoothly as \\(Q^+\\) evolves\u2014no discrete jumps!</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#connection-to-quantum-mechanics","title":"Connection to Quantum Mechanics","text":"Quantum Mechanics GRL Concepts Observable \\(\\hat{O}\\) Projection operator \\(P_k\\) Eigenspace \\(\\mathcal{H}_\\lambda\\) Concept subspace \\(\\mathcal{C}_k\\) State \\(\\|\\psi\\rangle\\) Field \\(Q^+\\) Measurement outcome Concept activation \\(A_k\\) Born rule: \\(p = \\|\\langle \\lambda \\| \\psi \\rangle\\|^2\\) Activation: \\(A_k = \\|P_k Q^+\\|^2\\) <p>This is not an analogy\u2014it's the same mathematical structure!</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#4-concept-conditioned-representations","title":"4. Concept-Conditioned Representations","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#projected-field-values","title":"Projected Field Values","text":"<p>Standard field evaluation:</p> \\[Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>Concept-conditioned evaluation:</p> \\[Q^+_k(z) = \\langle P_k Q^+, k(z, \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>Interpretation: Value at \\(z\\) according to concept \\(k\\) only.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#concept-conditioned-policy","title":"Concept-Conditioned Policy","text":"<p>Standard policy:</p> \\[\\pi(a|s) \\propto \\exp(\\beta Q^+(s, a))\\] <p>Concept-conditioned policy:</p> \\[\\pi_k(a|s) \\propto \\exp(\\beta Q^+_k(s, a))\\] <p>Use case: Different concepts induce different policies - Concept \"explore\" \u2192 high entropy policy - Concept \"exploit\" \u2192 peaked policy - Concept \"avoid\" \u2192 negative values</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#action-wavefunction-per-concept","title":"Action Wavefunction per Concept","text":"<p>From Chapter 4, we had action wavefunction \\(\\psi_s(a) = Q^+(s, a)\\).</p> <p>Concept-specific action wavefunction:</p> \\[\\psi_{s,k}(a) = Q^+_k(s, a)\\] <p>Interpretation: Action landscape at state \\(s\\) according to concept \\(k\\).</p> <p>Visual:</p> <pre><code>Full field:     \u03c8_s(a)\n                  ***\n                 *****\n\nConcept 1:      \u03c8_{s,1}(a)\n                  *\n                 ***\n                (sharp peak)\n\nConcept 2:      \u03c8_{s,2}(a)\n                *****\n               *******\n              (broad support)\n</code></pre> <p>Different concepts emphasize different action modes!</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#5-hierarchical-composition","title":"5. Hierarchical Composition","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#nested-subspaces","title":"Nested Subspaces","text":"<p>Key idea: Concepts can be hierarchical via nested subspaces.</p> <p>Example:</p> \\[\\mathcal{C}_1 \\supset \\mathcal{C}_{1,1} \\supset \\mathcal{C}_{1,1,1}\\] <p>Interpretation: - \\(\\mathcal{C}_1\\): \"Locomotion\" (coarse) - \\(\\mathcal{C}_{1,1}\\): \"Forward motion\" (medium) - \\(\\mathcal{C}_{1,1,1}\\): \"Running\" (fine)</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#hierarchical-projection","title":"Hierarchical Projection","text":"<p>Level 1 (coarse):</p> \\[Q^+_{\\text{coarse}} = P_1 Q^+\\] <p>Level 2 (medium):</p> \\[Q^+_{\\text{medium}} = P_{1,1} (P_1 Q^+)\\] <p>Level 3 (fine):</p> \\[Q^+_{\\text{fine}} = P_{1,1,1} (P_{1,1} (P_1 Q^+))\\] <p>Note: Because of nesting, \\(P_{1,1,1} P_{1,1} P_1 = P_{1,1,1}\\).</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#activation-hierarchy","title":"Activation Hierarchy","text":"<p>Coarse activation:</p> \\[A_1 = \\|P_1 Q^+\\|^2\\] <p>Medium activation (given coarse):</p> \\[A_{1,1|1} = \\frac{\\|P_{1,1} Q^+\\|^2}{\\|P_1 Q^+\\|^2}\\] <p>Fine activation (given medium):</p> \\[A_{1,1,1|1,1} = \\frac{\\|P_{1,1,1} Q^+\\|^2}{\\|P_{1,1} Q^+\\|^2}\\] <p>Interpretation: Conditional activations down the hierarchy.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#compositional-activation-tree","title":"Compositional Activation Tree","text":"<pre><code>                Q\u207a\n                 |\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502        \u2502        \u2502\n       P\u2081       P\u2082       P\u2083\n       0.6      0.3      0.1\n        |\n    \u250c\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2510\n    \u2502   \u2502   \u2502\n  P\u2081,\u2081 P\u2081,\u2082 P\u2081,\u2083\n  0.4  0.3  0.3\n    |\n  \u250c\u2500\u253c\u2500\u2510\nP\u2081,\u2081,\u2081 P\u2081,\u2081,\u2082\n 0.7    0.3\n</code></pre> <p>Reading:  - 60% activation in concept 1 - Within concept 1, 40% in sub-concept 1.1 - Within 1.1, 70% in sub-sub-concept 1.1.1</p> <p>This is a continuous hierarchy, not a discrete tree!</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#6-spectral-discovery-of-concepts","title":"6. Spectral Discovery of Concepts","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#algorithm-concept-subspace-extraction","title":"Algorithm: Concept Subspace Extraction","text":"<p>Input: Kernel matrix \\(K \\in \\mathbb{R}^{N \\times N}\\), number of concepts \\(C\\)</p> <p>Step 1: Eigendecomposition</p> \\[K = \\Phi \\Lambda \\Phi^T\\] <p>where \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots, \\lambda_N)\\) with \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots\\).</p> <p>Step 2: Select Top Eigenvectors</p> <p>Keep top \\(M\\) eigenvectors: \\(\\{\\phi_1, \\ldots, \\phi_M\\}\\) (e.g., \\(M = 50\\)).</p> <p>Step 3: Cluster Eigenvectors</p> <p>Apply k-means (or other clustering) to eigenvector matrix \\(\\Phi_M \\in \\mathbb{R}^{N \\times M}\\) to get \\(C\\) clusters.</p> <p>Step 4: Define Concept Subspaces</p> <p>For cluster \\(k\\) containing eigenvectors \\(\\{\\phi_{i_1}, \\ldots, \\phi_{i_{m_k}}\\}\\):</p> \\[\\mathcal{C}_k = \\text{span}\\{\\phi_{i_1}, \\ldots, \\phi_{i_{m_k}}\\}\\] <p>Output: Concept subspaces \\(\\{\\mathcal{C}_1, \\ldots, \\mathcal{C}_C\\}\\)</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#why-this-works","title":"Why This Works","text":"<p>Intuition: Eigenvectors of the kernel matrix capture modes of variation in the augmented space.</p> <p>Eigenvectors with similar profiles \u2192 related functional patterns \u2192 same concept.</p> <p>Mathematical justification: - Kernel PCA identifies principal components - Clustering groups related components - Subspaces capture multi-dimensional concepts</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#adaptive-concept-discovery","title":"Adaptive Concept Discovery","text":"<p>Instead of fixed \\(C\\), use adaptive methods:</p> <p>Gap heuristic: Choose \\(C\\) where eigenvalue gap is large:</p> \\[\\lambda_C \\gg \\lambda_{C+1}\\] <p>Information criterion: Minimize BIC or AIC for cluster count.</p> <p>Stability: Use consensus clustering across multiple runs.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#7-concept-dynamics-and-evolution","title":"7. Concept Dynamics and Evolution","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#temporal-concept-activation","title":"Temporal Concept Activation","text":"<p>As the agent learns, \\(Q^+(t)\\) evolves, so concept activation changes:</p> \\[A_k(t) = \\|P_k Q^+(t)\\|^2\\] <p>This gives interpretable learning curves:</p> <pre><code>Activation\n  ^\n  | Concept 1 (exploration)\n  | \u2500\u2500\u2500\u2500\u256e\n  |     \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  |\n  | Concept 2 (exploitation)\n  |       \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n  | \u2500\u2500\u2500\u2500\u2500\u2500\u256f\n  +-----------------------&gt; time\n</code></pre> <p>\"Agent transitioned from exploratory to exploitative concept.\"</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#concept-transition-matrix","title":"Concept Transition Matrix","text":"<p>Define concept dominance: \\(c(t) = \\arg\\max_k A_k(t)\\)</p> <p>Transition matrix: \\(T_{k\\ell} = P(c(t+1) = \\ell | c(t) = k)\\)</p> <p>This reveals concept dynamics without discrete states!</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#concept-persistence","title":"Concept Persistence","text":"<p>Measure stability: How long does a concept remain dominant?</p> \\[\\tau_k = \\mathbb{E}[\\text{time concept } k \\text{ stays active}]\\] <p>Persistent concepts = stable strategies Transient concepts = exploratory phases</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#8-practical-algorithms","title":"8. Practical Algorithms","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#algorithm-1-concept-conditioned-policy","title":"Algorithm 1: Concept-Conditioned Policy","text":"<p>Input: State \\(s\\), concept weights \\(\\{\\alpha_k\\}\\)</p> <p>Step 1: Compute concept-conditioned fields:</p> \\[Q^+_k(s, a) = \\langle P_k Q^+, k((s,a), \\cdot) \\rangle\\] <p>Step 2: Weighted combination:</p> \\[Q^+_{\\text{mix}}(s, a) = \\sum_k \\alpha_k Q^+_k(s, a)\\] <p>Step 3: Policy:</p> \\[\\pi(a|s) \\propto \\exp(\\beta Q^+_{\\text{mix}}(s, a))\\] <p>Use case: Mix exploration and exploitation by adjusting \\(\\alpha_k\\).</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#algorithm-2-hierarchical-action-selection","title":"Algorithm 2: Hierarchical Action Selection","text":"<p>Input: State \\(s\\), hierarchy depth \\(D\\)</p> <p>Level 1: Select coarse concept:</p> \\[c_1 = \\arg\\max_k A_k\\] <p>Level 2: Select medium concept within \\(c_1\\):</p> \\[c_2 = \\arg\\max_{\\ell \\in c_1} A_{\\ell|c_1}\\] <p>...</p> <p>Level D: Select action using fine concept:</p> \\[a^* = \\arg\\max_a Q^+_{c_D}(s, a)\\] <p>Benefit: Hierarchical decision-making with interpretable intermediate choices.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#algorithm-3-concept-based-transfer","title":"Algorithm 3: Concept-Based Transfer","text":"<p>Source task:</p> <ol> <li>Learn concept subspaces \\(\\{\\mathcal{C}_k^{\\text{source}}\\}\\)</li> <li>Store projection operators \\(\\{P_k^{\\text{source}}\\}\\)</li> </ol> <p>Target task:</p> <ol> <li>Initialize field: \\(Q^+_{\\text{target}} = 0\\)</li> <li> <p>For each experience \\((s, a, r)\\):</p> </li> <li> <p>Project particle onto source concepts:      $\\(z_{\\text{concept}} = \\sum_k P_k^{\\text{source}} k((s,a), \\cdot)\\)$</p> </li> <li>Update field using projected basis</li> </ol> <p>Why this works: Concepts capture abstract structure that transfers across tasks.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#9-connection-to-existing-work","title":"9. Connection to Existing Work","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#eigenoptions-machado-et-al-2017","title":"Eigenoptions (Machado et al., 2017)","text":"<p>Eigenoptions: Use eigenvectors of state transition graph as options (skills).</p> <p>GRL concepts: Use eigenvectors of kernel matrix as concept subspaces.</p> <p>Similarities: - Both use spectral methods - Both identify \"natural\" structures</p> <p>Differences: - Eigenoptions: single eigenvector = one option (discrete) - GRL concepts: subspace of eigenvectors = one concept (continuous) - Eigenoptions: hard assignment - GRL concepts: soft activation</p> <p>GRL generalizes eigenoptions to continuous, compositional representations.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#successor-features-barreto-et-al-2017","title":"Successor Features (Barreto et al., 2017)","text":"<p>Successor features: Represent value function as inner product \\(V(s) = \\langle \\psi(s), w \\rangle\\).</p> <p>GRL concepts: Represent field as projection \\(Q^+ = \\sum_k P_k Q^+\\).</p> <p>Similarities: - Both use linear combinations - Both enable transfer</p> <p>Differences: - Successor features: fixed basis (state features) - GRL concepts: learned basis (eigenvectors) - Successor features: flat structure - GRL concepts: hierarchical structure</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#affordances-gibson-1979-khetarpal-et-al-2020","title":"Affordances (Gibson, 1979; Khetarpal et al., 2020)","text":"<p>Affordance: What actions are possible in a state?</p> <p>GRL state wavefunction \\(\\phi_a(s)\\) (from Chapter 4) is a learned affordance map!</p> <p>Concept-conditioned affordances:</p> \\[\\phi_{a,k}(s) = \\langle P_k Q^+, k((s,a), \\cdot) \\rangle\\] <p>shows affordances from perspective of concept \\(k\\).</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#10-implications-for-part-ii-of-tutorial-paper","title":"10. Implications for Part II of Tutorial Paper","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#current-status-of-section-v","title":"Current Status of Section V","text":"<p>Original paper Section V: - Introduces spectral clustering idea - Shows empirical results - Demonstrates emergent concepts</p> <p>What's missing: - Formal definition of concepts (beyond \"clusters\") - Operational semantics (what do you do with concepts?) - Connection to learning algorithms</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#what-this-chapter-provides","title":"What This Chapter Provides","text":"<p>Formalization: - Concepts = subspaces \\(\\mathcal{C}_k \\subset \\mathcal{H}_k\\) - Activation = observable \\(A_k = \\|P_k Q^+\\|^2\\) - Hierarchy = nested subspaces</p> <p>Operations: - Project field onto concepts: \\(P_k Q^+\\) - Condition policy on concepts: \\(\\pi_k(a|s)\\) - Compose concepts hierarchically</p> <p>Algorithms: - Concept discovery (spectral + clustering) - Concept-conditioned learning - Hierarchical transfer</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#structure-for-extended-section-v","title":"Structure for Extended Section V","text":"<p>Proposed outline:</p> <p>V-A. Motivation - Why functional clustering? - Limitations of discrete concepts</p> <p>V-B. Concept Subspaces - Definition via eigenspaces - Projection operators - This chapter's formalism</p> <p>V-C. Spectral Discovery - Algorithm - Adaptive selection - Stability analysis</p> <p>V-D. Concept Dynamics - Activation evolution - Transition patterns - Persistence measures</p> <p>V-E. Hierarchical Composition - Nested subspaces - Multi-level activation - Compositional policies</p> <p>V-F. Empirical Results - Concept discovery in benchmark tasks - Activation curves - Transfer experiments</p> <p>V-G. Connections - Eigenoptions, successor features, affordances - Relation to hierarchical RL literature</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#summary","title":"Summary","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#key-contributions-of-this-chapter","title":"Key Contributions of This Chapter","text":"<ol> <li>Concepts as Subspaces</li> <li>Not clusters, but linear subspaces \\(\\mathcal{C}_k \\subset \\mathcal{H}_k\\)</li> <li> <p>Enables smooth, compositional representations</p> </li> <li> <p>Projection Operators</p> </li> <li>Formal definition: \\(P_k: \\mathcal{H}_k \\to \\mathcal{C}_k\\)</li> <li>Properties: idempotent, orthogonal, complete</li> <li> <p>Connection to QM measurement theory</p> </li> <li> <p>Concept Activation Observables</p> </li> <li>Measure: \\(A_k = \\|P_k Q^+\\|^2\\)</li> <li>Smooth evolution, no discrete jumps</li> <li> <p>Interpretable learning curves</p> </li> <li> <p>Hierarchical Composition</p> </li> <li>Nested subspaces: \\(\\mathcal{C}_1 \\supset \\mathcal{C}_{1,1} \\supset \\cdots\\)</li> <li>Conditional activation at each level</li> <li> <p>Natural multi-scale representation</p> </li> <li> <p>Practical Algorithms</p> </li> <li>Spectral discovery</li> <li>Concept-conditioned policies</li> <li>Hierarchical action selection</li> <li>Transfer learning</li> </ol>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#key-equations","title":"Key Equations","text":"<p>Projection operator:</p> \\[P_k Q^+ = \\sum_{i=1}^{m_k} \\langle Q^+, \\phi_{k,i} \\rangle_{\\mathcal{H}_k} \\phi_{k,i}\\] <p>Concept activation:</p> \\[A_k = \\|P_k Q^+\\|_{\\mathcal{H}_k}^2 = \\sum_{i=1}^{m_k} |\\langle Q^+, \\phi_{k,i} \\rangle|^2\\] <p>Concept-conditioned field:</p> \\[Q^+_k(z) = \\langle P_k Q^+, k(z, \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>Hierarchical activation:</p> \\[A_{k,\\ell|k} = \\frac{\\|P_{k,\\ell} Q^+\\|^2}{\\|P_k Q^+\\|^2}\\]"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#what-this-enables","title":"What This Enables","text":"<p>Theoretical: - Rigorous concept formalism - Quantum measurement theory connection - Hierarchical composition framework</p> <p>Practical: - Interpretable learning (activation curves) - Hierarchical policies (multi-level decisions) - Transfer learning (concept basis) - Compositional strategies (concept mixing)</p> <p>For Part II: - Mathematical foundation for Section V - Operational algorithms - Clear connection to QM</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#within-this-series","title":"Within This Series","text":"<ul> <li>Chapter 1a: State Vector vs. Wavefunction</li> <li>Chapter 2: RKHS Basis and Amplitudes</li> <li>Chapter 4: Action and State Projections</li> </ul>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#grl-tutorials","title":"GRL Tutorials","text":"<ul> <li>Tutorial Chapter 2: RKHS Foundations</li> <li>Tutorial Chapter 4: Reinforcement Field</li> <li>Part II (planned): Emergent Structure &amp; Spectral Abstraction</li> </ul>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#related-literature","title":"Related Literature","text":"<p>Spectral Methods in RL: - Machado et al. (2017). \"A Laplacian Framework for Option Discovery in Reinforcement Learning.\" ICML. - Mahadevan &amp; Maggioni (2007). \"Proto-value Functions: A Laplacian Framework.\" ICML.</p> <p>Hierarchical RL: - Sutton et al. (1999). \"Between MDPs and semi-MDPs: A Framework for Temporal Abstraction.\" - Bacon et al. (2017). \"The Option-Critic Architecture.\" AAAI.</p> <p>Transfer Learning: - Barreto et al. (2017). \"Successor Features for Transfer in Reinforcement Learning.\" NIPS. - Taylor &amp; Stone (2009). \"Transfer Learning for Reinforcement Learning Domains: A Survey.\"</p> <p>Quantum Measurement Theory: - von Neumann, J. (1932). Mathematical Foundations of Quantum Mechanics. - Peres, A. (1993). Quantum Theory: Concepts and Methods. Kluwer.</p> <p>Functional Data Analysis: - Ramsay &amp; Silverman (2005). Functional Data Analysis. Springer.</p>"},{"location":"GRL0/quantum_inspired/05-concept-projections-and-measurements/#next-steps","title":"Next Steps","text":"<p>For Research: 1. Implement spectral concept discovery algorithm 2. Test concept-conditioned policies on benchmarks 3. Develop hierarchical composition framework 4. Apply to transfer learning problems</p> <p>For Tutorial Paper (Part II): 1. Integrate this formalism into Section V 2. Add concept activation visualizations 3. Show hierarchical decomposition examples 4. Connect to experimental results</p> <p>For Extensions (Papers A/B/C): - Paper B: \"Hierarchical Reinforcement Learning via Concept Subspace Projections\" - Paper C: \"Transfer Learning with Functional Concept Bases\"</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/","title":"Chapter 6: The Agent's State and Belief Evolution","text":""},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#motivation","title":"Motivation","text":"<p>Throughout this series, we've discussed \"the state\" \\(Q^+\\), projections like \\(Q^+(s, a)\\), and operations like MemoryUpdate. But what exactly is the agent's state in GRL?</p> <p>This is not a philosophical question\u2014it's a precise technical question with important implications:</p> <ul> <li>What object encodes the agent's knowledge?</li> <li>What changes when the agent learns?</li> <li>What stays fixed during inference?</li> </ul> <p>This chapter provides definitive answers by clarifying:</p> <ol> <li>The agent's state = particle memory = reinforcement field</li> <li>Belief evolution = MemoryUpdate as state transition operator</li> <li>The role of weights = implicit GP-derived coefficients, not learned parameters</li> <li>Three distinct operations = fixing state, querying state, evolving state</li> </ol> <p>This resolves a common confusion: mixing up the state (what the agent knows) with observations (what the agent queries).</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#1-what-is-the-state-in-grl","title":"1. What Is \"The State\" in GRL?","text":""},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#the-question","title":"The Question","text":"<p>In traditional RL, \"state\" usually means environment state \\(s \\in \\mathcal{S}\\).</p> <p>But in GRL, we have multiple candidates:</p> <ul> <li>Environment state \\(s\\)?</li> <li>Augmented point \\(z = (s, a)\\)?</li> <li>Field value \\(Q^+(s, a)\\)?</li> <li>Action projection \\(Q^+(s, \\cdot)\\)?</li> <li>The entire field \\(Q^+\\)?</li> </ul> <p>Which one is \"the agent's state\"?</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#the-answer-the-entire-field","title":"The Answer: The Entire Field","text":"<p>The agent's state is the reinforcement field:</p> \\[Q^+ = \\sum_{i=1}^N w_i k(z_i, \\cdot) \\in \\mathcal{H}_k\\] <p>Why the entire field?</p> <p>Because this object completely specifies the agent's beliefs about value across all state-action combinations.</p> <p>Equivalent representation: Particle memory</p> \\[\\Omega = \\{(z_i, w_i)\\}_{i=1}^N\\] <p>Key equation:</p> \\[\\Omega \\Longleftrightarrow Q^+\\] <p>These are two views of the same object: - \\(\\Omega\\) = discrete representation (particles) - \\(Q^+\\) = continuous representation (field)</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#why-not-something-smaller","title":"Why Not Something Smaller?","text":"<p>Q: Why isn't \\(Q^+(s, \\cdot)\\) the state?</p> <p>A: Because \\(Q^+(s, \\cdot)\\) is a projection of the state onto a particular subspace, not the state itself.</p> <p>Analogy to QM:</p> Quantum Mechanics GRL State: \\(\\|\\psi\\rangle \\in \\mathcal{H}\\) State: \\(Q^+ \\in \\mathcal{H}_k\\) Wavefunction: \\(\\psi(x) = \\langle x \\| \\psi \\rangle\\) Field value: \\(Q^+(s, a) = \\langle Q^+, k((s,a), \\cdot) \\rangle\\) Position representation Augmented space representation <p>The wavefunction \\(\\psi(x)\\) is not the state\u2014it's a coordinate representation of the state.</p> <p>Same in GRL: \\(Q^+(s, a)\\) is not the state\u2014it's a coordinate representation (amplitude) of the state.</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#particle-memory-is-the-state","title":"Particle Memory IS the State","text":"<p>Critical insight:</p> \\[\\text{The particle set } \\{(z_i, w_i)\\}_{i=1}^N \\text{ completely determines } Q^+\\] <p>What the particles encode:</p> Component Meaning Type \\(z_i = (s_i, a_i)\\) Where experience occurred Position in \\(\\mathcal{Z} = \\mathcal{S} \\times \\Theta\\) \\(w_i\\) Evidence strength at \\(z_i\\) Real number (positive or negative) \\(k(z_i, \\cdot)\\) Kernel section Basis function in \\(\\mathcal{H}_k\\) <p>From particles to field:</p> \\[Q^+ = \\sum_{i=1}^N w_i k(z_i, \\cdot)\\] <p>This representation is complete! You can compute \\(Q^+(z)\\) for any \\(z\\):</p> \\[Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle_{\\mathcal{H}_k} = \\sum_{i=1}^N w_i k(z_i, z)\\]"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#what-about-qz_i-at-the-particle-locations","title":"What About \\(Q^+(z_i)\\) at the Particle Locations?","text":"<p>Question: \"Should we store \\(Q^+(z_i)\\) as part of the particle?\"</p> <p>Answer: No, it's redundant!</p> <p>\\(Q^+(z_i)\\) is computable from the particles:</p> \\[Q^+(z_i) = \\sum_{j=1}^N w_j k(z_j, z_i)\\] <p>So the particle representation is:</p> \\[\\text{Particle } i: (z_i, w_i)\\] <p>NOT:</p> \\[\\text{Particle } i: (z_i, w_i, Q^+(z_i)) \\leftarrow \\text{redundant!}\\] <p>What \\(w_i\\) represents: - Original paper: Fitness contribution - Modern framing: Energy contribution (negative fitness: \\(E(z_i) = -w_i k(z_i, z_i)\\)) - Mathematically: RKHS expansion coefficient</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#2-three-distinct-operations","title":"2. Three Distinct Operations","text":"<p>Now that we know the state is \\(Q^+\\) (equivalently: \\(\\Omega\\)), let's clarify three operations that are often confused:</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#operation-a-fixing-the-belief-state","title":"Operation A: Fixing the Belief State","text":"<p>At time \\(t\\), given particle memory \\(\\Omega_t\\):</p> \\[\\Omega_t = \\{(z_i^{(t)}, w_i^{(t)})\\}_{i=1}^{N_t}\\] <p>This fixes the belief state:</p> \\[Q^+_t = \\sum_{i=1}^{N_t} w_i^{(t)} k(z_i^{(t)}, \\cdot)\\] <p>Meaning: \"Conditional on the current memory, the agent's knowledge is \\(Q^+_t\\).\"</p> <p>This is NOT learning\u2014it's just stating what the current belief is.</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#operation-b-querying-the-state-inference","title":"Operation B: Querying the State (Inference)","text":"<p>Given fixed \\(Q^+_t\\), compute:</p> \\[Q^+_t(s, a) = \\langle Q^+_t, k((s,a), \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>or action wavefunction (from Chapter 4):</p> \\[\\psi_{s,t}(a) = Q^+_t(s, a)\\] <p>or concept activation (from Chapter 5):</p> \\[A_{k,t} = \\|P_k Q^+_t\\|^2\\] <p>Key point: These operations do not change \\(Q^+_t\\)!</p> <p>They are:</p> <ul> <li>Queries</li> <li>Projections</li> <li>Evaluations</li> <li>Inferences</li> </ul> <p>Analogy: Computing \\(\\psi(x) = \\langle x | \\psi \\rangle\\) doesn't change \\(|\\psi\\rangle\\).</p> <p>This is pure inference, no learning.</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#operation-c-evolving-the-state-learning-via-memoryupdate","title":"Operation C: Evolving the State (Learning via MemoryUpdate)","text":"<p>MemoryUpdate transforms the belief state:</p> \\[\\mathcal{U}: Q^+_t \\mapsto Q^+_{t+1}\\] <p>or equivalently:</p> \\[\\Omega_t \\xrightarrow{\\text{MemoryUpdate}} \\Omega_{t+1}\\] <p>What can change: - Add particles: \\(\\Omega_{t+1} = \\Omega_t \\cup \\{(z_{new}, w_{new})\\}\\) - Update weights: \\(w_i^{(t+1)} = w_i^{(t)} + \\Delta w_i\\) - Merge particles: Combine nearby particles into one - Prune particles: Remove low-influence particles</p> <p>Result: New belief state \\(Q^+_{t+1} \\neq Q^+_t\\)</p> <p>This IS learning!</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#summary-table","title":"Summary Table","text":"Operation Changes \\(Q^+\\)? Purpose A. Fix state No (just specify current state) Define what agent knows B. Query state No (projection/evaluation) Action selection, concept activation C. Evolve state Yes (belief update) Learning from experience <p>Critical distinction:</p> <p>Between MemoryUpdate events, \\(Q^+\\) is fixed. During MemoryUpdate, \\(Q^+\\) evolves.</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#3-two-time-scales","title":"3. Two Time Scales","text":"<p>This gives GRL a natural separation of time scales:</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#slow-time-scale-belief-evolution","title":"Slow Time Scale: Belief Evolution","text":"<p>MemoryUpdate events: \\(t = 0, 1, 2, \\ldots\\)</p> <p>State transitions:</p> \\[Q^+_0 \\xrightarrow{\\mathcal{U}_1} Q^+_1 \\xrightarrow{\\mathcal{U}_2} Q^+_2 \\xrightarrow{\\mathcal{U}_3} \\cdots\\] <p>This is learning.</p> <p>Frequency: Every episode, or every \\(K\\) steps, or based on novelty</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#fast-time-scale-inference","title":"Fast Time Scale: Inference","text":"<p>Between \\(t\\) and \\(t+1\\), \\(Q^+_t\\) is fixed.</p> <p>Agent performs many queries: - Evaluate \\(Q^+_t(s_1, a)\\) for action selection at \\(s_1\\) - Evaluate \\(Q^+_t(s_2, a)\\) for action selection at \\(s_2\\) - Compute concept activation \\(A_{k,t}\\) - Sample from policy \\(\\pi_t(a|s) \\propto \\exp(\\beta Q^+_t(s, a))\\)</p> <p>This is inference.</p> <p>Frequency: Every step, or multiple times per step</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#why-this-matters","title":"Why This Matters","text":"<p>Separation of concerns: - Learning: Happens via MemoryUpdate (slow) - Acting: Happens via inference (fast) - No gradient descent mixing learning and inference</p> <p>Computational efficiency: - Don't recompute entire field for every action - Cache kernel evaluations between updates - Amortize expensive operations (merging, pruning)</p> <p>Theoretical clarity: - Clean POMDP interpretation (belief state = \\(Q^+\\)) - Well-defined state transition operator (\\(\\mathcal{U}\\)) - No ambiguity about \"what changed\"</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#4-the-role-of-weights-implicit-not-learned","title":"4. The Role of Weights: Implicit, Not Learned","text":""},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#common-misconception","title":"Common Misconception","text":"<p>Misconception: \"The weights \\(w_i\\) are learned parameters, like neural network weights.\"</p> <p>Reality: The weights are implicit coefficients determined by the GP posterior, not explicit optimization variables.</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#how-weights-arise","title":"How Weights Arise","text":"<p>In Gaussian Process regression:</p> <p>Given data \\(\\mathcal{D} = \\{(z_i, y_i)\\}\\) and kernel \\(k\\), the posterior mean is:</p> \\[\\mu(z) = \\mathbf{k}(z)^T (\\mathbf{K} + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{y}\\] <p>where:</p> <ul> <li>\\(\\mathbf{k}(z) = [k(z_1, z), \\ldots, k(z_N, z)]^T\\)</li> <li>\\(\\mathbf{K}_{ij} = k(z_i, z_j)\\)</li> <li>\\(\\mathbf{y} = [y_1, \\ldots, y_N]^T\\)</li> </ul> <p>This can be written as:</p> \\[\\mu(z) = \\sum_{i=1}^N w_i k(z_i, z)\\] <p>where \\(\\mathbf{w} = (\\mathbf{K} + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{y}\\).</p> <p>The weights \\(w_i\\) are not learned\u2014they're computed from the data and kernel!</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#in-grl","title":"In GRL","text":"<p>Similar structure:</p> \\[Q^+(z) = \\sum_{i=1}^N w_i k(z_i, z)\\] <p>The weights arise from: 1. Experience accumulation: Each \\((z_i, r_i)\\) contributes 2. Kernel propagation: Overlap spreads influence 3. TD updates: Temporal difference signals adjust weights</p> <p>They are NOT: - Gradient descent parameters - Explicitly optimized - Independent of the kernel structure</p> <p>They ARE: - State variables (part of the belief state) - Functionally determined by experience and kernel - Evidence coefficients (strength of belief at each particle)</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#representer-theorem-connection","title":"Representer Theorem Connection","text":"<p>The representer theorem says:</p> <p>In RKHS, any function minimizing a regularized loss can be written as a finite sum over data points:</p> \\[f^* = \\sum_{i=1}^N \\alpha_i k(x_i, \\cdot)\\] <p>In GRL: - Data points = experience particles \\(z_i\\) - Coefficients = weights \\(w_i\\) - Function = reinforcement field \\(Q^+\\)</p> <p>So the particle representation is not arbitrary\u2014it's the optimal form given the RKHS structure!</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#5-memoryupdate-as-belief-transition-operator","title":"5. MemoryUpdate as Belief Transition Operator","text":""},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#formal-definition","title":"Formal Definition","text":"<p>MemoryUpdate is an operator:</p> \\[\\mathcal{U}: \\mathcal{H}_k \\to \\mathcal{H}_k\\] \\[Q^+_t \\mapsto Q^+_{t+1}\\] <p>In particle coordinates:</p> \\[\\mathcal{U}: \\{(z_i^{(t)}, w_i^{(t)})\\} \\mapsto \\{(z_j^{(t+1)}, w_j^{(t+1)})\\}\\]"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#what-memoryupdate-does-algorithm-1","title":"What MemoryUpdate Does (Algorithm 1)","text":"<p>From Tutorial Chapter 6, MemoryUpdate performs:</p> <p>Step 1: Particle instantiation</p> <p>Given experience \\((s_t, a_t, r_t)\\), create:</p> \\[z_{new} = (s_t, a_t), \\quad w_{new} = f(r_t)\\] <p>where \\(f(\\cdot)\\) maps reinforcement to weight (e.g., \\(f(r) = r\\), or \\(f(r) = -r\\) for energy).</p> <p>Step 2: Kernel association</p> <p>Compute similarity to existing particles:</p> \\[a_i = k(z_{new}, z_i^{(t)})\\] <p>Step 3: Weight propagation (optional)</p> <p>For particles with high association:</p> \\[w_i^{(t+1)} = w_i^{(t)} + \\lambda \\cdot a_i \\cdot w_{new}\\] <p>This is \"experience association\"\u2014evidence spreads through kernel geometry!</p> <p>Step 4: Memory integration</p> \\[\\Omega_{t+1} = \\Omega_t \\cup \\{(z_{new}, w_{new})\\}\\] <p>Step 5: Structural consolidation</p> <ul> <li>Merge: Combine particles with \\(k(z_i, z_j) &gt; \\tau_{merge}\\)</li> <li>Prune: Remove particles with \\(|w_i| &lt; \\tau_{prune}\\)</li> <li>Decay: \\(w_i^{(t+1)} = \\gamma w_i^{(t)}\\) for all \\(i\\)</li> </ul> <p>Result:</p> \\[Q^+_{t+1} = \\sum_{j=1}^{N_{t+1}} w_j^{(t+1)} k(z_j^{(t+1)}, \\cdot)\\] <p>This is a discrete, explicit state transition!</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#connection-to-gaussian-process-updates","title":"Connection to Gaussian Process Updates","text":"<p>MemoryUpdate can be viewed as:</p> <p>GP posterior update expressed in particle (inducing point) coordinates</p> <p>Standard GP update: 1. Observe new data: \\((z_{new}, y_{new})\\) 2. Update posterior: \\(p(f | \\mathcal{D}_{t+1}) \\propto p(y_{new} | f, z_{new}) \\cdot p(f | \\mathcal{D}_t)\\)</p> <p>GRL equivalent: 1. Observe new experience: \\((z_{new}, r_{new})\\) 2. Update particle memory: \\(\\Omega_{t+1}\\) (via MemoryUpdate) 3. Resulting field: \\(Q^+_{t+1}\\)</p> <p>Key difference: GRL also includes:</p> <ul> <li>Weight propagation (kernel association)</li> <li>Structural consolidation (merge/prune)</li> </ul> <p>These are not standard GP operations, but natural extensions for lifelong learning!</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#6-experience-association-what-it-really-is","title":"6. Experience Association: What It Really Is","text":""},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#the-original-papers-description","title":"The Original Paper's Description","text":"<p>Section IV-A describes \"experience association\"\u2014new experience affects nearby particles through kernel overlap.</p> <p>Formalizing experience association:</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#experience-association-as-operator","title":"Experience Association as Operator","text":"<p>Experience association is the weight propagation step in MemoryUpdate:</p> \\[\\mathcal{A}: (Q^+_t, z_{new}, w_{new}) \\mapsto Q^+_t + \\Delta Q^+\\] <p>where:</p> \\[\\Delta Q^+ = \\sum_{i: a_i &gt; \\varepsilon} (\\lambda \\cdot a_i \\cdot w_{new}) \\, k(z_i, \\cdot)\\] <p>In words: - New evidence at \\(z_{new}\\) with strength \\(w_{new}\\) - Propagates to associated particles \\(z_i\\) (where \\(a_i = k(z_{new}, z_i) &gt; \\varepsilon\\)) - Strength of propagation: \\(\\lambda \\cdot a_i \\cdot w_{new}\\)</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#why-this-differs-from-standard-gp","title":"Why This Differs from Standard GP","text":"<p>Standard GP: Each data point contributes independently</p> \\[\\mu(z) = \\sum_i \\alpha_i k(z_i, z)\\] <p>where \\(\\alpha_i\\) depends only on observation \\(y_i\\) and regularization.</p> <p>GRL with experience association: Data points influence each other's weights</p> \\[w_i^{(t+1)} = w_i^{(t)} + \\lambda \\sum_{j: \\text{new}} a_{ij} w_j^{\\text{new}}\\] <p>This is a form of: - Soft credit assignment (not just local TD error) - Geometric belief propagation (through kernel metric) - Non-local update (affects multiple particles simultaneously)</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#connection-to-kernel-based-message-passing","title":"Connection to Kernel-Based Message Passing","text":"<p>This is similar to: - Kernel mean embedding updates - Belief propagation in continuous spaces - Kernel density estimation with adaptive weights</p> <p>But GRL's version is unique because: - Weights can be positive or negative (not just probabilities) - Propagation is kernel-weighted (not uniform or discrete) - Updates are compositional (new evidence builds on old)</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#7-reconciling-with-quantum-mechanics","title":"7. Reconciling with Quantum Mechanics","text":""},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#the-qm-analogy-precisely-stated","title":"The QM Analogy, Precisely Stated","text":"<p>In quantum mechanics:</p> <p>State: \\(|\\psi\\rangle \\in \\mathcal{H}\\) (Hilbert space vector)</p> <p>Evolution: Unitary operators (between measurements)</p> \\[|\\psi(t)\\rangle = e^{-iHt/\\hbar} |\\psi(0)\\rangle\\] <p>Measurement: Projects onto observable eigenspace</p> \\[p(\\lambda) = |\\langle \\lambda | \\psi \\rangle|^2\\] <p>State \"fixed\": Between measurements</p> <p>In GRL:</p> <p>State: \\(Q^+ \\in \\mathcal{H}_k\\) (RKHS vector) \u2261 particle memory \\(\\Omega\\)</p> <p>Evolution: MemoryUpdate operator (between inference queries)</p> \\[Q^+_{t+1} = \\mathcal{U}(Q^+_t, \\text{experience}_t)\\] <p>Measurement: Projects onto query subspaces</p> \\[Q^+(s, a) = \\langle Q^+, k((s,a), \\cdot) \\rangle\\] <p>State \"fixed\": Between MemoryUpdate events</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#the-parallel-is-structural-not-metaphorical","title":"The Parallel Is Structural, Not Metaphorical","text":"Aspect Quantum Mechanics GRL State space Hilbert space \\(\\mathcal{H}\\) RKHS \\(\\mathcal{H}_k\\) State vector \\(\\|\\psi\\rangle\\) \\(Q^+\\) (or \\(\\Omega\\)) Basis \\(\\{\\|x\\rangle\\}\\) \\(\\{k(z, \\cdot)\\}\\) Coordinate rep \\(\\psi(x) = \\langle x \\| \\psi \\rangle\\) \\(Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle\\) Evolution Hamiltonian \\(\\hat{H}\\) MemoryUpdate \\(\\mathcal{U}\\) Measurement Observable \\(\\hat{O}\\) Projection \\(P_k\\) or query Time scales Between measurements: fixed Between updates: fixed <p>This is not poetry\u2014it's the same mathematical structure!</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#8-practical-implications","title":"8. Practical Implications","text":""},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#for-implementation","title":"For Implementation","text":"<p>Representation choice:</p> <p>Store particles, not the full field:</p> <pre><code>class BeliefState:\n    def __init__(self):\n        self.particles = []  # List of (z_i, w_i)\n\n    def query(self, z_query):\n        \"\"\"Compute Q^+(z_query) from particles\"\"\"\n        return sum(w_i * kernel(z_i, z_query) \n                   for z_i, w_i in self.particles)\n\n    def update(self, experience):\n        \"\"\"MemoryUpdate: evolve belief state\"\"\"\n        z_new, r_new = experience\n        w_new = r_new  # or more complex mapping\n\n        # Particle instantiation\n        self.particles.append((z_new, w_new))\n\n        # Experience association (weight propagation)\n        for i, (z_i, w_i) in enumerate(self.particles[:-1]):\n            a_i = kernel(z_new, z_i)\n            if a_i &gt; epsilon:\n                self.particles[i] = (z_i, w_i + lambda_prop * a_i * w_new)\n\n        # Structural consolidation\n        self.merge_particles()\n        self.prune_particles()\n</code></pre>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#for-efficiency","title":"For Efficiency","text":"<p>Between MemoryUpdate: - Cache kernel evaluations - Precompute Gram matrix if needed - Use sparse representations for large particle sets</p> <p>During MemoryUpdate: - Only update associated particles (threshold \\(\\varepsilon\\)) - Merge periodically, not every step - Use KD-trees for fast nearest-neighbor finding</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#for-interpretation","title":"For Interpretation","text":"<p>Visualize belief evolution:</p> <pre><code># Track field value at specific points over time\nhistory = []\nfor t in range(T):\n    state_t = agent.belief_state.query(z_test)\n    history.append(state_t)\n\n    # Agent acts, observes, learns\n    experience = agent.interact(env)\n    agent.belief_state.update(experience)\n\n# Plot belief evolution\nplt.plot(history)\nplt.xlabel('Time (MemoryUpdate events)')\nplt.ylabel('Q^+(z_test)')\nplt.title('Belief Evolution at Test Point')\n</code></pre>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#summary","title":"Summary","text":""},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#key-concepts","title":"Key Concepts","text":"<ol> <li>The Agent's State</li> <li>State = reinforcement field \\(Q^+ \\in \\mathcal{H}_k\\)</li> <li>Equivalently: particle memory \\(\\Omega = \\{(z_i, w_i)\\}\\)</li> <li> <p>Complete representation: particles determine field</p> </li> <li> <p>Three Operations</p> </li> <li>Fix state: Specify current belief (Operation A)</li> <li>Query state: Compute projections/evaluations (Operation B)</li> <li> <p>Evolve state: MemoryUpdate (Operation C)</p> </li> <li> <p>Two Time Scales</p> </li> <li>Slow: Learning via MemoryUpdate (\\(Q^+_t \\to Q^+_{t+1}\\))</li> <li> <p>Fast: Inference via queries (\\(Q^+_t(s, a)\\), fixed \\(Q^+_t\\))</p> </li> <li> <p>Weights Are Implicit</p> </li> <li>Not learned parameters</li> <li>GP-derived coefficients</li> <li> <p>State variables, not optimization variables</p> </li> <li> <p>MemoryUpdate as Operator</p> </li> <li>Belief state transition: \\(\\mathcal{U}: Q^+_t \\mapsto Q^+_{t+1}\\)</li> <li>Includes: instantiation, association, consolidation</li> <li> <p>Experience association = weight propagation</p> </li> <li> <p>QM Parallel</p> </li> <li>Same structure: state vector in Hilbert space</li> <li>Evolution via operators</li> <li>Fixed between update/measurement events</li> </ol>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#key-equations","title":"Key Equations","text":"<p>State specification:</p> \\[\\Omega = \\{(z_i, w_i)\\}_{i=1}^N \\Longleftrightarrow Q^+ = \\sum_{i=1}^N w_i k(z_i, \\cdot)\\] <p>Query (inference):</p> \\[Q^+(z) = \\langle Q^+, k(z, \\cdot) \\rangle = \\sum_i w_i k(z_i, z)\\] <p>Evolution (learning):</p> \\[\\mathcal{U}: Q^+_t \\mapsto Q^+_{t+1}\\] <p>Experience association:</p> \\[w_i^{(t+1)} = w_i^{(t)} + \\lambda \\cdot k(z_{new}, z_i) \\cdot w_{new}\\]"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#what-this-clarifies","title":"What This Clarifies","text":"<p>For theory: - Rigorous definition of \"the state\" - Clean separation of learning and inference - Well-defined belief evolution operator - Precise QM parallel</p> <p>For implementation: - What to store (particles) - What to compute (queries) - When to update (MemoryUpdate events) - How to optimize (caching, sparse ops)</p> <p>For Part II (Section V): - Concept activation operates on fixed \\(Q^+\\) - Concept evolution tracks \\(A_k(t)\\) over MemoryUpdate events - Clean distinction between concept inference and concept learning</p>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#within-this-series","title":"Within This Series","text":"<ul> <li>Chapter 2: RKHS Basis and Amplitudes</li> <li>Chapter 4: Action and State Projections</li> <li>Chapter 5: Concept Subspaces</li> </ul>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#grl-tutorials","title":"GRL Tutorials","text":"<ul> <li>Tutorial Chapter 5: Particle Memory</li> <li>Tutorial Chapter 6: MemoryUpdate Algorithm</li> </ul>"},{"location":"GRL0/quantum_inspired/06-agent-state-and-belief-evolution/#related-literature","title":"Related Literature","text":"<p>Gaussian Processes: - Rasmussen &amp; Williams (2006). Gaussian Processes for Machine Learning. MIT Press. - Qui &amp; Candela (2005). \"Sparse Gaussian Processes using Pseudo-inputs.\" NIPS.</p> <p>Belief-State RL: - Kaelbling et al. (1998). \"Planning and Acting in Partially Observable Stochastic Domains.\" - Ross et al. (2008). \"Online Planning Algorithms for POMDPs.\"</p> <p>Kernel Methods: - Sch\u00f6lkopf &amp; Smola (2002). Learning with Kernels. MIT Press. - Berlinet &amp; Thomas-Agnan (2004). Reproducing Kernel Hilbert Spaces in Probability and Statistics.</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/","title":"Chapter 7: Learning the Reinforcement Field \u2014 Beyond Gaussian Processes","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#motivation","title":"Motivation","text":"<p>In Chapter 6, we established that the agent's state is the reinforcement field \\(Q^+ \\in \\mathcal{H}_k\\), and MemoryUpdate is the operator that evolves this state.</p> <p>But how exactly should we update the field given new experience?</p> <p>The original GRL paper uses a Gaussian Process (GP) perspective, where weights \\(w_i\\) emerge from kernel-based inference. But GP is not the only option!</p> <p>This chapter explores:</p> <ol> <li>Why GP is one choice among many for learning \\(Q^+\\)</li> <li>Alternative learning mechanisms (kernel ridge, online optimization, sparse methods, deep nets)</li> <li>When to use which approach (trade-offs in scalability, sparsity, adaptivity)</li> <li>Amplitude-based learning from quantum-inspired probability</li> </ol> <p>Key insight: The state-as-field formalism is agnostic to the learning mechanism\u2014you can swap the inference engine while preserving GRL's structure.</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#1-the-learning-problem-what-are-we-optimizing","title":"1. The Learning Problem: What Are We Optimizing?","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#the-core-question","title":"The Core Question","text":"<p>Given:</p> <ul> <li>Current field: \\(Q^+_t = \\sum_i w_i^{(t)} k(z_i^{(t)}, \\cdot)\\)</li> <li>New experience: \\((s_t, a_t, r_t)\\) (or TD target \\(y_t\\))</li> </ul> <p>Find: Updated field \\(Q^+_{t+1}\\)</p> <p>Constraint: \\(Q^+_{t+1}\\) should:</p> <ul> <li>Fit the new evidence</li> <li>Generalize via kernel structure</li> <li>Remain bounded/stable</li> </ul>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#the-objective-general-form","title":"The Objective (General Form)","text":"<p>Most learning mechanisms solve:</p> \\[\\min_{Q^+ \\in \\mathcal{H}_k} \\underbrace{\\mathcal{L}(Q^+, \\mathcal{D}_{t+1})}_{\\text{fit data}} + \\underbrace{\\mathcal{R}(Q^+)}_{\\text{regularization}}\\] <p>where:</p> <ul> <li>\\(\\mathcal{D}_{t+1} = \\{(z_i, y_i)\\}_{i=1}^{N+1}\\) is accumulated experience</li> <li>\\(\\mathcal{L}\\) measures prediction error</li> <li>\\(\\mathcal{R}\\) controls complexity</li> </ul> <p>Different choices give different algorithms!</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#2-method-1-gaussian-process-regression-original-grl","title":"2. Method 1: Gaussian Process Regression (Original GRL)","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#the-gp-perspective","title":"The GP Perspective","text":"<p>Model: \\(Q^+(z) \\sim \\mathcal{GP}(\\mu(z), k(z, z'))\\)</p> <p>Posterior mean after observing data:</p> \\[Q^+(z) = \\sum_{i=1}^N \\alpha_i k(z_i, z)\\] <p>where \\(\\boldsymbol{\\alpha} = (\\mathbf{K} + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{y}\\)</p> <p>Properties: - \u2705 Probabilistic (gives uncertainty) - \u2705 Kernel-based (automatic generalization) - \u2705 Theoretically grounded - \u274c Requires matrix inversion (\\(O(N^3)\\)) - \u274c Full covariance matrix (\\(O(N^2)\\) memory)</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#when-gp-makes-sense","title":"When GP Makes Sense","text":"<p>Good for: - Small-to-medium particle sets (\\(N &lt; 10^4\\)) - Batch updates (re-solve periodically) - When you need calibrated uncertainty</p> <p>Not ideal for: - Large-scale online learning - Extremely sparse solutions - Real-time embedded systems</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#3-method-2-kernel-ridge-regression-deterministic-cousin","title":"3. Method 2: Kernel Ridge Regression (Deterministic Cousin)","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#the-regularized-least-squares-view","title":"The Regularized Least Squares View","text":"<p>Objective:</p> \\[\\min_{\\mathbf{w}} \\|\\mathbf{K}\\mathbf{w} - \\mathbf{y}\\|^2 + \\lambda \\mathbf{w}^T \\mathbf{K} \\mathbf{w}\\] <p>Solution:</p> \\[\\mathbf{w} = (\\mathbf{K} + \\lambda \\mathbf{I})^{-1} \\mathbf{y}\\] <p>Result:</p> \\[Q^+(z) = \\sum_{i=1}^N w_i k(z_i, z)\\]"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#comparison-to-gp","title":"Comparison to GP","text":"Aspect Gaussian Process Kernel Ridge Framework Probabilistic Deterministic Solution \\((\\mathbf{K} + \\sigma^2 \\mathbf{I})^{-1} \\mathbf{y}\\) \\((\\mathbf{K} + \\lambda \\mathbf{I})^{-1} \\mathbf{y}\\) Uncertainty Yes (full covariance) No Computation Same (\\(O(N^3)\\)) Same (\\(O(N^3)\\)) Interpretation Posterior mean Regularized fit <p>Practically equivalent for point predictions!</p> <p>When to use: If you don't need uncertainty estimates, kernel ridge is simpler.</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#4-method-3-online-convex-optimization-incremental-updates","title":"4. Method 3: Online Convex Optimization (Incremental Updates)","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#the-online-learning-view","title":"The Online Learning View","text":"<p>Instead of batch re-solving, update weights incrementally:</p> \\[w_i^{(t+1)} = w_i^{(t)} - \\eta_t \\nabla_{w_i} \\mathcal{L}_t\\] <p>where \\(\\mathcal{L}_t\\) is loss on current experience.</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#stochastic-gradient-descent-on-weights","title":"Stochastic Gradient Descent on Weights","text":"<p>For TD learning:</p> \\[\\mathcal{L}_t = \\frac{1}{2} [Q^+_t(z_t) - y_t]^2\\] <p>where \\(y_t = r_t + \\gamma \\max_{a'} Q^+_t(s_{t+1}, a')\\) (TD target).</p> <p>Gradient:</p> \\[\\nabla_{w_i} \\mathcal{L}_t = [Q^+_t(z_t) - y_t] \\cdot k(z_i, z_t)\\] <p>Update:</p> \\[w_i^{(t+1)} = w_i^{(t)} - \\eta_t [Q^+_t(z_t) - y_t] k(z_i, z_t)\\]"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#properties","title":"Properties","text":"<p>\u2705 Online: No matrix inversion \u2705 Scalable: \\(O(N)\\) per update \u2705 Flexible: Can use different loss functions (Huber, quantile) \u274c No closed form: Requires tuning learning rate \\(\\eta_t\\) \u274c Stability: May need momentum, adaptive rates</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#when-to-use-online-updates","title":"When to Use Online Updates","text":"<p>Good for: - Large-scale continuous learning - Non-stationary environments - Real-time systems - When batch re-solving is too expensive</p> <p>Example: RL with millions of experiences\u2014batch GP infeasible, online SGD works.</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#5-method-4-sparse-methods-compact-memory","title":"5. Method 4: Sparse Methods (Compact Memory)","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#the-sparsity-objective","title":"The Sparsity Objective","text":"<p>Goal: Learn \\(Q^+\\) with few active particles (small \\(\\|\\mathbf{w}\\|_0\\))</p> <p>LASSO in kernel space:</p> \\[\\min_{\\mathbf{w}} \\|\\mathbf{K}\\mathbf{w} - \\mathbf{y}\\|^2 + \\lambda_1 \\|\\mathbf{w}\\|_1 + \\lambda_2 \\mathbf{w}^T \\mathbf{K} \\mathbf{w}\\] <p>\\(\\ell_1\\) penalty encourages sparse \\(\\mathbf{w}\\) (many \\(w_i = 0\\)).</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#why-sparsity-matters","title":"Why Sparsity Matters","text":"<p>Particle memory grows unbounded without pruning:</p> <ul> <li>Every experience \u2192 new particle</li> <li>Memory: \\(O(N)\\)</li> <li>Computation: \\(O(N)\\) per query</li> </ul> <p>With sparsity: - Active set: \\(\\{i : w_i \\neq 0\\}\\) is small - Memory: \\(O(k)\\) where \\(k \\ll N\\) - Computation: \\(O(k)\\) per query</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#sparse-gp-variants","title":"Sparse GP Variants","text":"<p>Inducing point methods: - Select \\(M\\) representative particles (inducing points) - Approximate \\(Q^+\\) using only these \\(M\\) particles - Solve \\((M \\times M)\\) system instead of \\((N \\times N)\\)</p> <p>Example: FITC, PITC, VFE</p> <p>Compression ratio: \\(M/N\\) (e.g., 100 inducing points for 10,000 experiences)</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#when-to-use-sparse-methods","title":"When to Use Sparse Methods","text":"<p>Good for: - Memory-constrained systems (robots, embedded) - Lifelong learning (unbounded experience streams) - When most particles are redundant</p> <p>Trade-off: - Faster queries - Less memory - Slight approximation error</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#6-method-5-deep-neural-networks-function-approximation","title":"6. Method 5: Deep Neural Networks (Function Approximation)","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#replacing-kernels-with-neural-nets","title":"Replacing Kernels with Neural Nets","text":"<p>Standard GRL:</p> \\[Q^+(z) = \\sum_i w_i k(z_i, z)\\] <p>Neural GRL:</p> \\[Q^+(z) = Q_\\theta(z)\\] <p>where \\(Q_\\theta\\) is a deep neural network with parameters \\(\\theta\\).</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#how-to-train","title":"How to Train","text":"<p>Objective: Same TD learning</p> \\[\\mathcal{L} = \\mathbb{E}[(Q_\\theta(s, a) - y)^2]\\] <p>where \\(y = r + \\gamma \\max_{a'} Q_\\theta(s', a')\\)</p> <p>Update: Gradient descent on \\(\\theta\\)</p> \\[\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta \\mathcal{L}\\]"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#hybrid-neural-net-particle-memory","title":"Hybrid: Neural Net + Particle Memory","text":"<p>Keep the best of both:</p> <p>Long-term memory (neural):</p> \\[Q_{\\text{base}}(z) = Q_\\theta(z)\\] <p>Short-term memory (particles):</p> \\[Q_{\\text{episodic}}(z) = \\sum_{i \\in \\text{recent}} w_i k(z_i, z)\\] <p>Combined:</p> \\[Q^+(z) = Q_\\theta(z) + \\beta \\sum_{i \\in \\text{recent}} w_i k(z_i, z)\\] <p>Why this works: - Neural net: captures long-term patterns - Particles: fast adaptation to new experiences - Combination: benefits of both</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#when-to-use-deep-nets","title":"When to Use Deep Nets","text":"<p>Good for: - High-dimensional states (images, text) - When kernel methods don't scale - Transfer learning (pre-trained representations)</p> <p>Not ideal for: - Low-data regimes (kernels better) - When interpretability matters (particles clearer) - Embedded systems (large models)</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#7-method-6-mixture-of-experts-multiple-local-fields","title":"7. Method 6: Mixture of Experts (Multiple Local Fields)","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#the-idea","title":"The Idea","text":"<p>Instead of one global field, maintain multiple local fields with a gating function:</p> \\[Q^+(z) = \\sum_{m=1}^M g_m(z) Q_m(z)\\] <p>where:</p> <ul> <li>\\(Q_m(z)\\) = expert \\(m\\)'s field</li> <li>\\(g_m(z)\\) = gate (probability expert \\(m\\) is active)</li> <li>\\(\\sum_m g_m(z) = 1\\)</li> </ul>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#how-to-partition","title":"How to Partition","text":"<p>Option A: Geometric partition (augmented space)</p> <p>Gate based on kernel similarity to expert centers:</p> \\[g_m(z) \\propto \\exp(\\gamma \\, k(z, c_m))\\] <p>where \\(c_m\\) is the center of expert \\(m\\).</p> <p>Find centers via: - K-means clustering in augmented space - Spectral clustering (align with concepts!) - Online clustering as experiences arrive</p> <p>Option B: Concept-based partition (function space)</p> <p>Use concept subspaces from Chapter 5:</p> \\[Q_m = P_{\\mathcal{C}_m} Q^+\\] <p>Gate by concept activation:</p> \\[g_m(z) \\propto \\|P_{\\mathcal{C}_m} k(z, \\cdot)\\|^2\\] <p>Interpretation: \"How much does query \\(z\\) belong to concept \\(m\\)?\"</p> <p>This is extremely elegant: partition function space, not state space!</p> <p>Option C: State vs. action partition</p> <p>State-based experts: \\(g_m(z)\\) depends only on \\(s\\) - Good for environments with distinct regimes/modes - Example: different rooms, different game phases</p> <p>Action-based experts: \\(g_m(z)\\) depends only on \\(a\\) - Good for tool-using agents - Example: \"search expert,\" \"calculator expert,\" \"database expert\"</p> <p>Joint experts: \\(g_m(z)\\) depends on \\((s, a)\\) - Most general, captures \"this action works in this situation\" - Aligns with GRL's augmented space philosophy</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#training-mixture-of-experts","title":"Training Mixture of Experts","text":"<p>Option 1: Hard assignment - Assign each particle to one expert - Train experts independently</p> <p>Option 2: Soft assignment (EM-style) - E-step: Compute gates \\(g_m(z)\\) - M-step: Update each expert's weights with gate-weighted data</p> <p>Option 3: Joint training - Train gates and experts together - Use sparse MoE (only activate top-k experts)</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#when-to-use-moe","title":"When to Use MoE","text":"<p>Good for: - Complex environments with distinct modes - When single field is too simple - Tool-using/API-calling agents - Hierarchical decision-making</p> <p>Example: - Expert 1: Navigation in open space - Expert 2: Navigation through doors - Expert 3: Navigation in crowds - Gate routes based on context</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#8-amplitude-based-learning-quantum-inspired","title":"8. Amplitude-Based Learning (Quantum-Inspired)","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#the-probability-amplitude-view","title":"The Probability Amplitude View","text":"<p>Standard ML: Model probability directly</p> \\[p(x) = f_\\theta(x)\\] <p>Amplitude-based: Model amplitude, derive probability</p> \\[\\psi(x) = f_\\theta(x), \\quad p(x) = |\\psi(x)|^2\\]"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#why-this-is-interesting","title":"Why This Is Interesting","text":"<p>Allows interference: - Amplitudes can be negative or complex - Combine amplitudes before squaring - Constructive/destructive interference</p> <p>Example:</p> \\[\\psi_{\\text{total}}(x) = \\psi_1(x) + \\psi_2(x)\\] \\[p(x) = |\\psi_1(x) + \\psi_2(x)|^2 \\neq |\\psi_1(x)|^2 + |\\psi_2(x)|^2\\] <p>This captures correlations that direct probability models miss!</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#existing-work","title":"Existing Work","text":"<p>Born Machines (Cheng et al. 2018): - Use quantum circuits to generate \\(\\psi(x)\\) - Sample from \\(p(x) = |\\psi(x)|^2\\) - For generative modeling</p> <p>Complex-valued neural networks: - Amplitude + phase - Used in signal processing, audio, RF</p> <p>Quantum-inspired models: - Quantum cognition (decision theory) - Quantum probability theory - Order effects, context effects</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#for-grl-amplitude-based-value-functions","title":"For GRL: Amplitude-Based Value Functions","text":"<p>Proposal: Treat \\(Q^+\\) as an amplitude</p> \\[Q^+(z) \\in \\mathbb{C}\\] <p>Policy from Born rule:</p> \\[\\pi(a|s) \\propto |Q^+(s, a)|^2\\] <p>Why interesting: - Phase can encode temporal structure, direction, context - Interference between action options - Coherence measures correlation strength</p> <p>This approach is explored in detail in Chapter 3: Complex-Valued RKHS.</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#research-opportunity","title":"Research Opportunity","text":"<p>What's underexplored:</p> <p>Amplitude-based reinforcement learning \u2014 treating value/policy as amplitude fields with:</p> <ul> <li>Interference effects</li> <li>Phase semantics</li> <li>Born rule for action selection</li> </ul> <p>GRL is positioned to explore this!</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#9-comparison-summary","title":"9. Comparison Summary","text":"Method Scalability Uncertainty Sparsity Complexity GP \\(O(N^3)\\) Yes No Low (matrix inversion) Kernel Ridge \\(O(N^3)\\) No No Low Online SGD \\(O(N)\\) No No Medium (tuning \\(\\eta\\)) Sparse GP \\(O(M^3)\\) Yes Yes Medium (select inducing points) Deep NN \\(O(1)\\) query No Implicit High (architecture, hyperparams) MoE \\(O(M \\cdot N/M)\\) Depends Per expert High (gating + experts) Amplitude Depends Special Depends High (complex math)"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#10-decision-guide-which-method-to-use","title":"10. Decision Guide: Which Method to Use?","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#for-grl-v0-baseline","title":"For GRL v0 (Baseline)","text":"<p>Recommendation: Kernel ridge or sparse GP - Maintains RKHS structure - Computationally tractable - Well-understood theory</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#for-large-scale-applications","title":"For Large-Scale Applications","text":"<p>Recommendation: Online SGD or hybrid (NN + particles) - Scalable to millions of experiences - Can use modern deep learning infrastructure - Trade interpretability for capacity</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#for-embeddedreal-time-systems","title":"For Embedded/Real-Time Systems","text":"<p>Recommendation: Sparse methods - Bounded memory: \\(O(M)\\) with \\(M\\) small - Fast queries: \\(O(M)\\) - Pruning/merging for adaptation</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#for-researchnovel-contributions","title":"For Research/Novel Contributions","text":"<p>Recommendation: MoE with concept-based gating or amplitude-based learning - MoE: natural connection to hierarchical RL - Amplitude: novel probability formulation for ML - Both: strong theoretical foundations</p>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#summary","title":"Summary","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#key-insights","title":"Key Insights","text":"<ol> <li>Learning \\(Q^+\\) is a choice</li> <li>GP is elegant but not unique</li> <li> <p>Many alternatives preserve GRL structure</p> </li> <li> <p>Trade-offs matter</p> </li> <li>Scalability: online SGD, deep nets</li> <li>Sparsity: LASSO, inducing points</li> <li> <p>Structure: MoE, concept-based</p> </li> <li> <p>State-as-field is agnostic</p> </li> <li>Can swap learning mechanisms</li> <li> <p>Preserves: state = field, query = projection, update = operator</p> </li> <li> <p>Amplitude-based learning is underexplored</p> </li> <li>QM math can be used for ML/optimization</li> <li>GRL is positioned to pioneer this for RL</li> </ol>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#key-equations","title":"Key Equations","text":"<p>General learning objective:</p> \\[\\min_{Q^+} \\mathcal{L}(Q^+, \\mathcal{D}) + \\mathcal{R}(Q^+)\\] <p>Online weight update:</p> \\[w_i^{(t+1)} = w_i^{(t)} - \\eta [Q^+_t(z_t) - y_t] k(z_i, z_t)\\] <p>Mixture of experts:</p> \\[Q^+(z) = \\sum_m g_m(z) Q_m(z)\\] <p>Amplitude-based policy:</p> \\[\\pi(a|s) \\propto |Q^+(s, a)|^2\\]"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#within-this-series","title":"Within This Series","text":"<ul> <li>Chapter 3: Complex-Valued RKHS (amplitude with phase)</li> <li>Chapter 5: Concept Subspaces (for MoE gating)</li> <li>Chapter 6: State Evolution (general framework)</li> </ul>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#grl-tutorials","title":"GRL Tutorials","text":"<ul> <li>Tutorial Chapter 6: MemoryUpdate Algorithm</li> </ul>"},{"location":"GRL0/quantum_inspired/07-learning-the-field-beyond-gp/#related-literature","title":"Related Literature","text":"<p>Gaussian Processes: - Rasmussen &amp; Williams (2006). Gaussian Processes for Machine Learning.</p> <p>Online Learning: - Shalev-Shwartz (2011). \"Online Learning and Online Convex Optimization.\"</p> <p>Sparse Methods: - Qui\u00f1onero-Candela &amp; Rasmussen (2005). \"A Unifying View of Sparse Approximate Gaussian Process Regression.\"</p> <p>Born Machines: - Cheng et al. (2018). \"Quantum Generative Adversarial Learning.\" PRL.</p> <p>Mixture of Experts: - Jacobs et al. (1991). \"Adaptive Mixtures of Local Experts.\" Neural Computation. - Shazeer et al. (2017). \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.\"</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/","title":"Chapter 8: Memory Dynamics \u2014 Formation, Consolidation, and Retrieval","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#motivation","title":"Motivation","text":"<p>In Chapter 6, we established that:</p> <ul> <li>The agent's state is the reinforcement field \\(Q^+ \\in \\mathcal{H}_k\\)</li> <li>MemoryUpdate is the belief evolution operator</li> </ul> <p>In Chapter 7, we explored how to learn the field (GP, ridge, online SGD, sparse, deep nets, MoE).</p> <p>Now we address memory dynamics over time: 1. Formation: How is new experience written to memory? 2. Consolidation: What should be retained vs. forgotten? 3. Retrieval: How is memory accessed for decision-making?</p> <p>Why this matters: Current RL and LLM agents suffer from:</p> <ul> <li>Drift: Long-term memory contaminated by transient information</li> <li>Repetition: Same mistakes repeated (poor consolidation)</li> <li>Forgetting: Constraints/facts lost (no principled retention criteria)</li> </ul> <p>GRL can address these by treating memory dynamics as operators with learnable criteria, not ad hoc heuristics.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#1-the-three-layer-memory-stack","title":"1. The Three-Layer Memory Stack","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#inspired-by-recent-memory-research","title":"Inspired by Recent Memory Research","text":"<p>Recent work on AI agent memory (Cao et al. 2024, \"Memory in the Age of AI Agents\") identifies:</p> <ul> <li>Forms: What memory is made of (representation)</li> <li>Functions: What memory is for (roles)</li> <li>Dynamics: How memory evolves (write, consolidate, retrieve)</li> </ul>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#grls-memory-stack","title":"GRL's Memory Stack","text":"<p>Layer 1: Latent/Internal = The RKHS State</p> <p>The \"true\" agent memory is the function \\(Q^+ \\in \\mathcal{H}_k\\), represented by particles:</p> \\[\\Omega = \\{(z_i, w_i)\\}_{i=1}^N\\] <p>This is the belief state.</p> <p>Layer 2: External = Persistent Particle Store</p> <p>Engineering layer: particle database/graph/tree for:</p> <ul> <li>Scalable retrieval</li> <li>Compression/pruning</li> <li>Hierarchical organization</li> </ul> <p>Semantically: Stores basis elements of a field, not just \"documents\" (not RAG!).</p> <p>Layer 3: Token-Level = Derived Narrative Buffer</p> <p>For LLM integration: synthesize \"what matters\" from particle state into text:</p> <ul> <li>Top concepts</li> <li>Active constraints</li> <li>Recent surprises</li> </ul> <p>Explicitly downstream, not source of truth.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#key-distinction","title":"Key Distinction","text":"<p>GRL's primary memory is latent functional memory (the field). Token memory is an interface artifact.</p> <p>This prevents the \"memory is RAG\" confusion that plagues current LLM agents.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#2-memory-functions-what-memory-is-for","title":"2. Memory Functions: What Memory Is For","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#three-memory-roles","title":"Three Memory Roles","text":"<p>Factual Memory (Stable Constraints)</p> <p>What: Things that should not drift - Physical laws - Task constraints - Safety rules</p> <p>In GRL:  - High-persistence anchor particles - Hard constraints in kernel (ignore irrelevant dimensions) - Repeller regions in action field</p> <p>Example: \"Never use Tool X with PII\" \u2192 persistent negative weight in action subspace.</p> <p>Experiential Memory (What Happened + How It Felt)</p> <p>What: Episode traces with value - \\((s, a, r)\\) transitions - Success/failure outcomes - Temporal context</p> <p>In GRL: - This is native particle memory - Particles = experience evidence - Weights = fitness/energy - Kernel overlap = generalization</p> <p>Working Memory (Task-Local, Short-Horizon)</p> <p>What: Temporary context for current decision - Sub-goal state - Recent observations - Current plan step</p> <p>In GRL: Temporary overlay field</p> \\[Q^{\\text{work}}_t = Q^+_t + \\Delta_t\\] <p>where \\(\\Delta_t\\) is a fast-decaying particle set or low-rank concept activation.</p> <p>Why separate? Prevents working memory from polluting long-term belief (addresses drift!).</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#3-memory-dynamics-the-three-operators","title":"3. Memory Dynamics: The Three Operators","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#decomposition-of-memoryupdate","title":"Decomposition of MemoryUpdate","text":"<p>MemoryUpdate is actually three sub-operators:</p> \\[Q^+_{t+1} = \\underbrace{\\mathcal{C}}_{\\text{consolidate}} \\circ \\underbrace{\\mathcal{P}}_{\\text{propagate}} \\circ \\underbrace{\\mathcal{E}}_{\\text{inject}}(Q^+_t; \\text{experience}_t)\\] <p>Let's formalize each.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#4-formation-write-operator-mathcale","title":"4. Formation (Write): Operator \\(\\mathcal{E}\\)","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#what-formation-does","title":"What Formation Does","text":"<p>Inject new evidence into memory:</p> <p>Input: \\((Q^+_t, (s_t, a_t, r_t))\\)</p> <p>Output: \\(Q^+_t\\) with new particle or updated weights</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#option-a-add-new-particle","title":"Option A: Add New Particle","text":"<p>Simplest:</p> \\[\\Omega_{t+1} = \\Omega_t \\cup \\{(z_t, w_t)\\}\\] <p>where:</p> <ul> <li>\\(z_t = (s_t, a_t)\\) (augmented state)</li> <li>\\(w_t = r_t\\) or TD target \\(y_t = r_t + \\gamma \\max_{a'} Q^+_t(s_{t+1}, a')\\)</li> </ul> <p>Effect:</p> \\[Q^+_{t+1}(z) = Q^+_t(z) + w_t k(z_t, z)\\] <p>Pure growth: memory size increases by 1.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#option-b-update-existing-weights","title":"Option B: Update Existing Weights","text":"<p>If particle \\(z_t\\) is \"close\" to existing particles:</p> <p>Find neighbors: \\(\\mathcal{N}(z_t) = \\{i : k(z_i, z_t) &gt; \\epsilon\\}\\)</p> <p>Update their weights:</p> \\[w_i \\leftarrow w_i + \\alpha_i \\cdot w_t\\] <p>where \\(\\alpha_i = k(z_i, z_t)\\) (association strength).</p> <p>Effect: Spread evidence to neighbors via kernel overlap.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#option-c-tag-memory-type","title":"Option C: Tag Memory Type","text":"<p>Distinguish factual/experiential/working:</p> <p>Factual: High persistence flag - Decay rate: \\(\\lambda_{\\text{factual}} \\approx 0\\) (never forget) - Prune priority: low</p> <p>Experiential: Normal persistence - Decay rate: \\(\\lambda_{\\text{exp}} = 0.01\\) (slow decay) - Prune priority: based on predictive value</p> <p>Working: Fast decay - Decay rate: \\(\\lambda_{\\text{work}} = 0.5\\) (forget quickly) - Prune priority: high (after task episode)</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#formation-criteria","title":"Formation Criteria","text":"<p>When to create new particle vs. update existing?</p> <p>Novelty criterion:</p> \\[\\text{novelty}(z_t) = 1 - \\max_i k(z_i, z_t)\\] <ul> <li>If novelty \\(&gt; \\tau_{\\text{novel}}\\): create new particle</li> <li>Else: update neighbors</li> </ul> <p>Surprise criterion:</p> \\[\\text{surprise}(z_t) = |Q^+_t(z_t) - y_t|\\] <ul> <li>High surprise: store distinctly (new particle)</li> <li>Low surprise: consolidate into neighbors</li> </ul> <p>This is psychologically plausible! Human memory:</p> <ul> <li>Novel experiences \u2192 encoded distinctly</li> <li>Familiar experiences \u2192 integrated into schemas</li> </ul>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#5-consolidation-compress-operator-mathcalc","title":"5. Consolidation (Compress): Operator \\(\\mathcal{C}\\)","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#the-consolidation-problem","title":"The Consolidation Problem","text":"<p>Memory grows unbounded without consolidation:</p> <ul> <li>Every experience \u2192 new particle</li> <li>\\(N\\) increases indefinitely</li> <li>Computation/memory: \\(O(N)\\)</li> </ul> <p>Consolidation: Merge, prune, compress while preserving predictive power.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#the-hard-threshold-problem","title":"The Hard Threshold Problem","text":"<p>Original GRL (Algorithm 1):</p> <p>Associate particles if \\(k(z_i, z_j) &gt; \\tau\\)</p> <p>Problems: - \\(\\tau\\) is a hard hyperparameter (not learned) - Brittle: sensitive to \\(\\tau\\) choice - Doesn't adapt to local density</p> <p>We need something better!</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#alternative-1-soft-association-no-threshold","title":"Alternative 1: Soft Association (No Threshold)","text":"<p>Replace hard threshold with soft weights:</p> \\[\\alpha_{ij} = \\frac{\\exp(\\gamma \\, k(z_i, z_j))}{\\sum_{j'} \\exp(\\gamma \\, k(z_i, z_{j'}))}\\] <p>Properties: - No \\(\\tau\\)! - Smooth: differentiable - Temperature \\(\\gamma\\) controls spread (learnable)</p> <p>Effect: Soft neighborhood graph, not binary adjacency.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#alternative-2-adaptive-threshold-top-k-neighbors","title":"Alternative 2: Adaptive Threshold (Top-k Neighbors)","text":"<p>Per-particle threshold:</p> \\[\\tau_i = \\text{quantile}_q \\{k(z_i, z_j)\\}_{j \\neq i}\\] <p>Choose \\(\\tau_i\\) so each particle has \\(\\approx k\\) neighbors.</p> <p>Properties: - Self-normalizing across regions - Dense regions: higher \\(\\tau_i\\) - Sparse regions: lower \\(\\tau_i\\)</p> <p>Very \"memory-like\": Association density adapts to local structure.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#alternative-3-information-theoretic-consolidation-mdl","title":"Alternative 3: Information-Theoretic Consolidation (MDL)","text":"<p>Objective: Minimize description length</p> \\[\\min_{\\Omega'} \\underbrace{\\text{TD-error}(Q^+(\\Omega'))}_{\\text{accuracy}} + \\lambda |\\Omega'|\\] <p>Interpretation:  - Keep particles that reduce prediction error - Prune particles that don't contribute</p> <p>Merge criterion: Merge \\((z_i, w_i)\\) and \\((z_j, w_j)\\) if it reduces objective.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#practical-implementation","title":"Practical Implementation","text":"<p>Greedy merging:</p> <ol> <li> <p>For each pair \\((i, j)\\) with \\(k(z_i, z_j) &gt; \\epsilon_{\\min}\\):</p> </li> <li> <p>Compute merged particle: \\(z' = (w_i z_i + w_j z_j)/(w_i + w_j)\\), \\(w' = w_i + w_j\\)</p> </li> <li>Evaluate: \\(\\Delta \\text{error} = \\text{TD-error after merge} - \\text{TD-error before}\\)</li> <li> <p>Evaluate: \\(\\Delta \\text{size} = -1\\) (one fewer particle)</p> </li> <li> <p>Merge pair with best trade-off: \\(\\Delta \\text{error} + \\lambda \\Delta \\text{size}\\)</p> </li> <li> <p>Repeat until no beneficial merges remain</p> </li> </ol> <p>This is principled: Consolidation is optimization, not heuristic!</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#alternative-4-surprise-gated-consolidation","title":"Alternative 4: Surprise-Gated Consolidation","text":"<p>Idea: How human memory consolidates</p> <p>Rule: - High prediction error \u2192 store distinctly (don't merge) - Low prediction error \u2192 consolidate (merge with neighbors)</p> <p>Formally:</p> \\[\\text{merge-probability}(i, j) \\propto k(z_i, z_j) \\cdot \\exp(-\\beta \\cdot \\text{TD-error}_i)\\] <p>Properties: - Surprising experiences preserved (for learning) - Predictable experiences compressed (save space) - \\(\\beta\\) controls sensitivity (learnable)</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#alternative-5-nonparametric-clustering-dp-mixtures","title":"Alternative 5: Nonparametric Clustering (DP Mixtures)","text":"<p>Treat consolidation as clustering:</p> <p>Use Dirichlet Process mixture or Chinese Restaurant Process:</p> <ul> <li>Prior penalizes too many clusters</li> <li>But allows growth when needed</li> </ul> <p>Association = cluster assignment</p> <p>Properties: - No fixed \\(k\\) (clusters) - Automatic complexity control - Bayesian: uncertainty-aware</p> <p>For GRL: Each cluster is a \"concept\" (see Chapter 5!).</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#consolidation-summary","title":"Consolidation Summary","text":"Method Pros Cons Complexity Soft association No threshold, smooth Still need \\(\\gamma\\) Low Top-k neighbors Density-adaptive, simple Fixed \\(k\\) Low MDL Principled, objective-driven Computationally expensive Medium Surprise-gated Psychologically plausible Requires TD-error Medium Clustering Automatic, Bayesian Complex inference High <p>Recommended: Start with top-k (simple), move to MDL (principled) or surprise-gated (adaptive).</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#6-retrieval-read-operator-mathcalr","title":"6. Retrieval (Read): Operator \\(\\mathcal{R}\\)","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#what-retrieval-does","title":"What Retrieval Does","text":"<p>Query the memory for decision-making:</p> <p>Input: Query point \\(z = (s, a)\\)</p> <p>Output: Field value \\(Q^+(z)\\) and/or related context</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#retrieval-modes","title":"Retrieval Modes","text":"<p>Mode 1: Point Query (Standard)</p> \\[Q^+(z) = \\sum_{i=1}^N w_i k(z_i, z)\\] <p>Use: Standard action selection.</p> <p>Mode 2: Projection Query (Chapter 4)</p> <p>State field (fixed action):</p> \\[Q^+(s, a_{\\text{fixed}}) \\text{ for varying } s\\] <p>Action field (fixed state):</p> \\[Q^+(s_{\\text{fixed}}, a) \\text{ for varying } a\\] <p>Use: Visualize landscapes, precondition learning.</p> <p>Mode 3: Concept Projection Query (Chapter 5)</p> <p>Project onto concept subspace \\(\\mathcal{C}_m\\):</p> \\[Q^+_m = P_{\\mathcal{C}_m} Q^+\\] <p>Concept activation:</p> \\[\\text{activation}_m(z) = \\|P_{\\mathcal{C}_m} k(z, \\cdot)\\|^2\\] <p>Use: Abstract reasoning, hierarchical planning, transfer learning.</p> <p>Mode 4: Neighborhood Retrieval</p> <p>Find particles similar to \\(z\\):</p> \\[\\mathcal{N}(z) = \\{i : k(z_i, z) &gt; \\epsilon\\}\\] <p>Use: - Explain prediction (which particles contributed?) - Case-based reasoning - Memory inspection/debugging</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#retrieval-abstraction-levels","title":"Retrieval Abstraction Levels","text":"<p>GRL supports multi-scale retrieval:</p> Level Granularity Query Particle Fine \\(Q^+(z) = \\sum_i w_i k(z_i, z)\\) Neighborhood Local \\(\\mathcal{N}(z) = \\{i : k(z_i, z) &gt; \\epsilon\\}\\) Concept Coarse \\(P_{\\mathcal{C}_m} Q^+\\) Global Abstract \\(Q^+\\) itself (full field) <p>Key insight: Different retrieval protocols serve different purposes \u2014 fine-grained control uses particles, abstract reasoning uses concepts.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#7-the-complete-memory-dynamics-pipeline","title":"7. The Complete Memory Dynamics Pipeline","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#unified-framework","title":"Unified Framework","text":"<p>Operator composition:</p> \\[Q^+_{t+1} = \\mathcal{C}_{\\lambda} \\circ \\mathcal{P}_{\\text{soft}} \\circ \\mathcal{E}_{\\text{surprise}}(Q^+_t; (s_t, a_t, r_t))\\]"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#step-by-step-algorithm","title":"Step-by-Step Algorithm","text":"<pre><code>def memory_dynamics_update(Q_plus, experience, config):\n    \"\"\"\n    Complete memory dynamics: formation, propagation, consolidation.\n\n    Args:\n        Q_plus: Current field (particle set)\n        experience: (s_t, a_t, r_t, s_{t+1})\n        config: {epsilon, gamma, lambda_mdl, decay_rates, ...}\n\n    Returns:\n        Q_plus_new: Updated field\n    \"\"\"\n    s_t, a_t, r_t, s_next = experience\n    z_t = augment(s_t, a_t)\n\n    # === FORMATION ===\n    # Compute novelty and surprise\n    novelty = 1 - max(kernel(z_t, z_i) for z_i in Q_plus.particles)\n    y_t = r_t + gamma * max_a(Q_plus.query(s_next, a))\n    surprise = abs(Q_plus.query(z_t) - y_t)\n\n    if novelty &gt; config.tau_novel or surprise &gt; config.tau_surprise:\n        # High novelty/surprise: create new particle\n        Q_plus.add_particle(z_t, w_t=y_t, memory_type='experiential')\n    else:\n        # Low novelty/surprise: update neighbors\n        neighbors = Q_plus.neighbors(z_t, epsilon=config.epsilon)\n        for i in neighbors:\n            alpha_i = kernel(Q_plus.particles[i].z, z_t)\n            Q_plus.particles[i].w += alpha_i * y_t\n\n    # === PROPAGATION (soft association) ===\n    for i in range(len(Q_plus.particles)):\n        # Compute soft association weights\n        alphas = [softmax_kernel(z_i, z_j, gamma=config.gamma) \n                  for z_j in Q_plus.particles]\n        # Spread influence (optional, for coherence)\n        Q_plus.particles[i].w = sum(alphas[j] * Q_plus.particles[j].w \n                                    for j in range(len(Q_plus.particles)))\n\n    # === CONSOLIDATION ===\n    # Option A: MDL-based merging\n    Q_plus = mdl_merge(Q_plus, lambda_mdl=config.lambda_mdl)\n\n    # Option B: Pruning low-weight particles\n    Q_plus.prune(threshold=config.prune_threshold)\n\n    # Option C: Decay working memory\n    for particle in Q_plus.particles:\n        if particle.memory_type == 'working':\n            particle.w *= (1 - config.decay_work)\n\n    return Q_plus\n</code></pre>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#learnable-vs-fixed-parameters","title":"Learnable vs. Fixed Parameters","text":"Parameter Type Notes \\(\\epsilon\\) (novelty threshold) Can learn Adaptive per region \\(\\gamma\\) (temperature) Should learn Controls association spread \\(\\lambda_{\\text{MDL}}\\) (complexity) Can learn Trade accuracy/sparsity Decay rates Can learn Per memory type Kernel bandwidth Should learn Generalization scale <p>Modern approach: Meta-learn these on a distribution of tasks.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#8-addressing-agent-drift","title":"8. Addressing Agent Drift","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#the-drift-problem","title":"The Drift Problem","text":"<p>Current LLM agents: - Long-term memory contaminated by transient context - Constraints forgotten after few steps - Mistakes repeated (no consolidation)</p> <p>Root cause: No separation between working/long-term memory.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#grl-solution","title":"GRL Solution","text":"<p>1. Separate Memory Types (Formation)</p> <ul> <li>Factual: persistent, high priority</li> <li>Experiential: normal decay</li> <li>Working: fast decay</li> </ul> <p>2. Consolidation Criteria (Not Random)</p> <ul> <li>Merge low-surprise experiences (compress)</li> <li>Preserve high-surprise experiences (learn)</li> </ul> <p>3. Retrieval at Right Abstraction</p> <ul> <li>Use concepts for abstract reasoning</li> <li>Use particles for fine-grained control</li> </ul>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#why-this-works","title":"Why This Works","text":"<p>Drift prevention:</p> \\[Q^+_{\\text{total}} = \\underbrace{Q^+_{\\text{long-term}}}_{\\text{stable}} + \\underbrace{\\Delta_{\\text{work}}}_{\\text{decays fast}}\\] <p>Working memory \\(\\Delta_{\\text{work}}\\) doesn't contaminate \\(Q^+_{\\text{long-term}}\\) because it decays quickly.</p> <p>Constraint preservation:</p> <p>Factual memory has \\(\\lambda_{\\text{decay}} \\approx 0\\), so constraints never forgotten.</p> <p>Mistake avoidance:</p> <p>Consolidation based on TD-error: high-error experiences retained for learning.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#9-connection-to-biological-memory","title":"9. Connection to Biological Memory","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#human-memory-stages","title":"Human Memory Stages","text":"<p>Short-term (working) memory: - Capacity: \\(\\sim\\)7 items - Duration: seconds to minutes - Function: active task context</p> <p>Long-term memory: - Capacity: unlimited - Duration: lifetime - Function: knowledge, skills, episodes</p> <p>Consolidation: - Sleep-dependent - Surprise-modulated (emotional salience) - Semantic compression (gist extraction)</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#grl-parallels","title":"GRL Parallels","text":"Human GRL Mechanism Working memory \\(\\Delta_{\\text{work}}\\) Fast-decay particles Long-term memory \\(Q^+_{\\text{stable}}\\) Persistent particles Consolidation \\(\\mathcal{C}\\) Merge, prune, compress Surprise modulation Surprise-gated formation High TD-error \u2192 distinct storage Semantic compression Concept formation Spectral clustering (Chapter 5) <p>GRL provides computational mechanisms for these phenomena!</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#10-practical-implementation-notes","title":"10. Practical Implementation Notes","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#for-grl-v0-baseline","title":"For GRL v0 (Baseline)","text":"<p>Simplest viable memory dynamics:</p> <ol> <li>Formation: Add new particle if novelty \\(&gt; \\epsilon\\)</li> <li>Consolidation: Top-k neighbor graph + periodic pruning</li> <li>Retrieval: Standard kernel query</li> </ol> <p>Complexity: \\(O(N)\\) per update, \\(O(N)\\) per query</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#for-scalable-grl","title":"For Scalable GRL","text":"<p>Add:</p> <ol> <li>Sparse inducing points (\\(M \\ll N\\))</li> <li>Hierarchical storage (tree structure)</li> <li>Lazy consolidation (only when memory budget exceeded)</li> </ol> <p>Complexity: \\(O(M)\\) per update, \\(O(\\log M)\\) per query</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#for-research-extensions","title":"For Research Extensions","text":"<p>Explore:</p> <ol> <li>Meta-learning consolidation criteria</li> <li>Amplitude-based memory (complex weights for phase)</li> <li>Hierarchical consolidation (concepts at multiple scales)</li> </ol>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#summary","title":"Summary","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#key-insights","title":"Key Insights","text":"<ol> <li> <p>Memory has three dynamics: formation, consolidation, retrieval</p> </li> <li> <p>Each is an operator: \\(\\mathcal{E}\\), \\(\\mathcal{C}\\), \\(\\mathcal{R}\\)</p> </li> <li> <p>Hard thresholds are brittle \u2192 use adaptive/learned criteria</p> </li> <li> <p>Memory types matter \u2192 factual/experiential/working have different dynamics</p> </li> <li> <p>Consolidation is optimization \u2192 MDL, surprise-gating, not ad hoc</p> </li> <li> <p>Retrieval has abstraction levels \u2192 particle, neighborhood, concept, global</p> </li> <li> <p>Drift is preventable \u2192 separate working from long-term memory</p> </li> </ol>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#key-equations","title":"Key Equations","text":"<p>Complete update:</p> \\[Q^+_{t+1} = \\mathcal{C} \\circ \\mathcal{P} \\circ \\mathcal{E}(Q^+_t; \\text{experience}_t)\\] <p>Soft association:</p> \\[\\alpha_{ij} = \\frac{\\exp(\\gamma \\, k(z_i, z_j))}{\\sum_{j'} \\exp(\\gamma \\, k(z_i, z_{j'}))}\\] <p>MDL consolidation:</p> \\[\\min_{\\Omega'} \\text{TD-error}(Q^+(\\Omega')) + \\lambda |\\Omega'|\\] <p>Working + long-term:</p> \\[Q^+_{\\text{total}} = Q^+_{\\text{stable}} + \\Delta_{\\text{work}}\\] <p>Surprise-gated formation:</p> \\[\\text{store-distinct if} \\quad |Q^+(z_t) - y_t| &gt; \\tau_{\\text{surprise}}\\]"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#principled-memory-management","title":"Principled Memory Management","text":"<p>Key Principles for Memory Update:</p> <p>Replace hard threshold \\(\\tau\\) with adaptive criteria:</p> <ul> <li>Soft association: Temperature-controlled (\\(\\gamma\\))</li> <li>Top-k adaptive neighbors: Density-aware</li> <li>MDL-based consolidation: Optimization-driven</li> <li>Surprise-gating: Psychologically plausible</li> </ul> <p>Retention Strategy:</p> <p>What to Retain: - High surprise (large TD-error) \u2014 valuable for learning - High novelty (far from existing particles) \u2014 new information - Factual constraints (tagged) \u2014 critical knowledge</p> <p>What to Forget (merge/prune): - Low surprise (predictable) \u2014 redundant information - Redundant (close to neighbors) \u2014 can be compressed - Working memory (after episode) \u2014 task-specific, temporary</p> <p>Implementation: MDL consolidation or surprise-gated formation provide principled, data-driven criteria rather than fixed hyperparameters.</p>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#within-this-series","title":"Within This Series","text":"<ul> <li>Chapter 5: Concept Subspaces (hierarchical retrieval)</li> <li>Chapter 6: State Evolution Framework</li> <li>Chapter 7: Learning Mechanisms</li> </ul>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#grl-tutorials","title":"GRL Tutorials","text":"<ul> <li>Tutorial 5: Particle Memory Basics</li> <li>Tutorial 6: MemoryUpdate Algorithm</li> </ul>"},{"location":"GRL0/quantum_inspired/08-memory-dynamics-formation-consolidation-retrieval/#related-literature","title":"Related Literature","text":"<p>Agent Memory: - Cao et al. (2024). \"Memory in the Age of AI Agents.\" arXiv:2512.13564.</p> <p>Memory Consolidation: - McClelland et al. (1995). \"Why There Are Complementary Learning Systems.\"</p> <p>Information-Theoretic Learning: - Rissanen (1978). \"Modeling by Shortest Data Description.\" Automatica.</p> <p>Surprise-Modulated Memory: - Schultz &amp; Dickinson (2000). \"Neuronal Coding of Prediction Errors.\" Ann. Rev. Neurosci.</p> <p>Nonparametric Clustering: - Rasmussen (1999). \"The Infinite Gaussian Mixture Model.\" NIPS.</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/","title":"Chapter 09: Path Integrals and Action Principles","text":"<p>Purpose: This chapter makes explicit the deep connection between GRL's energy-based formulation and quantum mechanics via Feynman's path integral formulation. While Chapter 03a introduced the least action principle from a classical control perspective, here we explore the quantum mechanical version and its implications for amplitude-based learning.</p> <p>Key insight: Just as quantum mechanics can be formulated via path integrals over complex-valued amplitudes, GRL can be extended to a path integral formulation over complex-valued reinforcement fields, enabling interference effects and richer policy representations.</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#1-feynmans-path-integral-formulation","title":"1. Feynman's Path Integral Formulation","text":""},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#11-from-schrodinger-to-path-integrals","title":"1.1 From Schr\u00f6dinger to Path Integrals","text":"<p>In standard quantum mechanics, the state evolves via the Schr\u00f6dinger equation:</p> \\[i\\hbar \\frac{\\partial}{\\partial t}|\\psi(t)\\rangle = \\hat{H}|\\psi(t)\\rangle\\] <p>Feynman showed this is equivalent to summing over all possible paths:</p> \\[\\langle x_f | e^{-i\\hat{H}T/\\hbar} | x_i \\rangle = \\int \\mathcal{D}[x(t)] \\, e^{iS[x(t)]/\\hbar}\\] <p>where:</p> <ul> <li>\\(S[x(t)] = \\int_0^T L(x, \\dot{x}, t) dt\\) = action functional</li> <li>\\(L = T - V\\) = Lagrangian (kinetic - potential energy)</li> <li>\\(\\hbar\\) = Planck's constant (sets the scale)</li> </ul> <p>Interpretation:  The probability amplitude to go from \\(x_i\\) to \\(x_f\\) is the sum over all paths, each weighted by \\(e^{iS/\\hbar}\\).</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#12-key-differences-from-classical-action","title":"1.2 Key Differences from Classical Action","text":"<p>Classical mechanics: One path minimizes action - \\(\\delta S = 0\\) \u2192 Euler-Lagrange equations - Deterministic trajectory</p> <p>Quantum mechanics: All paths contribute - \\(\\text{Amplitude} = \\sum_{\\text{paths}} e^{iS/\\hbar}\\) - Interference between paths (constructive/destructive) - Paths near the classical trajectory dominate (stationary phase)</p> <p>Why complex amplitudes? - Phase \\(e^{iS/\\hbar}\\) encodes action along path - Allows interference: paths can cancel or reinforce - Recovers classical limit as \\(\\hbar \\to 0\\) (only minimal action path survives)</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#13-the-imaginary-time-wick-rotation","title":"1.3 The Imaginary Time (Wick Rotation)","text":"<p>Key trick: Replace \\(t \\to -i\\tau\\) (imaginary time):</p> \\[e^{-i\\hat{H}t/\\hbar} \\to e^{-\\hat{H}\\tau/\\hbar}\\] <p>Path integral becomes:</p> \\[\\langle x_f | e^{-\\hat{H}\\tau/\\hbar} | x_i \\rangle = \\int \\mathcal{D}[x(\\tau)] \\, e^{-S_E[x(\\tau)]/\\hbar}\\] <p>where \\(S_E\\) is the Euclidean action (with sign flip).</p> <p>Why this matters for RL:</p> <ul> <li>Real exponentials \\(e^{-S_E/\\hbar}\\) (no oscillations!)</li> <li>Becomes a probability distribution (after normalization)</li> <li>This is the statistical mechanics / thermodynamics connection</li> <li>Temperature \\(\\beta = 1/(k_B T)\\) plays the role of \\(1/\\hbar\\)</li> </ul>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#2-stochastic-control-as-imaginary-time-qm","title":"2. Stochastic Control as Imaginary Time QM","text":""},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#21-the-feynman-kac-formula","title":"2.1 The Feynman-Kac Formula","text":"<p>The Feynman-Kac formula connects:</p> <ul> <li>Quantum mechanics (path integrals)</li> <li>Stochastic processes (Brownian motion)</li> <li>Parabolic PDEs (diffusion equations)</li> </ul> <p>For RL: The optimal value function satisfies:</p> \\[V(s) = -\\nu \\log \\mathbb{E}_{\\text{paths}}\\left[e^{-\\frac{1}{\\nu}\\int_0^\\infty C(s_t, u_t) dt}\\right]\\] <p>This is a path integral over trajectories with cost functional \\(C\\)!</p> <p>Explicitly:</p> \\[e^{-V(s)/\\nu} = \\int \\mathcal{D}[\\tau] \\, e^{-\\frac{1}{\\nu}\\int_0^\\infty C(s_t, u_t) dt}\\] <p>Comparison to QM:</p> Quantum Mechanics Stochastic Control \\(\\hbar\\) (Planck's constant) \\(\\nu\\) (temperature) \\(\\psi(x)\\) (wavefunction) \\(e^{-V(s)/\\nu}\\) (value function) \\(\\hat{H}\\) (Hamiltonian) \\(\\hat{H}_{\\text{HJB}}\\) (Hamilton-Jacobi-Bellman) Schr\u00f6dinger equation Diffusion equation"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#22-the-hjb-equation-as-schrodinger-equation","title":"2.2 The HJB Equation as Schr\u00f6dinger Equation","text":"<p>The Hamilton-Jacobi-Bellman (HJB) equation for optimal control:</p> \\[-\\frac{\\partial V}{\\partial t} = \\min_u \\left[C(s, u) + \\nabla V \\cdot f(s, u) + \\frac{\\nu}{2}\\nabla^2 V\\right]\\] <p>In imaginary time QM, the Schr\u00f6dinger equation becomes:</p> \\[-\\frac{\\partial \\psi}{\\partial \\tau} = -\\frac{\\hbar}{2m}\\nabla^2 \\psi + V(x)\\psi\\] <p>With \\(\\psi = e^{-V/\\nu}\\), these are isomorphic!</p> HJB Schr\u00f6dinger (imaginary time) \\(V(s, t)\\) \\(-\\nu \\log \\psi(x, \\tau)\\) \\(C(s, u)\\) Potential \\(V(x)\\) \\(\\nu\\) \\(\\hbar\\) Diffusion term Kinetic energy \\(-\\frac{\\hbar}{2m}\\nabla^2\\) <p>This is not just an analogy\u2014it's a mathematical equivalence!</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#3-grls-path-integral-formulation","title":"3. GRL's Path Integral Formulation","text":""},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#31-the-grl-action-functional-revisited","title":"3.1 The GRL Action Functional (Revisited)","text":"<p>In augmented space \\(z = (s, \\theta)\\), the action along trajectory \\(\\tau = \\{z_t\\}\\) is:</p> \\[S_{\\text{GRL}}[\\tau] = \\int_0^T \\left[E(z_t) + \\frac{1}{2\\lambda}\\|\\dot{z}_t\\|^2\\right] dt\\] <p>where:</p> <ul> <li>\\(E(z) = -Q^+(z)\\) = energy</li> <li>\\(\\lambda\\) = temperature (exploration parameter)</li> </ul> <p>Path integral formulation:</p> \\[\\mathcal{Z}(z_i, z_f; T) = \\int \\mathcal{D}[\\tau] \\, e^{-S_{\\text{GRL}}[\\tau]/\\lambda}\\] <p>This is the partition function summing over all trajectories from \\(z_i\\) to \\(z_f\\) in time \\(T\\).</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#32-the-boltzmann-policy-as-path-integral","title":"3.2 The Boltzmann Policy as Path Integral","text":"<p>Single-step decision (instantaneous action selection):</p> \\[\\pi(\\theta|s) = \\frac{e^{-E(s,\\theta)/\\lambda}}{\\int e^{-E(s,\\theta')/\\lambda} d\\theta'} = \\frac{e^{Q^+(s,\\theta)/\\lambda}}{\\int e^{Q^+(s,\\theta')/\\lambda} d\\theta'}\\] <p>Multi-step trajectory:</p> \\[\\pi[\\tau | s_0] \\propto e^{-S_{\\text{GRL}}[\\tau]/\\lambda}\\] <p>This says: The probability of a trajectory is proportional to \\(\\exp(-\\text{action}/\\lambda)\\).</p> <p>In QM language: This is the Boltzmann weight of the trajectory in imaginary time.</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#33-soft-state-transitions-revisited","title":"3.3 Soft State Transitions Revisited","text":"<p>In Chapter 01a (Wavefunction Interpretation), we noted that GRL induces soft state transitions due to kernel overlap.</p> <p>Path integral perspective: </p> <ul> <li>Each trajectory contributes to the transition amplitude</li> <li>Kernel \\(k(z, z')\\) encodes transition amplitude between configurations</li> <li>Overlap creates interference-like effects (constructive/destructive reinforcement)</li> </ul> <p>Formally, the transition kernel is:</p> \\[P(s_{t+1}, \\theta_{t+1} | s_t, \\theta_t) = \\frac{1}{\\mathcal{Z}}\\int \\mathcal{D}[\\text{paths}] \\, e^{-S[\\text{path}]/\\lambda}\\] <p>where paths connect \\((s_t, \\theta_t)\\) to \\((s_{t+1}, \\theta_{t+1})\\).</p> <p>This is a path integral! The \"softness\" comes from summing over all possible intermediate trajectories.</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#4-complex-valued-grl-enabling-true-interference","title":"4. Complex-Valued GRL: Enabling True Interference","text":""},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#41-motivation-real-vs-complex-amplitudes","title":"4.1 Motivation: Real vs. Complex Amplitudes","text":"<p>Current GRL (Chapters 01-08):</p> <ul> <li>Real-valued \\(Q^+(z)\\)</li> <li>Boltzmann weights \\(e^{Q^+/\\lambda}\\) are always positive</li> <li>No true interference (only additive contributions)</li> </ul> <p>Complex-valued extension:</p> <ul> <li>\\(Q^+(z) \\in \\mathbb{C}\\) (complex reinforcement field)</li> <li>Amplitudes \\(\\psi(z) = e^{i\\phi(z)}\\) where \\(\\phi(z) = Q^+(z)/\\hbar_{\\text{eff}}\\)</li> <li>Phase \\(\\phi(z)\\) enables interference</li> </ul> <p>Why complex? - Interference: Paths can destructively interfere (cancel out) - Richer representations: Phase encodes additional structure - Quantum-inspired exploration: \"Tunneling\" through barriers</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#42-complex-rkhs-chapter-03-revisited","title":"4.2 Complex RKHS (Chapter 03 Revisited)","text":"<p>From Chapter 03: Complex-Valued RKHS, we can extend GRL to complex functions:</p> \\[Q^+: \\mathcal{Z} \\to \\mathbb{C}\\] <p>with complex kernel \\(k: \\mathcal{Z} \\times \\mathcal{Z} \\to \\mathbb{C}\\) (sesquilinear).</p> <p>Path integral with complex action:</p> \\[\\mathcal{Z}(z_i, z_f) = \\int \\mathcal{D}[\\tau] \\, e^{iS_{\\text{complex}}[\\tau]/\\hbar_{\\text{eff}}}\\] <p>where \\(S_{\\text{complex}}[\\tau] = \\int [Q^+(z_t) + \\frac{i}{2\\hbar_{\\text{eff}}}\\|\\dot{z}_t\\|^2] dt\\).</p> <p>Probability (Born rule):</p> \\[P(\\tau) = \\frac{|\\mathcal{Z}_\\tau|^2}{\\sum_{\\tau'} |\\mathcal{Z}_{\\tau'}|^2}\\] <p>This allows paths to interfere!</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#43-example-double-slit-in-action-space","title":"4.3 Example: Double-Slit in Action Space","text":"<p>Setup: Two discrete actions \\(\\theta_A\\) and \\(\\theta_B\\) both lead to the same next state \\(s'\\).</p> <p>Classical GRL (real \\(Q^+\\)):</p> \\[P(s') = P(s' | \\theta_A)P(\\theta_A) + P(s' | \\theta_B)P(\\theta_B)\\] <p>(Incoherent sum, no interference)</p> <p>Complex GRL:</p> \\[\\text{Amplitude}(s') = \\psi(s' | \\theta_A) + \\psi(s' | \\theta_B)\\] \\[P(s') = |\\text{Amplitude}(s')|^2 = |\\psi(s'|\\theta_A) + \\psi(s'|\\theta_B)|^2\\] <p>Expanding:</p> \\[P(s') = |\\psi(s'|\\theta_A)|^2 + |\\psi(s'|\\theta_B)|^2 + 2\\text{Re}[\\psi^*(s'|\\theta_A)\\psi(s'|\\theta_B)]\\] <p>The last term is the interference term! It can be positive (constructive) or negative (destructive).</p> <p>Interpretation:</p> <ul> <li>If phases align: \\(\\psi(s'|\\theta_A)\\) and \\(\\psi(s'|\\theta_B)\\) reinforce \u2192 higher \\(P(s')\\)</li> <li>If phases oppose: They cancel \u2192 lower \\(P(s')\\)</li> <li>This allows the agent to suppress undesirable state transitions</li> </ul>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#5-practical-path-integral-algorithms","title":"5. Practical Path Integral Algorithms","text":""},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#51-path-integral-policy-improvement-pi2","title":"5.1 Path Integral Policy Improvement (PI\u00b2)","text":"<p>Algorithm (Theodorou et al., 2010):</p> <ol> <li> <p>Rollout: Sample \\(K\\) noisy trajectories around current policy    $\\(\\tau_k \\sim \\pi(\\cdot | s_t) + \\epsilon_k, \\quad k = 1, \\ldots, K\\)$</p> </li> <li> <p>Compute costs: \\(S_k = \\int_0^T C(s_t^{(k)}, u_t^{(k)}) dt\\)</p> </li> <li> <p>Weight trajectories: \\(w_k = \\frac{e^{-S_k/\\lambda}}{\\sum_{j=1}^K e^{-S_j/\\lambda}}\\)</p> </li> <li> <p>Update policy: \\(\\pi_{\\text{new}}(u|s) = \\sum_{k=1}^K w_k \\, \\delta(u - u_k)\\)</p> </li> </ol> <p>This is empirical path integration!</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#52-grl-adaptation-particle-weighted-path-integration","title":"5.2 GRL Adaptation: Particle-Weighted Path Integration","text":"<p>For GRL:</p> <ol> <li> <p>Particle memory: \\(\\{(z_i, w_i)\\}\\) defines \\(Q^+(z)\\)</p> </li> <li> <p>Trajectory rollout: From state \\(s_t\\), sample \\(K\\) action sequences:    $\\(\\{\\theta_t^{(k)}, \\theta_{t+1}^{(k)}, \\ldots\\}_{k=1}^K\\)$</p> </li> <li> <p>Compute cumulative return:     $\\(R_k = \\sum_{\\tau=t}^T r_\\tau^{(k)}\\)$</p> </li> <li> <p>Path action:    $\\(S_k = -R_k + \\frac{1}{2\\lambda}\\sum_{\\tau} \\|\\theta_{\\tau+1}^{(k)} - \\theta_\\tau^{(k)}\\|^2\\)$</p> </li> <li> <p>Weight paths: \\(w_k \\propto e^{-S_k/\\lambda}\\)</p> </li> <li> <p>Update particles: Add/modify particles along high-weight paths</p> </li> </ol> <p>Key difference from PI\u00b2: GRL uses particle memory to encode \\(Q^+\\), not parametric policy.</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#53-langevin-sampling-on-the-field","title":"5.3 Langevin Sampling on the Field","text":"<p>For continuous action spaces, sample actions via Langevin dynamics:</p> \\[\\theta_{k+1} = \\theta_k + \\epsilon \\nabla_\\theta Q^+(s, \\theta_k) + \\sqrt{2\\epsilon\\lambda} \\, \\xi_k\\] <p>where \\(\\xi_k \\sim \\mathcal{N}(0, I)\\).</p> <p>This is gradient flow on the reinforcement field!</p> <p>Path integral view: </p> <ul> <li>Each Langevin trajectory is a sample from the path integral</li> <li>Multiple samples approximate \\(\\mathcal{Z}(z_i, z_f)\\)</li> <li>Action selection = finding high-probability paths</li> </ul>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#6-connection-to-quantum-measurement-chapter-05","title":"6. Connection to Quantum Measurement (Chapter 05)","text":""},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#61-concept-subspaces-as-measurement-operators","title":"6.1 Concept Subspaces as Measurement Operators","text":"<p>From Chapter 05: Concept Projections and Measurements, concepts are subspaces \\(\\mathcal{H}_c \\subset \\mathcal{H}\\) with projection operator \\(\\hat{P}_c\\).</p> <p>Path integral interpretation:</p> <ul> <li>Measuring concept \\(c\\) = projecting trajectory onto \\(\\mathcal{H}_c\\)</li> <li>Post-measurement state: \\(|\\psi'\\rangle = \\hat{P}_c |\\psi\\rangle / \\|\\hat{P}_c |\\psi\\rangle\\|\\)</li> <li>Measurement collapses the path integral to concept-compatible paths</li> </ul> <p>Formally, after measuring \\(c\\):</p> \\[\\mathcal{Z}_c(z_i, z_f) = \\int_{\\tau \\in \\mathcal{H}_c} \\mathcal{D}[\\tau] \\, e^{-S[\\tau]/\\lambda}\\] <p>This is a conditional path integral over concept-consistent trajectories.</p>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#62-hierarchical-composition-via-path-integrals","title":"6.2 Hierarchical Composition via Path Integrals","text":"<p>Nested concepts \\(\\mathcal{H}_{c_1} \\subset \\mathcal{H}_{c_2}\\) define a hierarchy.</p> <p>Path integral view:</p> <ul> <li>Low-level concept \\(c_1\\): Paths stay in local subspace</li> <li>High-level concept \\(c_2\\): Paths stay in larger subspace</li> <li>Transition \\(c_1 \\to c_1'\\): Path integral between subspaces</li> </ul> <p>This is exactly how quantum mechanics handles multi-scale dynamics!</p> <p>Example: Molecular dynamics - Electronic states (fast, local paths) - Nuclear motion (slow, global paths) - Born-Oppenheimer approximation: Separate path integrals</p> <p>In GRL: </p> <ul> <li>Low-level: Action parameter trajectories</li> <li>High-level: Concept activation trajectories</li> <li>Spectral methods: Identify slow vs. fast modes</li> </ul>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#7-advanced-topics-feynman-diagrams-for-grl","title":"7. Advanced Topics: Feynman Diagrams for GRL?","text":""},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#71-perturbation-theory","title":"7.1 Perturbation Theory","text":"<p>In QM, Feynman diagrams represent terms in the perturbative expansion of path integrals.</p> <p>For GRL, could we develop a diagrammatic expansion?</p> <p>Idea:</p> <ul> <li>Vertices: MemoryUpdate operations (particle creation/modification)</li> <li>Edges: Kernel interactions (particle propagation)</li> <li>Loops: Circular dependencies in belief update</li> </ul> <p>This is speculative, but could formalize:</p> <ul> <li>How perturbations propagate through memory</li> <li>Which particles are \"entangled\" (strongly coupled)</li> <li>Computational complexity of belief updates</li> </ul>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#72-instantons-and-rare-events","title":"7.2 Instantons and Rare Events","text":"<p>In QM, instantons are classical solutions to the Euclidean equations of motion that dominate tunneling.</p> <p>For GRL:</p> <ul> <li>Instanton = rare, high-cost trajectory that dramatically changes \\(Q^+\\)</li> <li>Example: Discovering a shortcut in navigation (low probability, high reward)</li> <li>Instanton calculus: Approximate path integral by saddle points</li> </ul> <p>Practical use:</p> <ul> <li>Identify critical experiences (instantons) for learning</li> <li>Prioritize replay of high-impact trajectories</li> <li>Understand exploration-exploitation via tunneling rates</li> </ul>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#8-implementation-sketch","title":"8. Implementation Sketch","text":""},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#81-complex-valued-particle-weights","title":"8.1 Complex-Valued Particle Weights","text":"<p>Extend particles to complex weights:</p> <pre><code>class ComplexParticle:\n    def __init__(self, z, w_real, w_imag):\n        self.z = z  # Augmented state (s, \u03b8)\n        self.w = complex(w_real, w_imag)  # Complex weight\n\n    def phase(self):\n        return np.angle(self.w)\n\n    def magnitude(self):\n        return np.abs(self.w)\n</code></pre> <p>Complex kernel:</p> <pre><code>def complex_kernel(z1, z2, length_scale=1.0, phase_scale=1.0):\n    \"\"\"Complex-valued RBF kernel with phase.\"\"\"\n    dist = np.linalg.norm(z1 - z2)\n    magnitude = np.exp(-dist**2 / (2 * length_scale**2))\n    phase = phase_scale * np.dot(z1 - z2, np.ones_like(z1))  # Example phase\n    return magnitude * np.exp(1j * phase)\n</code></pre>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#82-path-integral-sampling","title":"8.2 Path Integral Sampling","text":"<p>Sample action via path integral:</p> <pre><code>def sample_action_path_integral(particles, s, lambda_temp, n_samples=100):\n    \"\"\"Sample action using path integral over trajectories.\"\"\"\n    theta_samples = []\n    weights = []\n\n    for _ in range(n_samples):\n        # Sample candidate action\n        theta = sample_from_prior(s)\n\n        # Compute path action\n        S = compute_action(particles, s, theta, lambda_temp)\n\n        # Boltzmann weight\n        w = np.exp(-S / lambda_temp)\n\n        theta_samples.append(theta)\n        weights.append(w)\n\n    # Normalize\n    weights = np.array(weights)\n    weights /= weights.sum()\n\n    # Sample from weighted distribution\n    idx = np.random.choice(len(theta_samples), p=weights)\n    return theta_samples[idx]\n\ndef compute_action(particles, s, theta, lambda_temp):\n    \"\"\"Compute action for single-step decision.\"\"\"\n    z = (s, theta)\n\n    # Energy term (from particles)\n    Q_plus = sum(p.w.real * kernel(z, p.z) for p in particles)\n    E = -Q_plus\n\n    # Kinetic term (assume small step)\n    kinetic = 0.0  # Simplified for single step\n\n    return E + kinetic / (2 * lambda_temp)\n</code></pre>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#83-interference-visualization","title":"8.3 Interference Visualization","text":"<p>Plot interference pattern in action space:</p> <pre><code>import matplotlib.pyplot as plt\n\ndef plot_interference(particles, s, theta_min, theta_max, n_points=200):\n    \"\"\"Plot real and imaginary parts of complex Q+.\"\"\"\n    theta_range = np.linspace(theta_min, theta_max, n_points)\n    Q_real = np.zeros(n_points)\n    Q_imag = np.zeros(n_points)\n\n    for i, theta in enumerate(theta_range):\n        z = (s, theta)\n        Q_complex = sum(p.w * complex_kernel(z, p.z) for p in particles)\n        Q_real[i] = Q_complex.real\n        Q_imag[i] = Q_complex.imag\n\n    plt.figure(figsize=(12, 4))\n\n    plt.subplot(1, 3, 1)\n    plt.plot(theta_range, Q_real, label='Re(Q+)')\n    plt.xlabel('\u03b8')\n    plt.ylabel('Real part')\n    plt.legend()\n\n    plt.subplot(1, 3, 2)\n    plt.plot(theta_range, Q_imag, label='Im(Q+)')\n    plt.xlabel('\u03b8')\n    plt.ylabel('Imaginary part')\n    plt.legend()\n\n    plt.subplot(1, 3, 3)\n    prob = Q_real**2 + Q_imag**2  # |\u03c8|^2\n    plt.plot(theta_range, prob, label='|Q+|^2 (probability)')\n    plt.xlabel('\u03b8')\n    plt.ylabel('Probability density')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#9-summary-why-path-integrals-matter-for-grl","title":"9. Summary: Why Path Integrals Matter for GRL","text":"<p>Quantum mechanical foundation:</p> <ul> <li>GRL's Boltzmann policy is the imaginary-time path integral solution</li> <li>Not an analogy\u2014a mathematical equivalence</li> <li>Connects RL to 70+ years of QM path integral techniques</li> </ul> <p>Complex-valued extension:</p> <ul> <li>Enables true interference (constructive/destructive)</li> <li>Richer policy representations via phase</li> <li>Tunneling-like exploration through barriers</li> </ul> <p>Practical algorithms:</p> <ul> <li>Path integral policy improvement (PI\u00b2)</li> <li>Langevin sampling as path integration</li> <li>Complex particle weights for interference</li> </ul> <p>Future directions:</p> <ul> <li>Feynman diagrams for belief propagation</li> <li>Instanton calculus for rare events</li> <li>Hierarchical path integrals for concept composition</li> </ul> <p>This completes the quantum-inspired trilogy:</p> <ol> <li>Chapters 01-02: RKHS \u2194 Hilbert space parallel</li> <li>Chapters 03-08: Amplitude formulation and learning</li> <li>Chapter 09: Path integrals as foundational formalism</li> </ol>"},{"location":"GRL0/quantum_inspired/09-path-integrals-and-action-principles/#further-reading","title":"Further Reading","text":"<p>Feynman Path Integrals:</p> <ul> <li>Feynman, R. P., &amp; Hibbs, A. R. (1965). Quantum Mechanics and Path Integrals. McGraw-Hill.</li> <li>Kleinert, H. (2009). Path Integrals in Quantum Mechanics, Statistics, Polymer Physics, and Financial Markets (5<sup>th</sup> ed.). World Scientific.</li> </ul> <p>Path Integral Control:</p> <ul> <li>Kappen, H. J. (2005). \"Path integrals and symmetry breaking for optimal control theory.\" Journal of Statistical Mechanics.</li> <li>Theodorou, E., Buchli, J., &amp; Schaal, S. (2010). \"A generalized path integral control approach to reinforcement learning.\" JMLR.</li> <li>Levine, S. (2018). \"Reinforcement learning and control as probabilistic inference: Tutorial and review.\" arXiv:1805.00909.</li> </ul> <p>Quantum Computation &amp; ML:</p> <ul> <li>Schuld, M., &amp; Petruccione, F. (2018). Supervised Learning with Quantum Computers. Springer.</li> <li>Wittek, P. (2014). Quantum Machine Learning. Academic Press.</li> <li>Biamonte, J., et al. (2017). \"Quantum machine learning.\" Nature, 549, 195-202.</li> </ul> <p>Complex-Valued Neural Networks:</p> <ul> <li>Trabelsi, C., et al. (2018). \"Deep complex networks.\" ICLR.</li> <li>Virtue, P., Yu, S. X., &amp; Lustig, M. (2017). \"Better than real: Complex-valued neural nets for MRI fingerprinting.\" ICIP.</li> </ul> <p>\u2190 Back to Chapter 08: Memory Dynamics | Quantum-Inspired README</p> <p>Related: Tutorial Chapter 03a - Least Action Principle</p>"},{"location":"GRL0/tutorials/","title":"GRL Tutorial Chapters","text":"<p>Format: Tutorial paper chapters Audience: ML practitioners with basic RL knowledge Style: Narrative, educational, self-contained</p>"},{"location":"GRL0/tutorials/#overview","title":"Overview","text":"<p>These chapters form the core of the GRL tutorial paper. Each chapter builds on previous ones while remaining accessible for selective reading.</p>"},{"location":"GRL0/tutorials/#chapter-index","title":"Chapter Index","text":""},{"location":"GRL0/tutorials/#part-i-foundations","title":"Part I: Foundations","text":"# Title Key Concepts Status 00 Overview What is GRL, motivation \u2705 Complete 01 Core Concepts Augmented space, parametric actions \u2705 Complete 02 RKHS Foundations Kernels, inner products, function spaces \u2705 Complete 03 Energy and Fitness Sign conventions, EBM connection \u2705 Complete 03a Least Action Principle (supplement) Path integrals, Boltzmann policy, action discovery \u2705 Complete"},{"location":"GRL0/tutorials/#part-ii-reinforcement-field","title":"Part II: Reinforcement Field","text":"# Title Key Concepts Status 04 Reinforcement Field Functional field, RKHS gradient \u2705 Complete 04a Riesz Representer (supplement) Gradients in function space, examples \u2705 Complete 05 Particle Memory Particles as basis, memory as belief \u2705 Complete"},{"location":"GRL0/tutorials/#part-iii-algorithms","title":"Part III: Algorithms","text":"# Title Key Concepts Status 06 MemoryUpdate Belief transition, Algorithm 1, particle evolution \u2705 Complete 06a Advanced Memory Dynamics (supplement) Top-k neighbors, surprise-gating, practical improvements \u2705 Complete 07 RF-SARSA Functional TD learning, two-layer architecture, Algorithm 2 \u2705 Complete 07a Continuous Policy Inference (supplement) Beyond discrete actions, Langevin sampling, actor-critic in RKHS \u2705 Complete"},{"location":"GRL0/tutorials/#part-iv-interpretation","title":"Part IV: Interpretation","text":"# Title Key Concepts Status 08 Soft State Transitions Emergent uncertainty \u23f3 Planned 09 POMDP Interpretation Belief-based view \u23f3 Planned 10 Complete System Putting it together \u23f3 Planned"},{"location":"GRL0/tutorials/#reading-recommendations","title":"Reading Recommendations","text":""},{"location":"GRL0/tutorials/#new-to-grl","title":"New to GRL","text":"<p>Start with Chapter 00 (Overview), then proceed sequentially through the completed chapters.</p>"},{"location":"GRL0/tutorials/#familiar-with-rl-theory","title":"Familiar with RL Theory","text":"<p>Skim Chapter 00, focus on Chapters 02-04 for mathematical foundations.</p>"},{"location":"GRL0/tutorials/#want-to-implement","title":"Want to Implement","text":"<p>Read Chapter 00, skim 01-04, then wait for Chapters 05-07 on algorithms.</p>"},{"location":"GRL0/tutorials/#quick-understanding","title":"Quick Understanding","text":"<p>Read Chapters 00, 01, and 04 for the essential concepts.</p>"},{"location":"GRL0/tutorials/#chapter-progression","title":"Chapter Progression","text":"<pre><code>00-Overview\n    \u2193\n01-Core Concepts\n    \u2193\n02-RKHS Foundations\n    \u2193\n03-Energy and Fitness\n    \u2193\n03a-Least Action Principle (supplement) \u2190 New!\n    \u2193\n04-Reinforcement Field\n    \u2193\n04a-Riesz Representer (supplement)\n    \u2193\n05-Particle Memory\n    \u2193\n06-MemoryUpdate\n    \u2193\n06a-Advanced Memory Dynamics (supplement)\n    \u2193\n07-RF-SARSA\n    \u2193\n07a-Continuous Policy Inference (supplement) \u2190 We are here\n    \u2193\n08-Soft State Transitions (planned)\n    \u2193\n...\n</code></pre>"},{"location":"GRL0/tutorials/#chapter-template","title":"Chapter Template","text":"<p>Each chapter follows this structure:</p> <ol> <li>Header: Purpose, prerequisites, key concepts</li> <li>Introduction: Why this topic matters</li> <li>Main Content: Narrative explanation with examples</li> <li>Key Takeaways: Summary points</li> <li>Next Steps: Connection to following chapters</li> </ol>"},{"location":"GRL0/tutorials/#notation-conventions","title":"Notation Conventions","text":"Symbol Meaning \\(s\\) Environment state \\(\\theta\\) Action parameters \\(z = (s, \\theta)\\) Augmented state-action point \\(k(\\cdot, \\cdot)\\) Kernel function \\(\\mathcal{H}_k\\) RKHS induced by kernel \\(k\\) \\(Q^+(z)\\) Field value (fitness) at \\(z\\) \\(E(z)\\) Energy at \\(z\\), equals \\(-Q^+(z)\\) \\(\\Omega\\) Particle memory \\(w_i\\) Weight of particle \\(i\\)"},{"location":"GRL0/tutorials/#key-equations","title":"Key Equations","text":""},{"location":"GRL0/tutorials/#reinforcement-field","title":"Reinforcement Field","text":"\\[Q^+(z) = \\sum_{i=1}^N w_i \\, k(z, z_i)\\]"},{"location":"GRL0/tutorials/#rkhs-inner-product","title":"RKHS Inner Product","text":"\\[\\langle k(x_1, \\cdot), k(x_2, \\cdot) \\rangle_{\\mathcal{H}_k} = k(x_1, x_2)\\]"},{"location":"GRL0/tutorials/#energy-fitness-relationship","title":"Energy-Fitness Relationship","text":"\\[E(z) = -Q^+(z)\\]"},{"location":"GRL0/tutorials/#functional-gradient","title":"Functional Gradient","text":"\\[\\nabla_z Q^+(z) = \\sum_{i=1}^N w_i \\, \\nabla_z k(z, z_i)\\]"},{"location":"GRL0/tutorials/#boltzmann-policy","title":"Boltzmann Policy","text":"\\[\\pi(\\theta | s) \\propto \\exp(\\beta \\, Q^+(s, \\theta))\\]"},{"location":"GRL0/tutorials/#beyond-part-i","title":"Beyond Part I","text":""},{"location":"GRL0/tutorials/#quantum-inspired-extensions","title":"Quantum-Inspired Extensions","text":"<p>Advanced Topics \u2192 (9 chapters complete)</p> <p>Mathematical connections to quantum mechanics, amplitude-based learning, and novel memory dynamics.</p>"},{"location":"GRL0/tutorials/#research-roadmap","title":"Research Roadmap","text":"<p>Full Roadmap \u2192</p> <p>Comprehensive plan for GRL v0, extensions, and future papers (A, B, C).</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/tutorials/00-overview/","title":"Chapter 0: What is Generalized Reinforcement Learning?","text":"<p>Purpose: Introduce GRL and explain why it matters Prerequisites: Basic understanding of reinforcement learning Key Concepts: Parametric actions, operator view, enriched action space</p>"},{"location":"GRL0/tutorials/00-overview/#introduction","title":"Introduction","text":"<p>Imagine you're teaching a robot to navigate a room. In traditional reinforcement learning, you might give it a fixed set of actions: move forward, turn left, turn right, stop. The robot learns which action to take in each situation.</p> <p>But what if instead of choosing from predefined actions, the robot could synthesize its own movements? What if it could learn to generate smooth trajectories, apply forces with varying magnitudes, or create entirely new motion patterns that weren't in any predefined action set?</p> <p>This is the core idea behind Generalized Reinforcement Learning (GRL).</p>"},{"location":"GRL0/tutorials/00-overview/#the-limitation-of-traditional-rl","title":"The Limitation of Traditional RL","text":"<p>In standard reinforcement learning, an agent interacts with an environment through a fixed action space. Whether discrete (like game controls) or continuous (like motor torques), actions are typically treated as symbols or vectors that the agent selects.</p> <pre><code>Standard RL:\n  State s \u2192 Policy \u03c0 \u2192 Action a \u2208 A \u2192 Next State s'\n</code></pre> <p>This works well for many problems, but it has a fundamental limitation: the action space is predetermined. The agent can only choose from what the designer provides.</p> <p>Consider these scenarios where fixed actions fall short:</p> <ul> <li>Continuous control: Discretizing continuous actions loses precision</li> <li>Compositional actions: Complex behaviors require sequences of primitives</li> <li>Novel situations: Predefined actions may not cover all possibilities</li> <li>Transfer: Action spaces don't generalize across environments</li> </ul>"},{"location":"GRL0/tutorials/00-overview/#the-grl-perspective-actions-as-operators","title":"The GRL Perspective: Actions as Operators","text":"<p>GRL takes a radically different view. Instead of treating actions as symbols to select, GRL treats them as mathematical operators that transform the state space.</p> <pre><code>GRL:\n  State s \u2192 Policy \u03c0 \u2192 Operator \u00d4 \u2192 New State s' = \u00d4(s)\n</code></pre> <p>Think of it this way:</p> <ul> <li>In traditional RL, an action is a label (\"turn left\")</li> <li>In GRL, an action is a transformation (a rotation matrix, a force field, a differential equation)</li> </ul> <p>This shift has profound implications.</p>"},{"location":"GRL0/tutorials/00-overview/#what-is-an-action-operator","title":"What is an Action Operator?","text":"<p>An action operator \\(\\hat{O}\\) is a mathematical object that, when applied to the current state, produces the next state. Examples include:</p> Environment Operator Type Example Robot navigation Force vector Apply 3N forward, 1N right Pendulum control Torque Apply 0.5 Nm clockwise Game playing State transformation Swap positions of pieces A and B Portfolio management Allocation function Redistribute 10% to bonds <p>The key insight is that these operators are parameterized. A force vector has magnitude and direction parameters. A torque has magnitude. An allocation function has percentages.</p>"},{"location":"GRL0/tutorials/00-overview/#parametric-actions","title":"Parametric Actions","text":"<p>GRL represents actions through their parameters \\(\\theta\\):</p> \\[ \\hat{O} = \\hat{O}(\\theta) \\] <p>The agent doesn't select from a fixed set of operators. Instead, it learns to generate the right parameters for the right situation.</p>"},{"location":"GRL0/tutorials/00-overview/#the-enriched-action-space","title":"The Enriched Action Space","text":"<p>When actions become parameterized operators, the action space transforms from a finite set to a continuous manifold of possibilities.</p> Traditional RL GRL Discrete: Continuous: direction \u2208 [0, 2\u03c0] Finite: |A| = 4 Infinite: dim(\u0398) = d Enumerable Differentiable <p>This \"enriched action space\" is the space of all possible operator parameters. It's typically a smooth manifold where nearby points correspond to similar operators.</p>"},{"location":"GRL0/tutorials/00-overview/#augmented-state-space","title":"Augmented State Space","text":"<p>To reason about actions and states together, GRL introduces the augmented state space. This combines the environment state \\(s\\) with action parameters \\(\\theta\\):</p> <p>$$</p> <p>z = (s, \\theta) \\in \\mathcal{S} \\times \\Theta $$</p> <p>Why combine them? Because in GRL, we want to evaluate \"how good is this action in this state?\" as a continuous function over the joint space.</p> <p>Think of it as asking: \"If I'm in state \\(s\\) and I apply an operator with parameters \\(\\theta\\), what value do I expect?\"</p> <p>This unified view enables:</p> <ul> <li>Smooth generalization across similar state-action pairs</li> <li>Continuous value functions over the joint space</li> <li>Gradient-based reasoning about actions</li> </ul>"},{"location":"GRL0/tutorials/00-overview/#the-reinforcement-field","title":"The Reinforcement Field","text":"<p>Traditional RL learns a value function \\(V(s)\\) or \\(Q(s, a)\\) that assigns values to states or state-action pairs.</p> <p>GRL learns a reinforcement field \\(Q^+(z) = Q^+(s, \\theta)\\) \u2014 a smooth function over the entire augmented space that tells us the value of each possible state-action configuration.</p> <p>Because this function lives in a special mathematical space (a Reproducing Kernel Hilbert Space, which we'll explore later), it has nice properties:</p> <ul> <li>Smoothness: Nearby configurations have similar values</li> <li>Generalization: We can estimate values for unseen configurations</li> <li>Gradients: We can compute how value changes with small parameter changes</li> </ul>"},{"location":"GRL0/tutorials/00-overview/#how-grl-learns","title":"How GRL Learns","text":"<p>GRL doesn't optimize a policy network directly. Instead, it maintains a particle-based representation of the reinforcement field.</p> <p>Each \"particle\" is a remembered experience embedded in the augmented space:</p> <ul> <li>Location: Where in \\((s, \\theta)\\) space this experience occurred</li> <li>Value: What reinforcement was received</li> </ul> <p>Through interaction with the environment:</p> <ol> <li>New experiences create new particles</li> <li>Particles accumulate and interact</li> <li>The reinforcement field emerges from the particle ensemble</li> <li>Action selection queries the field to find high-value regions</li> </ol> <p>This is similar to how a swarm of samples can approximate a probability distribution \u2014 except here, the particles approximate a value landscape.</p>"},{"location":"GRL0/tutorials/00-overview/#why-this-matters","title":"Why This Matters","text":"<p>GRL's operator view offers several advantages:</p>"},{"location":"GRL0/tutorials/00-overview/#1-continuous-action-generation","title":"1. Continuous Action Generation","text":"<p>Instead of discretizing continuous actions (and losing precision), GRL naturally handles continuous parameter spaces.</p>"},{"location":"GRL0/tutorials/00-overview/#2-compositional-actions","title":"2. Compositional Actions","text":"<p>Operators can be composed: \\(\\hat{O}_2 \\circ \\hat{O}_1\\) applies \\(\\hat{O}_1\\) then \\(\\hat{O}_2\\). This enables hierarchical and compositional action structures.</p>"},{"location":"GRL0/tutorials/00-overview/#3-transfer-and-generalization","title":"3. Transfer and Generalization","text":"<p>Because actions are parameterized transformations, similar operators (nearby in parameter space) produce similar effects. This enables smooth generalization.</p>"},{"location":"GRL0/tutorials/00-overview/#4-physical-interpretability","title":"4. Physical Interpretability","text":"<p>In physics-based domains, operator parameters often have direct physical meaning (forces, torques, fields), making the learned behavior more interpretable.</p>"},{"location":"GRL0/tutorials/00-overview/#5-uncertainty-quantification","title":"5. Uncertainty Quantification","text":"<p>The particle-based representation naturally captures uncertainty: sparse particles mean high uncertainty, dense particles mean confidence.</p>"},{"location":"GRL0/tutorials/00-overview/#preview-of-whats-ahead","title":"Preview of What's Ahead","text":"<p>This tutorial will build up the full GRL framework:</p> <ol> <li>Core Concepts (Chapter 1): Augmented space, particles, kernels</li> <li>RKHS Foundations (Chapter 2): The mathematical space where GRL lives</li> <li>Energy and Fitness (Chapter 3): How we measure value</li> <li>Reinforcement Field (Chapter 4): The value landscape</li> <li>Particle Memory (Chapter 5): How experience is represented</li> <li>Algorithms (Chapters 6-7): MemoryUpdate and RF-SARSA</li> <li>Interpretation (Chapters 8-10): Soft transitions, POMDP view, synthesis</li> </ol> <p>By the end, you'll understand:</p> <ul> <li>How GRL represents and learns from experience</li> <li>Why its particle-based approach differs from policy gradient methods</li> <li>How to implement and apply GRL to control problems</li> </ul>"},{"location":"GRL0/tutorials/00-overview/#key-takeaways","title":"Key Takeaways","text":"<ul> <li>Traditional RL treats actions as symbols to select from a fixed set</li> <li>GRL treats actions as parametric operators that transform state</li> <li>The action space becomes a continuous manifold of operator parameters</li> <li>Augmented space combines state and action parameters: \\(z = (s, \\theta)\\)</li> <li>The reinforcement field is a value function over augmented space</li> <li>Particles represent remembered experiences in this space</li> <li>GRL enables continuous actions, composition, generalization, and uncertainty</li> </ul>"},{"location":"GRL0/tutorials/00-overview/#beyond-this-tutorial-quantum-inspired-extensions","title":"Beyond This Tutorial: Quantum-Inspired Extensions","text":"<p>One of GRL's most distinctive theoretical contributions is its connection to quantum mechanical formalism\u2014not in physics, but in the mathematical structure of probability.</p>"},{"location":"GRL0/tutorials/00-overview/#probability-amplitudes-in-ml","title":"Probability Amplitudes in ML","text":"<p>Traditional ML uses probabilities directly: \\(p(x)\\). GRL, through its RKHS formulation, naturally introduces:</p> <ul> <li>Probability amplitudes: Inner products \\(\\langle \\psi | \\phi \\rangle\\) that must be squared to get probabilities</li> <li>Superposition: Particle ensembles as weighted sums of basis states</li> <li>Interference: Complex-valued RKHS enabling constructive/destructive interference</li> <li>Phase semantics: Complex phases encoding temporal, contextual, or directional information</li> </ul> <p>This formulation is novel to mainstream ML and opens new directions for probabilistic reasoning, uncertainty representation, and multi-modal learning.</p> <p>See: Section V of the original paper for concept-driven learning and spectral methods.</p>"},{"location":"GRL0/tutorials/00-overview/#references","title":"References","text":"<p>This tutorial series is based on:</p> <p>Chiu, P.-H., &amp; Huber, M. (2022). Generalized Reinforcement Learning: Experience Particles, Action Operator, Reinforcement Field, Memory Association, and Decision Concepts. arXiv:2208.04822. https://arxiv.org/abs/2208.04822</p>"},{"location":"GRL0/tutorials/00-overview/#next-steps","title":"Next Steps","text":"<p>In Chapter 1: Core Concepts, we'll dive deeper into:</p> <ul> <li>How augmented state space is constructed</li> <li>What particles represent mathematically</li> <li>The role of kernel functions in defining similarity</li> </ul> <p>Last Updated: January 12, 2026</p>"},{"location":"GRL0/tutorials/01-core-concepts/","title":"Chapter 1: Core Concepts","text":"<p>Purpose: Introduce the fundamental building blocks of GRL Prerequisites: Chapter 0 (Overview) Key Concepts: Augmented state space, parametric actions, experience particles, kernel similarity</p>"},{"location":"GRL0/tutorials/01-core-concepts/#introduction","title":"Introduction","text":"<p>In Chapter 0, we introduced the central idea of GRL: treating actions as parametric operators rather than fixed symbols. Now we'll formalize the core building blocks that make this possible:</p> <ol> <li>Parametric Actions: How actions are represented as parameter vectors</li> <li>Augmented State Space: The joint space of states and action parameters  </li> <li>Experience Particles: How we represent and store experience</li> <li>Kernel Similarity: How we measure relationships between experiences</li> </ol> <p>These concepts form the foundation on which the reinforcement field, algorithms, and policy inference are built.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#1-parametric-actions","title":"1. Parametric Actions","text":""},{"location":"GRL0/tutorials/01-core-concepts/#from-symbols-to-parameters","title":"From Symbols to Parameters","text":"<p>In traditional RL, an action \\(a\\) is either:</p> <ul> <li>A discrete symbol from a finite set: \\(a \\in \\{1, 2, ..., K\\}\\)</li> <li>A continuous vector from a bounded region: \\(a \\in \\mathbb{R}^d\\)</li> </ul> <p>In GRL, we take the continuous view further. An action is represented by a parameter vector \\(\\theta \\in \\Theta\\) that specifies an operator:</p> \\[ \\theta \\to \\hat{O}(\\theta) \\] <p>The parameter space \\(\\Theta\\) is typically \\(\\mathbb{R}^d\\) for some dimension \\(d\\), though it could be a more structured manifold.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#examples","title":"Examples","text":"Domain Parameters \\(\\theta\\) Operator \\(\\hat{O}(\\theta)\\) 2D Navigation \\((F_x, F_y)\\) Force vector applied to agent Pendulum \\(\\tau\\) Torque applied to joint Portfolio \\((w_1, ..., w_n)\\) Asset allocation weights Image transformation \\((r, \\theta, s)\\) Rotation, angle, scale"},{"location":"GRL0/tutorials/01-core-concepts/#why-parameters-matter","title":"Why Parameters Matter","text":"<p>By treating actions as parameters, we gain:</p> <p>Continuity: Nearby parameters \\(\\theta\\) and \\(\\theta'\\) produce similar effects. This enables smooth generalization.</p> <p>Differentiability: We can compute gradients of outcomes with respect to \\(\\theta\\).</p> <p>Compositionality: Parameters can be structured (e.g., hierarchical) to enable compositional actions.</p> <p>Interpretability: Parameters often have physical meaning (force magnitude, angle, etc.).</p>"},{"location":"GRL0/tutorials/01-core-concepts/#2-augmented-state-space","title":"2. Augmented State Space","text":""},{"location":"GRL0/tutorials/01-core-concepts/#combining-states-and-actions","title":"Combining States and Actions","text":"<p>The key insight of GRL is to reason about states and actions together as points in a unified space. We define the augmented state-action point:</p> \\[ z = (s, \\theta) \\in \\mathcal{Z} = \\mathcal{S} \\times \\Theta \\] <p>where:</p> <ul> <li>\\(s \\in \\mathcal{S}\\) is the environment state (possibly embedded/encoded)</li> <li>\\(\\theta \\in \\Theta\\) is the action parameter vector</li> <li>\\(\\mathcal{Z}\\) is the augmented space</li> </ul>"},{"location":"GRL0/tutorials/01-core-concepts/#why-augment","title":"Why Augment?","text":"<p>In standard RL, we might learn \\(Q(s, a)\\) \u2014 the value of taking action \\(a\\) in state \\(s\\). In GRL, we learn \\(Q^+(z) = Q^+(s, \\theta)\\) \u2014 a value function over the entire augmented space.</p> <p>This has several advantages:</p> <p>Smooth Value Landscape: The value function is smooth over the continuous augmented space, enabling generalization.</p> <p>Unified Representation: State and action are treated symmetrically, enabling richer representations.</p> <p>Gradient Information: We can compute \\(\\nabla_\\theta Q^+(s, \\theta)\\) \u2014 how value changes with action parameters.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#embedding-functions","title":"Embedding Functions","text":"<p>In practice, we often use embedding functions to transform raw states and actions into suitable representations:</p> <p>$$</p> <p>z = (x_s(s), x_a(\\theta)) $$</p> <p>where:</p> <ul> <li>\\(x_s: \\mathcal{S} \\to \\mathbb{R}^{d_s}\\) embeds states</li> <li>\\(x_a: \\Theta \\to \\mathbb{R}^{d_a}\\) embeds action parameters</li> </ul> <p>These embeddings might be:</p> <ul> <li>Identity (raw features)</li> <li>Learned neural network encodings</li> <li>Hand-crafted features</li> </ul> <p>The choice of embedding affects the geometry of the augmented space and thus how similarity and generalization work.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#3-experience-particles","title":"3. Experience Particles","text":""},{"location":"GRL0/tutorials/01-core-concepts/#what-is-a-particle","title":"What is a Particle?","text":"<p>In GRL, experience is stored not as a replay buffer of transitions, but as a collection of particles in augmented space. Each particle is a tuple:</p> <p>$$</p> <p>\\omega_i = (z_i, w_i) = ((s_i, \\theta_i), w_i) $$</p> <p>where:</p> <ul> <li>\\(z_i = (s_i, \\theta_i)\\) is the location in augmented space</li> <li>\\(w_i \\in \\mathbb{R}\\) is the weight (typically related to value)</li> </ul>"},{"location":"GRL0/tutorials/01-core-concepts/#particle-memory","title":"Particle Memory","text":"<p>The agent maintains a particle memory:</p> <p>$$</p> <p>\\Omega = {(z_1, w_1), (z_2, w_2), ..., (z_N, w_N)} $$</p> <p>This collection of weighted particles represents the agent's accumulated experience. It's analogous to:</p> <ul> <li>A weighted sample approximation to a distribution</li> <li>A nonparametric function representation</li> <li>A memory of \"what happened where and how good it was\"</li> </ul>"},{"location":"GRL0/tutorials/01-core-concepts/#particles-vs-replay-buffer","title":"Particles vs. Replay Buffer","text":"Replay Buffer Particle Memory Stores transitions \\((s, a, r, s')\\) Stores points \\((z, w)\\) in augmented space Used for sampling and replaying Used for function approximation and inference Finite capacity, FIFO or priority Dynamic, merging/pruning operations Supports temporal learning Supports spatial generalization"},{"location":"GRL0/tutorials/01-core-concepts/#particle-operations","title":"Particle Operations","text":"<p>The particle memory supports several operations:</p> <p>Add: Insert a new particle \\((z, w)\\)</p> <p>Query: Evaluate the reinforcement field at a point \\(z\\) using nearby particles</p> <p>Merge: Combine similar particles to prevent unbounded growth</p> <p>Prune: Remove low-influence particles</p> <p>Update: Modify weights based on new reinforcement signals</p> <p>These operations are formalized in Algorithm 1 (MemoryUpdate), covered in Chapter 6.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#4-kernel-similarity","title":"4. Kernel Similarity","text":""},{"location":"GRL0/tutorials/01-core-concepts/#the-role-of-kernels","title":"The Role of Kernels","text":"<p>How do we determine which particles are \"nearby\" or \"similar\"? GRL uses kernel functions to define similarity in augmented space.</p> <p>A kernel \\(k: \\mathcal{Z} \\times \\mathcal{Z} \\to \\mathbb{R}\\) measures how similar two points are:</p> \\[ k(z, z') = k((s, \\theta), (s', \\theta')) \\] <p>Higher values mean more similar.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#common-kernels","title":"Common Kernels","text":"<p>Radial Basis Function (RBF) / Gaussian:</p> <p>$$</p> <p>k(z, z') = \\exp\\left(-\\frac{|z - z'|<sup>2}{2\\ell</sup>2}\\right) $$</p> <p>where \\(\\ell\\) is the lengthscale controlling how quickly similarity decays with distance.</p> <p>Automatic Relevance Determination (ARD):</p> <p>$$</p> <p>k(z, z') = \\exp\\left(-\\sum_{d=1}^{D} \\frac{(z_d - z'_d)<sup>2}{2\\ell_d</sup>2}\\right) $$</p> <p>Each dimension has its own lengthscale \\(\\ell_d\\), allowing the kernel to learn which features matter most.</p> <p>Composite Kernels:</p> <p>For augmented space, we might use:</p> <p>$$</p> <p>k(z, z') = k_s(s, s') \\cdot k_a(\\theta, \\theta') $$</p> <p>where \\(k_s\\) and \\(k_a\\) are separate kernels for state and action components.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#why-kernels-matter","title":"Why Kernels Matter","text":"<p>Kernels are central to GRL because they define:</p> <p>Generalization: How experience at one point informs predictions at other points</p> <p>Smoothness: How quickly the value function can change</p> <p>Feature Relevance: Which dimensions of state/action matter (via ARD)</p> <p>Geometry: The \"shape\" of the augmented space for learning</p>"},{"location":"GRL0/tutorials/01-core-concepts/#kernel-induced-function-representation","title":"Kernel-Induced Function Representation","text":"<p>Given particles \\(\\Omega = \\{(z_i, w_i)\\}\\) and kernel \\(k\\), we can define a function over the entire augmented space:</p> <p>$$</p> <p>f(z) = \\sum_{i=1}^{N} w_i \\, k(z, z_i) $$</p> <p>This is the reinforcement field \u2014 a smooth function that assigns values to every point in augmented space based on the weighted contributions of all particles.</p> <p>This representation:</p> <ul> <li>Is nonparametric: No fixed neural network architecture</li> <li>Is smooth: Inherits smoothness from the kernel</li> <li>Generalizes: Points far from any particle get low values</li> <li>Is adaptive: Adding particles reshapes the function</li> </ul>"},{"location":"GRL0/tutorials/01-core-concepts/#5-putting-it-together","title":"5. Putting It Together","text":"<p>Let's trace how these concepts connect:</p>"},{"location":"GRL0/tutorials/01-core-concepts/#1-agent-in-state-s","title":"1. Agent in State \\(s\\)","text":"<p>The agent observes state \\(s\\) from the environment.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#2-consider-action-parameters","title":"2. Consider Action Parameters","text":"<p>The agent considers action parameter \\(\\theta\\), forming augmented point \\(z = (s, \\theta)\\).</p>"},{"location":"GRL0/tutorials/01-core-concepts/#3-query-particle-memory","title":"3. Query Particle Memory","text":"<p>Using the kernel, the agent computes the reinforcement field value:</p> <p>$$</p> <p>Q^+(z) = \\sum_{i} w_i \\, k(z, z_i) $$</p>"},{"location":"GRL0/tutorials/01-core-concepts/#4-select-action","title":"4. Select Action","text":"<p>The agent selects action parameters that maximize \\(Q^+\\) (or samples according to a policy).</p>"},{"location":"GRL0/tutorials/01-core-concepts/#5-execute-and-observe","title":"5. Execute and Observe","text":"<p>The action is executed, reward \\(r\\) is received, next state \\(s'\\) is observed.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#6-update-particles","title":"6. Update Particles","text":"<p>A new particle is added or existing particles are updated based on the experience.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#7-repeat","title":"7. Repeat","text":"<p>The cycle continues, with the reinforcement field evolving as particles accumulate.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#visual-intuition","title":"Visual Intuition","text":"<p>Imagine a 2D augmented space where:</p> <ul> <li>The x-axis represents some aspect of state (e.g., position)</li> <li>The y-axis represents action parameter (e.g., force magnitude)</li> </ul> <p>Each particle is a point in this 2D space with an associated weight (color/size indicating value).</p> <p>The kernel defines how much each particle influences nearby points \u2014 like a Gaussian \"bump\" centered at each particle.</p> <p>The reinforcement field is the sum of all these bumps \u2014 a smooth landscape over the entire space.</p> <p>High regions: Good state-action combinations (high expected return)</p> <p>Low regions: Poor combinations (low expected return)</p> <p>Sparse regions: Uncertainty (few particles nearby)</p> <p>Policy learning = navigating and reshaping this landscape.</p>"},{"location":"GRL0/tutorials/01-core-concepts/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Parametric actions represent actions as parameter vectors \\(\\theta\\) that specify operators</p> </li> <li> <p>Augmented state space \\(\\mathcal{Z} = \\mathcal{S} \\times \\Theta\\) combines state and action parameters</p> </li> <li> <p>Experience particles \\((z_i, w_i)\\) are weighted points in augmented space representing experience</p> </li> <li> <p>Kernel functions \\(k(z, z')\\) define similarity and enable smooth generalization</p> </li> <li> <p>The reinforcement field \\(Q^+(z) = \\sum_i w_i k(z, z_i)\\) emerges from particles and kernel</p> </li> <li> <p>Together, these enable continuous action spaces, smooth generalization, and uncertainty quantification</p> </li> </ol>"},{"location":"GRL0/tutorials/01-core-concepts/#next-steps","title":"Next Steps","text":"<p>In Chapter 2: RKHS Foundations, we'll explore:</p> <ul> <li>What is a Reproducing Kernel Hilbert Space (RKHS)?</li> <li>Why the reinforcement field lives in an RKHS</li> <li>The mathematical properties that make this useful</li> <li>Connection to Gaussian Processes</li> </ul> <p>Related: Chapter 0: Overview, Chapter 2: RKHS Foundations</p> <p>Last Updated: January 11, 2026</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/","title":"Chapter 2: RKHS Foundations","text":"<p>Purpose: Understand the mathematical space where GRL lives Prerequisites: Chapter 1 (Core Concepts) Key Concepts: Reproducing Kernel Hilbert Space, inner products, function spaces, GP connection</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#introduction","title":"Introduction","text":"<p>In Chapter 1, we introduced kernel functions as a way to measure similarity between points in augmented space. We saw that the reinforcement field is a weighted sum of kernel evaluations:</p> \\[ Q^+(z) = \\sum_i w_i \\, k(z, z_i) \\] <p>But why does this representation have such nice properties? Why does it generalize smoothly? Why can we take gradients?</p> <p>The answer lies in a beautiful mathematical structure called a Reproducing Kernel Hilbert Space (RKHS). Understanding RKHS is essential because it explains:</p> <ul> <li>Why GRL's value functions are well-behaved</li> <li>How generalization works mathematically</li> <li>What \"functional gradient\" really means</li> <li>Why GRL connects to Gaussian Processes</li> </ul>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#1-what-is-a-hilbert-space","title":"1. What is a Hilbert Space?","text":""},{"location":"GRL0/tutorials/02-rkhs-foundations/#vectors-beyond-arrows","title":"Vectors Beyond Arrows","text":"<p>When you first learned about vectors, you probably thought of arrows in 2D or 3D space. But mathematically, a vector is anything that can be:</p> <ul> <li>Added to other vectors</li> <li>Scaled by numbers</li> <li>Measured for length and angle</li> </ul> <p>Functions can be vectors too! Consider continuous functions on \\([0, 1]\\). We can:</p> <ul> <li>Add functions: \\((f + g)(x) = f(x) + g(x)\\)</li> <li>Scale functions: \\((cf)(x) = c \\cdot f(x)\\)</li> <li>Measure: Using an inner product</li> </ul>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#inner-products","title":"Inner Products","text":"<p>An inner product \\(\\langle \\cdot, \\cdot \\rangle\\) generalizes the dot product. For vectors \\(u, v\\):</p> \\[ \\langle u, v \\rangle \\to \\text{a number measuring \"alignment\"} \\] <p>Properties:</p> <ul> <li>\\(\\langle u, u \\rangle \\geq 0\\) (non-negative)</li> <li>\\(\\langle u, u \\rangle = 0\\) implies \\(u = 0\\) (definite)</li> <li>\\(\\langle u, v \\rangle = \\langle v, u \\rangle\\) (symmetric)</li> <li>Linear in each argument</li> </ul>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#definition-of-hilbert-space","title":"Definition of Hilbert Space","text":"<p>A Hilbert space is a vector space with an inner product that is complete (limits of sequences stay in the space).</p> <p>Examples:</p> <ul> <li>\\(\\mathbb{R}^n\\) with the standard dot product</li> <li>The space of square-integrable functions \\(L^2\\)</li> <li>The space of functions induced by a kernel (RKHS)</li> </ul>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#2-reproducing-kernel-hilbert-spaces","title":"2. Reproducing Kernel Hilbert Spaces","text":""},{"location":"GRL0/tutorials/02-rkhs-foundations/#the-special-property","title":"The Special Property","text":"<p>An RKHS is a Hilbert space of functions with a remarkable property: evaluation is continuous.</p> <p>What does this mean? In a general function space, knowing that two functions are \"close\" (small \\(\\|f - g\\|\\)) doesn't guarantee their values are close at any particular point. In an RKHS, it does.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#the-reproducing-property","title":"The Reproducing Property","text":"<p>An RKHS \\(\\mathcal{H}_k\\) has a kernel \\(k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}\\) such that:</p> <ol> <li>For each \\(x\\), the function \\(k(x, \\cdot)\\) is in \\(\\mathcal{H}_k\\)</li> <li>For any \\(f \\in \\mathcal{H}_k\\): \\(\\langle f, k(x, \\cdot) \\rangle = f(x)\\)</li> </ol> <p>The second property is called reproducing: inner product with \\(k(x, \\cdot)\\) \"reproduces\" the value at \\(x\\).</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#kernel-as-similarity","title":"Kernel as Similarity","text":"<p>From the reproducing property:</p> \\[ \\langle k(x_1, \\cdot), k(x_2, \\cdot) \\rangle_{\\mathcal{H}_k} = k(x_1, x_2) \\] <p>The kernel IS the inner product between feature representations.</p> <p>This is profound: the kernel function directly measures how similar two points are in the feature space induced by the RKHS.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#3-why-rkhs-matters-for-grl","title":"3. Why RKHS Matters for GRL","text":""},{"location":"GRL0/tutorials/02-rkhs-foundations/#functions-as-vectors","title":"Functions as Vectors","text":"<p>In GRL, the value function \\(Q^+\\) is not just \"some function.\" It is a vector in an RKHS:</p> <p>$$</p> <p>Q^+(\\cdot) = \\sum_i w_i \\, k(z_i, \\cdot) \\in \\mathcal{H}_k $$</p> <p>This function is a linear combination of \"basis functions\" \\(k(z_i, \\cdot)\\), exactly like a finite-dimensional vector is a linear combination of basis vectors.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#smoothness","title":"Smoothness","text":"<p>RKHS functions inherit smoothness from the kernel. For example, with an RBF kernel:</p> <p>$$</p> <p>k(z, z') = \\exp\\left(-\\frac{|z - z'|<sup>2}{2\\ell</sup>2}\\right) $$</p> <p>The induced functions are infinitely differentiable. Small changes in input produce small changes in output.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#generalization","title":"Generalization","text":"<p>When we add a new particle \\((z_N, w_N)\\), the updated function:</p> <p>$$</p> <p>Q^+{\\text{new}}(z) = Q^+(z) + w_N k(z, z_N) $$}</p> <p>The influence spreads smoothly according to the kernel. Points similar to \\(z_N\\) (high \\(k(z, z_N)\\)) are affected more; distant points are affected less.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#well-defined-gradients","title":"Well-Defined Gradients","text":"<p>In an RKHS, we can differentiate the value function:</p> <p>$$</p> <p>\\nabla_z Q^+(z) = \\sum_i w_i \\nabla_z k(z, z_i) $$</p> <p>This gradient exists and is smooth\u2014essential for policy improvement.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#4-points-exist-only-through-functions","title":"4. Points Exist Only Through Functions","text":""},{"location":"GRL0/tutorials/02-rkhs-foundations/#a-philosophical-shift","title":"A Philosophical Shift","text":"<p>Classical ML treats data points as primary objects and functions as derived. RKHS inverts this:</p> <p>Functions are primary. Points exist only through how they act on functions.</p> <p>Each point \\(x\\) is represented by the function \\(k(x, \\cdot)\\) \u2014 its \"feature representation.\" Two points are compared via:</p> <p>$$</p> <p>k(x_1, x_2) = \\langle k(x_1, \\cdot), k(x_2, \\cdot) \\rangle $$</p> <p>Points don't have intrinsic coordinates; they have positions in function space.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#epistemic-interpretation","title":"Epistemic Interpretation","text":"<p>In GRL, computing \\(k(z, z')\\) doesn't just mean \"these points are close.\" It means:</p> <p>Evidence gathered at \\(z'\\) is relevant for reasoning about \\(z\\).</p> <p>This is epistemic, not just geometric. The kernel defines what counts as relevant experience.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#5-connection-to-gaussian-processes","title":"5. Connection to Gaussian Processes","text":""},{"location":"GRL0/tutorials/02-rkhs-foundations/#rkhs-is-the-foundation-not-gps","title":"RKHS is the Foundation, Not GPs","text":"<p>Important Distinction:</p> <ul> <li>RKHS is the mathematical framework that makes GRL work</li> <li>Gaussian Processes are ONE tool for building functions in RKHS</li> <li>GRL does not require GPs \u2014 any method that constructs kernel superpositions works</li> </ul> <p>GRL is fundamentally about:</p> <ol> <li>Representing belief states as particles in augmented space</li> <li>Defining value functions as elements of an RKHS</li> <li>Policy inference from functional gradients</li> </ol> <p>GPs happen to be a natural fit because they also live in RKHS, but they are not essential to the framework.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#gps-and-rkhs-share-structure","title":"GPs and RKHS Share Structure","text":"<p>That said, Gaussian Processes are closely related to RKHS. For a GP with covariance function \\(k\\):</p> GP Object RKHS Object Covariance \\(k(x, x')\\) Inner product \\(\\langle k(x, \\cdot), k(x', \\cdot) \\rangle\\) Posterior mean Vector in RKHS Sample paths (May or may not be in RKHS)"},{"location":"GRL0/tutorials/02-rkhs-foundations/#the-posterior-mean-is-always-in-rkhs","title":"The Posterior Mean is Always in RKHS","text":"<p>Given data \\(\\{(x_i, y_i)\\}\\), the GP posterior mean is:</p> \\[ \\mu(x) = \\sum_{i=1}^n \\alpha_i k(x, x_i) \\] <p>This is exactly a finite linear combination of kernel sections \u2014 by definition, an element of the RKHS.</p> <p>For GRL: The value function \\(Q^+\\) can be constructed as a kernel superposition (whether via GP regression, kernel ridge regression, or direct weighted sum), which means it is guaranteed to be in the RKHS. All the nice mathematical properties apply.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#sample-paths-a-subtlety","title":"Sample Paths: A Subtlety","text":"<p>Individual random draws from a GP may or may not belong to the RKHS, depending on the kernel's smoothness:</p> Kernel Sample Paths in RKHS? RBF (Gaussian) Yes Mat\u00e9rn (\\(\\nu \\geq 3/2\\)) Yes Brownian motion No <p>For GRL: We use posterior means (or direct kernel superpositions), not random samples, so this subtlety doesn't affect us.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#6-rkhs-inner-products-and-probability","title":"6. RKHS Inner Products and Probability","text":""},{"location":"GRL0/tutorials/02-rkhs-foundations/#beyond-distance","title":"Beyond Distance","text":"<p>In Euclidean space, similarity is measured by distance. In RKHS, similarity is measured by inner products.</p> <p>The inner product \\(\\langle f, g \\rangle\\) captures:</p> <ul> <li>How \"aligned\" two functions are</li> <li>The degree to which \\(f\\) and \\(g\\) \"agree\"</li> <li>A generalized notion of correlation</li> </ul>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#parallel-to-quantum-mechanics","title":"Parallel to Quantum Mechanics","text":"<p>There's a deep structural parallel to quantum mechanics:</p> Quantum Mechanics GRL with RKHS State vector \\(\\|\\psi\\rangle\\) Kernel feature \\(k(x, \\cdot)\\) Inner product \\(\\langle \\phi \\| \\psi \\rangle\\) Kernel evaluation \\(k(x, x')\\) Probability via \\(\\|\\langle \\phi \\| \\psi \\rangle\\|^2\\) Compatibility via \\(k\\) Observables as operators Value functionals <p>In both frameworks:</p> <ul> <li>Inner products are fundamental</li> <li>Probability/compatibility emerges from overlap</li> <li>The \"state\" of the system is a vector in Hilbert space</li> </ul>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#probability-as-derived-not-primitive","title":"Probability as Derived, Not Primitive","text":"<p>Just as quantum mechanics derives probability from amplitudes:</p> \\[ P(x) = |\\langle \\psi | x \\rangle|^2 \\] <p>GRL derives policy from field values:</p> <p>$$</p> <p>\\pi(a|s) \\propto \\exp(\\beta \\, Q^+(s, a)) $$</p> <p>In both cases, probability is a derived quantity, not a primitive input.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#a-novel-probability-formulation-for-ml","title":"A Novel Probability Formulation for ML","text":"<p>This amplitude-based formulation is not yet mainstream in machine learning:</p> Traditional ML GRL (Quantum-Inspired) Direct probabilities \\(p(x)\\) Amplitudes \\(\\langle \\psi \\| \\phi \\rangle\\) Single-valued distributions Superposition of states Real-valued only Complex-valued RKHS possible No interference Constructive/destructive interference <p>Potential Impact:</p> <ol> <li>Interference effects: Complex-valued RKHS enables new dynamics</li> <li>Phase semantics: Complex phases encode temporal, contextual, or directional information</li> <li>Richer uncertainty: Multi-modal distributions via superposition</li> <li>Novel algorithms: Amplitude-based reasoning opens new learning mechanisms</li> </ol> <p>GRL introduces this formulation to reinforcement learning\u2014potentially opening entirely new directions for probabilistic ML.</p> <p>See Part II (Emergent Structure &amp; Spectral Abstraction) for spectral methods and concept discovery that leverage this framework.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#7-practical-implications","title":"7. Practical Implications","text":""},{"location":"GRL0/tutorials/02-rkhs-foundations/#kernel-choice-matters","title":"Kernel Choice Matters","text":"<p>The kernel defines:</p> <ul> <li>Smoothness: How quickly the value function can change</li> <li>Lengthscale: The \"range of influence\" of each particle</li> <li>Feature relevance: Which dimensions matter (via ARD kernels)</li> </ul> <p>Common choices for GRL:</p> Kernel Properties When to Use RBF Infinitely smooth, isotropic Default choice, smooth domains Mat\u00e9rn Controllable smoothness When less smoothness is appropriate ARD-RBF Learns feature relevance High-dimensional with irrelevant features Composite Separate state/action kernels When state and action have different scales"},{"location":"GRL0/tutorials/02-rkhs-foundations/#computational-considerations","title":"Computational Considerations","text":"<p>RKHS representations have complexity:</p> <ul> <li>Memory: \\(O(N)\\) storage for \\(N\\) particles</li> <li>Query: \\(O(N)\\) kernel evaluations per point</li> <li>Update: \\(O(N)\\) to add/modify particles</li> </ul> <p>For large particle sets, approximations may be needed (inducing points, random features, neural approximations).</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#8-summary-rkhs-as-the-foundation-of-grl","title":"8. Summary: RKHS as the Foundation of GRL","text":""},{"location":"GRL0/tutorials/02-rkhs-foundations/#key-concepts","title":"Key Concepts","text":"Concept Meaning in GRL RKHS The function space where \\(Q^+\\) lives Kernel Defines similarity and smoothness Inner product Measures compatibility and overlap Reproducing property Evaluation is a linear functional RKHS norm Measures complexity/smoothness of functions"},{"location":"GRL0/tutorials/02-rkhs-foundations/#why-this-foundation-matters","title":"Why This Foundation Matters","text":"<ol> <li>Mathematical Rigor: All operations on \\(Q^+\\) are well-defined</li> <li>Guaranteed Smoothness: No pathological functions</li> <li>Principled Generalization: Kernel determines how experience spreads</li> <li>Gradient Existence: Policy improvement is well-posed</li> <li>Connection to GPs: Uncertainty quantification is natural</li> </ol>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#the-core-insight","title":"The Core Insight","text":"<p>GRL replaces pointwise reasoning with Hilbert-space reasoning.</p> <p>Similarity, value, policy, and learning are all geometric consequences of the RKHS inner product structure.</p>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>RKHS is a Hilbert space of functions where evaluation is continuous</li> <li>The kernel defines the inner product: \\(k(x, x') = \\langle k(x, \\cdot), k(x', \\cdot) \\rangle\\)</li> <li>Value functions in GRL are vectors in RKHS: \\(Q^+ = \\sum_i w_i k(z_i, \\cdot)\\)</li> <li>Smoothness and generalization are inherited from the kernel</li> <li>GP posterior means are always in RKHS \u2014 validating GRL's mathematical foundation</li> <li>Inner products measure compatibility, not just distance</li> <li>Points exist through functions: \\(x\\) is represented by \\(k(x, \\cdot)\\)</li> </ol>"},{"location":"GRL0/tutorials/02-rkhs-foundations/#next-steps","title":"Next Steps","text":"<p>In Chapter 3: Energy and Fitness, we'll explore:</p> <ul> <li>The relationship between fitness and energy conventions</li> <li>How to interpret the value landscape</li> <li>Connection to energy-based models</li> <li>Why sign conventions matter</li> </ul> <p>Related: Chapter 1: Core Concepts, Chapter 3: Energy and Fitness</p> <p>Last Updated: January 11, 2026</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/","title":"Chapter 3: Energy and Fitness","text":"<p>Purpose: Understand the value landscape and its two interpretations Prerequisites: Chapter 2 (RKHS Foundations) Key Concepts: Fitness function, energy function, sign conventions, energy-based models</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#introduction","title":"Introduction","text":"<p>In the previous chapters, we've described the reinforcement field as a function \\(Q^+(z)\\) over augmented space. But what does this function represent? How should we interpret its values?</p> <p>The original GRL paper called this a fitness function \u2014 higher values mean better configurations. Modern machine learning often uses energy functions \u2014 lower values mean better configurations.</p> <p>These are two views of the same landscape, like looking at a mountain from above (where peaks are high) versus mapping depth (where valleys are low). Understanding this relationship is essential for:</p> <ul> <li>Connecting GRL to energy-based models</li> <li>Interpreting gradients correctly</li> <li>Extending to probabilistic and diffusion-based methods</li> </ul>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#1-fitness-functions-the-original-grl-view","title":"1. Fitness Functions: The Original GRL View","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#the-intuition","title":"The Intuition","text":"<p>In evolutionary biology and optimization, fitness measures how \"good\" a configuration is. Higher fitness = better adapted = more likely to survive.</p> <p>GRL uses the same intuition:</p> \\[ F(s, \\theta) \\quad \\text{high} \\;\\Rightarrow\\; \\text{good / compatible / desirable} \\] <p>The fitness function answers: \"How desirable is it to use action parameters \\(\\theta\\) in state \\(s\\)?\"</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#properties-of-the-fitness-view","title":"Properties of the Fitness View","text":"<ul> <li>Improvement = ascent: Getting better means going uphill</li> <li>Gradients point toward improvement: \\(\\nabla F\\) indicates the direction of increasing value</li> <li>Policy = maximize fitness: \\(\\theta^* = \\arg\\max_\\theta F(s, \\theta)\\)</li> </ul> <p>This is natural for RL where we maximize expected return.</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#2-energy-functions-the-modern-ml-view","title":"2. Energy Functions: The Modern ML View","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#the-intuition_1","title":"The Intuition","text":"<p>In physics and modern ML, energy measures how much a system \"resists\" a configuration. Low energy = preferred = more likely.</p> <p>$$</p> <p>E(s, \\theta) \\quad \\text{low} \\;\\Rightarrow\\; \\text{good / compatible / likely} $$</p> <p>The energy function answers: \"How much does the system resist this configuration?\"</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#properties-of-the-energy-view","title":"Properties of the Energy View","text":"<ul> <li>Improvement = descent: Getting better means going downhill</li> <li>Negative gradients point toward improvement: \\(-\\nabla E\\) indicates improvement direction</li> <li>Policy = minimize energy: \\(\\theta^* = \\arg\\min_\\theta E(s, \\theta)\\)</li> </ul> <p>This is natural for physics-inspired models where systems seek minimum energy.</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#3-the-mathematical-relationship","title":"3. The Mathematical Relationship","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#the-simple-connection","title":"The Simple Connection","text":"<p>The relationship between fitness and energy is:</p> <p>$$</p> <p>E(s, \\theta) = -F(s, \\theta) $$</p> <p>That's it. Just a sign flip.</p> <p>Optionally, with a temperature parameter:</p> <p>$$</p> <p>E(s, \\theta) = -\\frac{1}{\\tau} F(s, \\theta) $$</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#same-landscape-opposite-conventions","title":"Same Landscape, Opposite Conventions","text":"Aspect Fitness \\(F\\) Energy \\(E = -F\\) Good configurations High \\(F\\) Low \\(E\\) Improvement direction \\(+\\nabla F\\) (ascent) \\(-\\nabla E\\) (descent) Optimal action \\(\\arg\\max F\\) \\(\\arg\\min E\\) \"Hills\" Desirable Barriers \"Valleys\" Undesirable Attractors <p>Critical point: The extrema are in the same locations. A fitness maximum is an energy minimum:</p> <p>$$</p> <p>z^* = \\arg\\max_z F(z) = \\arg\\min_z E(z) $$</p> <p>The geometry is identical; only the labeling changes.</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#4-why-this-distinction-matters","title":"4. Why This Distinction Matters","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#41-gradient-interpretation","title":"4.1 Gradient Interpretation","text":"<p>Getting the sign right is essential for implementation:</p> <p>Fitness (GRL original):</p> <ul> <li>Reinforcement field gradient: \\(\\nabla F(s, \\theta)\\) points toward improvement</li> <li>Policy improvement: Move in the direction of \\(\\nabla F\\)</li> </ul> <p>Energy (modern):</p> <ul> <li>Force field: \\(-\\nabla E(s, \\theta)\\) points toward improvement</li> <li>Policy improvement: Move against \\(\\nabla E\\)</li> </ul> <p>Common bug: Forgetting the sign when switching conventions.</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#42-dynamics-and-flows","title":"4.2 Dynamics and Flows","text":"<p>For continuous-time extensions of GRL, the energy convention is standard.</p> <p>Gradient descent:</p> <p>$$</p> <p>\\frac{d\\theta}{dt} = -\\nabla E(s, \\theta) $$</p> <p>Langevin dynamics (with exploration):</p> <p>$$</p> <p>d\\theta_t = -\\nabla E(s, \\theta_t) \\, dt + \\sqrt{2\\beta^{-1}} \\, dW_t $$</p> <p>Writing these in fitness language works but feels unnatural to practitioners familiar with physics or diffusion models.</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#43-probabilistic-interpretation","title":"4.3 Probabilistic Interpretation","text":"<p>Fitness doesn't naturally define a probability distribution. Energy does:</p> <p>$$</p> <p>p(z) \\propto \\exp(-E(z)) = \\exp(F(z)) $$</p> <p>This Boltzmann distribution immediately enables:</p> <ul> <li>Probabilistic policies</li> <li>Sampling-based exploration</li> <li>Control-as-inference frameworks</li> <li>Connection to entropy-regularized RL</li> </ul>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#5-energy-based-models-and-grl","title":"5. Energy-Based Models and GRL","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#the-connection","title":"The Connection","text":"<p>GRL's reinforcement field is essentially an energy-based model (EBM) over augmented state-action space:</p> <p>$$</p> <p>E(z) = -Q^+(z) = -\\sum_i w_i \\, k(z, z_i) $$</p> <p>Each particle contributes to the energy landscape:</p> <ul> <li>Positive weight \\(w_i &gt; 0\\): Creates an energy well (attractor)</li> <li>Negative weight \\(w_i &lt; 0\\): Creates an energy barrier (repeller)</li> <li>Kernel bandwidth: Controls spatial extent of influence</li> </ul>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#ebm-properties-in-grl","title":"EBM Properties in GRL","text":"EBM Concept GRL Realization Energy function Negative reinforcement field \\(-Q^+\\) Low-energy regions High-value state-action pairs Energy minimization Policy optimization Boltzmann sampling Softmax action selection Energy gradient Reinforcement field gradient"},{"location":"GRL0/tutorials/03-energy-and-fitness/#the-boltzmann-policy","title":"The Boltzmann Policy","text":"<p>GRL's softmax policy over action values is exactly Boltzmann sampling from the energy:</p> <p>$$</p> <p>\\pi(\\theta \\mid s) \\propto \\exp\\big(-\\beta E(s, \\theta)\\big) = \\exp\\big(\\beta Q^+(s, \\theta)\\big) $$</p> <p>where \\(\\beta\\) is inverse temperature:</p> <ul> <li>\\(\\beta \\to 0\\): Uniform random actions (infinite temperature)</li> <li>\\(\\beta \\to \\infty\\): Greedy selection of energy minimum (zero temperature)</li> <li>Finite \\(\\beta\\): Stochastic exploration biased toward low energy</li> </ul>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#6-visual-intuition","title":"6. Visual Intuition","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#the-landscape-metaphor","title":"The Landscape Metaphor","text":"<p>Imagine the augmented space as a terrain:</p> <p>Fitness view (original GRL):</p> <ul> <li>Peaks = good actions (high fitness)</li> <li>Valleys = bad actions (low fitness)</li> <li>Policy = climb toward peaks</li> <li>Gradient = uphill direction</li> </ul> <p>Energy view (modern):</p> <ul> <li>Valleys = good actions (low energy)</li> <li>Peaks = bad actions (high energy)</li> <li>Policy = descend toward valleys</li> <li>Gradient = steepest uphill (so move opposite)</li> </ul>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#particles-shape-the-landscape","title":"Particles Shape the Landscape","text":"<p>Each experience particle \\((z_i, w_i)\\) contributes a \"bump\" to the landscape:</p> <ul> <li>Positive \\(w_i\\) in fitness view: A hill centered at \\(z_i\\)</li> <li>Positive \\(w_i\\) in energy view: A valley (well) centered at \\(z_i\\)</li> </ul> <p>The full landscape is the superposition of all particle contributions, smoothed by the kernel.</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#7-practical-recommendations","title":"7. Practical Recommendations","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#for-grl-v0-reimplementation","title":"For GRL-v0 Reimplementation","text":"<ol> <li>Preserve original conventions when replicating the paper</li> <li>Match gradient signs exactly to avoid bugs</li> <li>Test: Verify that policy improvement increases \\(Q^+\\)</li> </ol>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#for-modern-extensions","title":"For Modern Extensions","text":"<p>Switch to energy language when:</p> <ul> <li>Using diffusion-based methods</li> <li>Connecting to physics-based control</li> <li>Interfacing with EBM literature</li> <li>Implementing Langevin dynamics</li> </ul>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#convention-table","title":"Convention Table","text":"Task Use Fitness Use Energy Reimplementing GRL-v0 \u2713 Reading original paper \u2713 Diffusion policy \u2713 Score matching \u2713 Control-as-inference \u2713 Connecting to EBMs \u2713"},{"location":"GRL0/tutorials/03-energy-and-fitness/#8-the-energy-landscape-in-grl","title":"8. The Energy Landscape in GRL","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#multi-modal-landscapes","title":"Multi-Modal Landscapes","text":"<p>GRL's particle-based representation naturally handles multi-modal landscapes:</p> <ul> <li>Multiple particles can create multiple wells</li> <li>Each well corresponds to a viable policy mode</li> <li>No mode collapse (unlike some parametric methods)</li> <li>Exploration can discover multiple solutions</li> </ul>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#energy-wells-and-basins","title":"Energy Wells and Basins","text":"<p>An energy well is a local minimum of \\(E(z)\\) \u2014 a region where actions are good. The basin of attraction is the set of points that flow toward that well under gradient descent.</p> <p>GRL's particle memory implicitly defines these basins through kernel overlap. Similar particles reinforce each other, creating deeper wells.</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#no-explicit-parameterization","title":"No Explicit Parameterization","text":"<p>A key feature of GRL: the energy landscape is not explicitly parameterized. It emerges from:</p> <ul> <li>Particle positions \\(z_i\\)</li> <li>Particle weights \\(w_i\\)</li> <li>Kernel function \\(k\\)</li> </ul> <p>This is fundamentally different from neural network-based EBMs where \\(E_\\theta(z)\\) is a parameterized function.</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#9-score-functions-and-gradients","title":"9. Score Functions and Gradients","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#the-score-function","title":"The Score Function","text":"<p>In modern generative models, the score function is the gradient of log-probability:</p> \\[ \\nabla_z \\log p(z) = -\\nabla_z E(z) \\quad \\text{(for Boltzmann } p \\propto e^{-E}\\text{)} \\] <p>For GRL's reinforcement field:</p> <p>$$</p> <p>\\nabla_z \\log \\pi(z) \\propto \\nabla_z Q^+(z) = -\\nabla_z E(z) $$</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#connection-to-diffusion-models","title":"Connection to Diffusion Models","text":"<p>Diffusion models learn to reverse a noising process using the score function. GRL's reinforcement field gradient serves an analogous role: it indicates the direction toward high-value regions.</p> <p>This connection opens paths to:</p> <ul> <li>Diffusion-based policy learning</li> <li>Score matching for value functions</li> <li>Denoising approaches to action selection</li> </ul>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#10-summary","title":"10. Summary","text":""},{"location":"GRL0/tutorials/03-energy-and-fitness/#the-core-relationship","title":"The Core Relationship","text":"<p>$$</p> <p>E(s, \\theta) = -F(s, \\theta) $$</p> <p>Fitness and energy are two views of the same landscape:</p> <ul> <li>Fitness: High = good (evolutionary/RL language)</li> <li>Energy: Low = good (physics/EBM language)</li> </ul>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#why-it-matters","title":"Why It Matters","text":"Aspect Impact Gradients Sign determines improvement direction Dynamics Energy form is standard for physics-based methods Probability \\(p \\propto e^{-E}\\) is immediate Modern ML EBM, diffusion, score matching use energy"},{"location":"GRL0/tutorials/03-energy-and-fitness/#the-core-innovation-remains","title":"The Core Innovation Remains","text":"<p>GRL's contribution was never about sign conventions. It was the idea that:</p> <p>Reinforcement is a field over augmented state-action space, not a lookup table.</p> <p>Whether called fitness or energy, this field-based perspective is the foundation of GRL.</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Fitness (high = good) and energy (low = good) are equivalent via \\(E = -F\\)</li> <li>Same geometry, opposite labeling conventions</li> <li>Gradients flip sign: \\(\\nabla F\\) vs. \\(-\\nabla E\\) both point toward improvement</li> <li>Energy enables probability: \\(p \\propto \\exp(-E)\\) gives Boltzmann distribution</li> <li>GRL is an EBM over augmented space with particle-based energy</li> <li>Boltzmann policy = softmax over \\(Q^+\\) = sampling from energy</li> <li>Choose convention based on context: original GRL uses fitness, modern methods use energy</li> </ol>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#further-reading-why-energy-the-physics-connection","title":"Further Reading: Why Energy? The Physics Connection","text":"<p>The energy formulation is not arbitrary!</p> <p>The energy function \\(E(z) = -Q^+(z)\\) connects to one of the most fundamental principles in physics: the principle of least action. This principle states that systems evolve along trajectories that minimize an \"action\" functional, which naturally leads to energy-based formulations and Boltzmann distributions.</p> <p>\ud83d\udcd6 See Supplement: Chapter 03a - The Principle of Least Action</p> <p>This supplement explores:</p> <ul> <li>Classical mechanics and the action principle</li> <li>Path integral control theory</li> <li>Why GRL's Boltzmann policy emerges from action minimization</li> <li>How agents can discover smooth, optimal actions (not just select from fixed sets)</li> <li>Connection to neural network policy optimization</li> </ul> <p>For the quantum mechanical perspective: - Quantum-Inspired Chapter 09 - Path Integrals and Action Principles</p>"},{"location":"GRL0/tutorials/03-energy-and-fitness/#next-steps","title":"Next Steps","text":"<p>In Chapter 4: The Reinforcement Field, we'll explore:</p> <ul> <li>What exactly is a \"functional field\"?</li> <li>Why the reinforcement field is NOT a vector field</li> <li>How policy emerges from field geometry</li> <li>The continuous-time interpretation</li> </ul> <p>Related: Chapter 2: RKHS Foundations, Chapter 4: Reinforcement Field, Supplement 03a: Least Action Principle</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/","title":"Chapter 03a: The Principle of Least Action (Supplement)","text":"<p>Purpose: This supplement bridges classical physics and reinforcement learning, showing why GRL's energy-based formulation is not just convenient notation\u2014it's a principled framework grounded in one of the most fundamental laws of physics: the principle of least action.</p> <p>Why this matters for GRL:</p> <ul> <li>Explains why the Boltzmann policy \\(\\pi(\\theta|s) \\propto \\exp(Q^+/\\lambda)\\) emerges naturally</li> <li>Provides a principled way for agents to discover smooth, optimal actions (not just select from pre-defined sets)</li> <li>Connects modern RL to 300+ years of physics and optimal control theory</li> </ul>"},{"location":"GRL0/tutorials/03a-least-action-principle/#1-classical-mechanics-a-crash-course","title":"1. Classical Mechanics: A Crash Course","text":""},{"location":"GRL0/tutorials/03a-least-action-principle/#11-the-action-functional","title":"1.1 The Action Functional","text":"<p>In classical mechanics, a particle doesn't \"choose\" its trajectory arbitrarily. Among all possible paths from point A to point B, nature selects the one that minimizes a quantity called the action.</p> <p>The action functional \\(S[\\gamma]\\) assigns a real number to each possible trajectory \\(\\gamma(t)\\):</p> \\[S[\\gamma] = \\int_{t_0}^{t_f} L(q(t), \\dot{q}(t), t) \\, dt\\] <p>where:</p> <ul> <li>\\(L(q, \\dot{q}, t)\\) = Lagrangian = Kinetic Energy - Potential Energy</li> <li>\\(q(t)\\) = position at time \\(t\\)</li> <li>\\(\\dot{q}(t)\\) = velocity at time \\(t\\)</li> </ul> <p>Principle of Least Action: The actual trajectory taken by the system is the one that makes \\(S[\\gamma]\\) stationary (usually a minimum).</p> <p>Example: Free particle</p> <p>For a particle of mass \\(m\\) moving freely:</p> \\[L = \\frac{1}{2}m\\dot{q}^2\\] <p>The action is:</p> \\[S[\\gamma] = \\int_{t_0}^{t_f} \\frac{1}{2}m\\dot{q}^2 \\, dt\\] <p>Result: The trajectory that minimizes action is a straight line at constant velocity\u2014Newton's first law emerges from a variational principle!</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#12-the-euler-lagrange-equations","title":"1.2 The Euler-Lagrange Equations","text":"<p>Minimizing the action leads to the Euler-Lagrange equations:</p> \\[\\frac{d}{dt}\\left(\\frac{\\partial L}{\\partial \\dot{q}}\\right) - \\frac{\\partial L}{\\partial q} = 0\\] <p>These are equivalent to Newton's equations of motion.</p> <p>Example: Particle in potential</p> <p>For \\(L = \\frac{1}{2}m\\dot{q}^2 - V(q)\\):</p> \\[m\\ddot{q} = -\\frac{\\partial V}{\\partial q}\\] <p>This is Newton's second law: \\(F = ma\\).</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#13-why-is-this-powerful","title":"1.3 Why Is This Powerful?","text":"<p>The action principle is powerful because:</p> <ol> <li>Unified framework: Works for all physical systems (mechanics, optics, quantum mechanics, field theory)</li> <li>Coordinate-free: The action is independent of coordinate choices</li> <li>Reveals conservation laws: Via Noether's theorem (symmetries \u2194 conserved quantities)</li> <li>Generalizes naturally: Extends to stochastic systems, optimal control, and RL</li> </ol>"},{"location":"GRL0/tutorials/03a-least-action-principle/#2-from-physics-to-control-path-integral-control","title":"2. From Physics to Control: Path Integral Control","text":""},{"location":"GRL0/tutorials/03a-least-action-principle/#21-the-control-problem","title":"2.1 The Control Problem","text":"<p>In optimal control, we want to find a trajectory that:</p> <ul> <li>Starts at state \\(s_0\\)</li> <li>Ends at goal state \\(s_f\\) (or maximizes reward)</li> <li>Minimizes a cost functional</li> </ul> <p>Sound familiar? This is exactly an action minimization problem!</p> <p>Control action functional:</p> \\[S[\\tau] = \\int_{t_0}^{t_f} \\left[ C(s_t, u_t) + \\frac{1}{2\\nu} \\|u_t\\|^2 \\right] dt\\] <p>where:</p> <ul> <li>\\(C(s, u)\\) = instantaneous cost (like potential energy)</li> <li>\\(\\frac{1}{2\\nu} \\|u_t\\|^2\\) = control cost (like kinetic energy)</li> <li>\\(u_t\\) = control input at time \\(t\\)</li> <li>\\(\\nu\\) = \"temperature\" parameter (exploration vs. exploitation)</li> </ul> <p>The control Lagrangian is:</p> \\[L(s, u) = -C(s, u) - \\frac{1}{2\\nu} \\|u\\|^2\\] <p>(Note the minus signs: we minimize cost, which is like maximizing negative cost)</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#22-optimal-policy-from-action","title":"2.2 Optimal Policy from Action","text":"<p>Key insight (Kappen 2005, Todorov 2009): The optimal stochastic policy is:</p> \\[\\pi^*(u|s) \\propto \\exp\\left(-\\frac{1}{\\nu} S[s \\to u]\\right)\\] <p>where \\(S[s \\to u]\\) is the action along the optimal trajectory from \\(s\\) when applying control \\(u\\).</p> <p>This is a Boltzmann distribution over actions!</p> <p>The policy naturally emerges from minimizing action, with temperature \\(\\nu\\) controlling the sharpness:</p> <ul> <li>High \\(\\nu\\) (high temperature) \u2192 More exploration, softer policy</li> <li>Low \\(\\nu\\) (low temperature) \u2192 More exploitation, sharper policy</li> </ul>"},{"location":"GRL0/tutorials/03a-least-action-principle/#23-the-cost-to-go-as-potential","title":"2.3 The Cost-to-Go as \"Potential\"","text":"<p>In path integral control, the cost-to-go (or value function) plays the role of potential energy:</p> \\[V(s) = \\min_{\\tau} \\mathbb{E}\\left[\\int_{t}^{\\infty} C(s_\\tau, u_\\tau) d\\tau \\mid s_t = s\\right]\\] <p>The optimal control is:</p> \\[u^*(s) = -\\nu \\nabla_s \\log \\mathcal{Z}(s)\\] <p>where \\(\\mathcal{Z}(s)\\) is the \"partition function\" (sum over all trajectories).</p> <p>This is gradient descent on an energy landscape!</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#3-grls-boltzmann-policy-as-least-action","title":"3. GRL's Boltzmann Policy as Least Action","text":""},{"location":"GRL0/tutorials/03a-least-action-principle/#31-the-grl-action-functional","title":"3.1 The GRL Action Functional","text":"<p>In GRL, we work in augmented state-action space \\(z = (s, \\theta)\\). The natural action functional is:</p> \\[S[\\tau] = \\int_{t_0}^{t_f} \\left[ E(s_t, \\theta_t) + \\frac{1}{2\\lambda} \\|\\dot{\\theta}_t\\|^2 \\right] dt\\] <p>where:</p> <ul> <li>\\(E(s, \\theta) = -Q^+(s, \\theta)\\) = energy landscape (potential)</li> <li>\\(\\|\\dot{\\theta}_t\\|^2\\) = \"kinetic energy\" of action parameter changes</li> <li>\\(\\lambda\\) = temperature (controls exploration)</li> </ul> <p>The GRL Lagrangian:</p> \\[L(s, \\theta, \\dot{\\theta}) = -E(s, \\theta) - \\frac{1}{2\\lambda} \\|\\dot{\\theta}\\|^2 = Q^+(s, \\theta) - \\frac{1}{2\\lambda} \\|\\dot{\\theta}\\|^2\\] <p>This says: Good trajectories have high \\(Q^+\\) (high reward potential) and smooth changes in action parameters (low kinetic cost).</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#32-why-the-boltzmann-policy-emerges","title":"3.2 Why the Boltzmann Policy Emerges","text":"<p>From path integral control theory, the optimal policy is:</p> \\[\\pi^*(\\theta|s) \\propto \\exp\\left(-\\frac{1}{\\lambda} S[s \\to \\theta]\\right)\\] <p>For a single-step decision (no trajectory dynamics), this simplifies to:</p> \\[\\pi^*(\\theta|s) \\propto \\exp\\left(-\\frac{1}{\\lambda} E(s, \\theta)\\right) = \\exp\\left(\\frac{Q^+(s, \\theta)}{\\lambda}\\right)\\] <p>This is exactly GRL's Boltzmann policy!</p> <p>It's not an ad-hoc choice\u2014it's the optimal policy under the action minimization principle.</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#33-smooth-actions-from-kinetic-regularization","title":"3.3 Smooth Actions from Kinetic Regularization","text":"<p>The kinetic term \\(\\frac{1}{2\\lambda}\\|\\dot{\\theta}\\|^2\\) penalizes rapid changes in action parameters.</p> <p>Why this matters:</p> <ul> <li>Prevents jerky, discontinuous actions</li> <li>Encourages smooth, physically realizable trajectories</li> <li>Natural regularization (Occam's razor for actions)</li> </ul> <p>Example: Robotic reaching</p> <p>Without kinetic penalty: Agent might command wild, discontinuous joint torques With kinetic penalty: Agent learns smooth, human-like reaching motions</p> <p>In GRL: The kernel function \\(k(z, z')\\) implicitly encodes this smoothness preference!</p> \\[k((s, \\theta), (s', \\theta')) = k_s(s, s') \\cdot k_\\theta(\\theta, \\theta')\\] <p>A smooth kernel (e.g., RBF) ensures that nearby actions have similar \\(Q^+\\) values, effectively implementing the kinetic penalty.</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#4-implications-for-action-discovery","title":"4. Implications for Action Discovery","text":""},{"location":"GRL0/tutorials/03a-least-action-principle/#41-beyond-fixed-action-sets","title":"4.1 Beyond Fixed Action Sets","text":"<p>Traditional RL assumes a fixed action space:</p> <ul> <li>Discrete: \\(\\mathcal{A} = \\{a_1, a_2, \\ldots, a_n\\}\\)</li> <li>Continuous: \\(\\mathcal{A} = \\mathbb{R}^d\\) with pre-defined parameterization</li> </ul> <p>GRL with least action: Actions are discovered by minimizing the action functional.</p> <p>The agent learns:</p> <ol> <li>What actions are smooth (low kinetic cost)</li> <li>What actions are effective (high \\(Q^+\\), low energy)</li> <li>How to balance exploration and exploitation (\\(\\lambda\\) temperature)</li> </ol> <p>No pre-defined action repertoire needed!</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#42-gradient-flow-on-the-energy-landscape","title":"4.2 Gradient Flow on the Energy Landscape","text":"<p>From the Euler-Lagrange equations, the optimal trajectory satisfies:</p> \\[\\lambda \\ddot{\\theta}_t = -\\nabla_\\theta E(s_t, \\theta_t) + \\sqrt{2\\lambda} \\, \\xi_t\\] <p>where \\(\\xi_t\\) is Brownian noise (from stochasticity).</p> <p>In the overdamped limit (high friction), this becomes:</p> \\[\\dot{\\theta}_t = -\\nabla_\\theta E(s_t, \\theta_t) + \\sqrt{2\\lambda} \\, \\xi_t = \\nabla_\\theta Q^+(s_t, \\theta_t) + \\sqrt{2\\lambda} \\, \\xi_t\\] <p>This is Langevin dynamics!</p> <ul> <li>Deterministic part: Follow the gradient of \\(Q^+\\) uphill (toward high-value actions)</li> <li>Stochastic part: Explore via temperature-controlled noise</li> <li>Result: Agent naturally discovers smooth, high-value action trajectories</li> </ul>"},{"location":"GRL0/tutorials/03a-least-action-principle/#43-neural-network-policies-as-action-minimizers","title":"4.3 Neural Network Policies as Action Minimizers","text":"<p>When implementing GRL with a neural network \\(Q_\\phi(s, \\theta)\\):</p> <p>Policy optimization becomes:</p> \\[\\theta_t \\sim \\pi_\\phi(\\theta|s_t) = \\frac{\\exp(Q_\\phi(s_t, \\theta)/\\lambda)}{\\int \\exp(Q_\\phi(s_t, \\theta')/\\lambda) d\\theta'}\\] <p>Sampling via gradient flow:</p> <ol> <li>Initialize \\(\\theta_0\\) randomly or from heuristic</li> <li>Update: \\(\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta Q_\\phi(s_t, \\theta_t) + \\sqrt{2\\alpha\\lambda} \\, \\epsilon_t\\)</li> <li>Repeat until convergence</li> </ol> <p>This is Langevin Monte Carlo sampling from the Boltzmann distribution!</p> <p>The agent doesn't need a pre-defined action set\u2014it samples actions from the energy landscape shaped by learning.</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#5-principled-policy-optimization","title":"5. Principled Policy Optimization","text":""},{"location":"GRL0/tutorials/03a-least-action-principle/#51-the-energy-based-learning-objective","title":"5.1 The Energy-Based Learning Objective","text":"<p>Given the least action principle, the natural learning objective is:</p> <p>Minimize expected action over trajectories:</p> \\[J(\\phi) = \\mathbb{E}_{\\tau \\sim \\pi_\\phi}\\left[\\int_0^T \\left[E(s_t, \\theta_t) + \\frac{1}{2\\lambda}\\|\\dot{\\theta}_t\\|^2\\right] dt\\right]\\] <p>subject to environment dynamics \\(s_{t+1} = f(s_t, \\theta_t, w_t)\\).</p> <p>In practice (episodic RL):</p> \\[J(\\phi) = \\mathbb{E}_{\\tau \\sim \\pi_\\phi}\\left[\\sum_{t=0}^T \\left[-r_t + \\frac{1}{2\\lambda}\\|\\theta_{t+1} - \\theta_t\\|^2\\right]\\right]\\] <p>This naturally balances:</p> <ul> <li>Reward maximization: via \\(-r_t\\) (minimize negative reward)</li> <li>Action smoothness: via kinetic penalty</li> </ul>"},{"location":"GRL0/tutorials/03a-least-action-principle/#52-natural-gradient-on-the-policy-manifold","title":"5.2 Natural Gradient on the Policy Manifold","text":"<p>The least action principle also suggests using the natural gradient (Amari 1998):</p> \\[\\nabla_\\phi^{\\text{nat}} J = F^{-1} \\nabla_\\phi J\\] <p>where \\(F\\) is the Fisher information matrix (the Riemannian metric on the policy space).</p> <p>Why this is \"natural\": It measures policy distance in terms of KL divergence, not Euclidean distance in parameter space.</p> <p>Connection to action: The Fisher metric is the infinitesimal version of the action metric on the policy manifold.</p> <p>Practical algorithms:</p> <ul> <li>TRPO (Trust Region Policy Optimization)</li> <li>PPO (Proximal Policy Optimization)</li> <li>Natural Actor-Critic</li> </ul> <p>All implicitly minimize action-like functionals!</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#53-smoothness-as-inductive-bias","title":"5.3 Smoothness as Inductive Bias","text":"<p>The kinetic term \\(\\frac{1}{2\\lambda}\\|\\dot{\\theta}\\|^2\\) is an inductive bias favoring smooth policies.</p> <p>Why this helps learning:</p> <ul> <li>Reduces sample complexity (smooth functions generalize better)</li> <li>Improves stability (prevents policy collapse)</li> <li>Encodes physical priors (real systems have inertia)</li> </ul> <p>In GRL: This is naturally encoded by:</p> <ol> <li>Kernel smoothness: RBF kernels enforce continuity</li> <li>Particle memory: Weighted neighbors smooth the \\(Q^+\\) estimate</li> <li>MemoryUpdate propagation: \\(\\lambda_{\\text{prop}}\\) controls local smoothing</li> </ol>"},{"location":"GRL0/tutorials/03a-least-action-principle/#6-connection-to-grls-core-ideas","title":"6. Connection to GRL's Core Ideas","text":""},{"location":"GRL0/tutorials/03a-least-action-principle/#61-energy-function-chapter-03","title":"6.1 Energy Function (Chapter 03)","text":"<p>The energy \\(E(z) = -Q^+(z)\\) is the potential in the action functional:</p> \\[S[\\tau] = \\int \\left[E(z_t) + \\frac{1}{2\\lambda}\\|\\dot{z}_t\\|^2\\right] dt\\] <p>Why call it energy? - Consistent with physics (potential energy landscape) - Optimal trajectories minimize total energy + kinetic cost - Connects to statistical mechanics (Boltzmann distribution)</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#62-reinforcement-field-chapter-04","title":"6.2 Reinforcement Field (Chapter 04)","text":"<p>The reinforcement field \\(Q^+: \\mathcal{Z} \\to \\mathbb{R}\\) defines the potential energy landscape.</p> <p>From least action perspective:</p> <ul> <li>High \\(Q^+\\) regions: Low potential energy, attractors</li> <li>Low \\(Q^+\\) regions: High potential energy, repellers</li> <li>Gradient \\(\\nabla Q^+\\): Force field guiding action selection</li> </ul> <p>The field emerges from particles (Chapter 05), which act like \"mass distributions\" creating the energy landscape.</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#63-memoryupdate-chapter-06","title":"6.3 MemoryUpdate (Chapter 06)","text":"<p>MemoryUpdate modifies the particle ensemble, which reshapes the energy landscape.</p> <p>From least action perspective:</p> <ul> <li>Adding particle \\((z_{\\text{new}}, w_{\\text{new}})\\): Creates a potential well at \\(z_{\\text{new}}\\)</li> <li>Propagating weights: Smooths the landscape (kinetic regularization)</li> <li>Hard threshold \\(\\epsilon\\): Limits influence radius (finite-range potential)</li> </ul> <p>The updated field \\(Q^+_{\\text{new}}\\) guides future action selection via gradient flow.</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#64-rf-sarsa-chapter-07-coming-next","title":"6.4 RF-SARSA (Chapter 07, coming next)","text":"<p>RF-SARSA implements temporal difference learning on the energy landscape.</p> <p>From least action perspective:</p> <ul> <li>TD error: Mismatch between predicted and actual action along trajectory</li> <li>Weight update: Adjusts potential to make future trajectories optimal</li> <li>Exploration (\\(\\lambda\\)): Temperature for Langevin sampling</li> </ul> <p>The algorithm is performing stochastic gradient descent on the expected action over trajectories!</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#7-practical-implementation-notes","title":"7. Practical Implementation Notes","text":""},{"location":"GRL0/tutorials/03a-least-action-principle/#71-choosing-the-temperature-lambda","title":"7.1 Choosing the Temperature \\(\\lambda\\)","text":"<p>The temperature \\(\\lambda\\) controls exploration:</p> <p>High \\(\\lambda\\) (hot):</p> <ul> <li>Broad distribution over actions</li> <li>More exploration</li> <li>Good early in learning</li> </ul> <p>Low \\(\\lambda\\) (cold):</p> <ul> <li>Peaked distribution (near-greedy)</li> <li>More exploitation</li> <li>Good after convergence</li> </ul> <p>Typical schedule: Exponential decay \\(\\lambda_t = \\lambda_0 \\cdot \\alpha^t\\) with \\(\\alpha \\approx 0.99\\).</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#72-implementing-gradient-flow","title":"7.2 Implementing Gradient Flow","text":"<p>For continuous action parameters \\(\\theta \\in \\mathbb{R}^d\\):</p> <pre><code>def sample_action_langevin(Q_field, s, theta_init, lambda_temp, n_steps=10, step_size=0.01):\n    \"\"\"Sample action via Langevin dynamics on Q+ landscape.\"\"\"\n    theta = theta_init.clone()\n\n    for _ in range(n_steps):\n        # Compute gradient of Q+ w.r.t. theta\n        grad_Q = Q_field.gradient(s, theta)  # \u2207_\u03b8 Q+(s, \u03b8)\n\n        # Langevin update\n        theta = theta + step_size * grad_Q + np.sqrt(2 * step_size * lambda_temp) * np.random.randn(*theta.shape)\n\n    return theta\n</code></pre> <p>Practical note: For high-dimensional \\(\\theta\\), use Metropolis-adjusted Langevin (MALA) for better convergence.</p>"},{"location":"GRL0/tutorials/03a-least-action-principle/#73-kinetic-regularization-in-loss","title":"7.3 Kinetic Regularization in Loss","text":"<p>When training a neural network \\(Q_\\phi(s, \\theta)\\):</p> <pre><code>def compute_loss(Q_phi, trajectories, lambda_temp, lambda_kinetic):\n    \"\"\"Compute action-based loss.\"\"\"\n    loss = 0.0\n\n    for tau in trajectories:\n        for t in range(len(tau) - 1):\n            s_t, theta_t, r_t = tau[t]\n            s_tp1, theta_tp1, r_tp1 = tau[t + 1]\n\n            # Energy term: negative reward\n            loss += -r_t\n\n            # Kinetic term: smoothness penalty\n            loss += (1 / (2 * lambda_kinetic)) * torch.norm(theta_tp1 - theta_t)**2\n\n            # TD error (coming in Chapter 07)\n            Q_current = Q_phi(s_t, theta_t)\n            Q_next = Q_phi(s_tp1, theta_tp1)\n            td_error = r_t + gamma * Q_next - Q_current\n            loss += td_error**2\n\n    return loss / len(trajectories)\n</code></pre>"},{"location":"GRL0/tutorials/03a-least-action-principle/#8-summary-why-least-action-matters-for-grl","title":"8. Summary: Why Least Action Matters for GRL","text":"<p>Physics justification:</p> <ul> <li>Energy-based formulation is not arbitrary\u2014it's grounded in fundamental physics</li> <li>Boltzmann policy emerges naturally from action minimization</li> <li>Smooth trajectories are optimal, not just convenient</li> </ul> <p>Algorithmic benefits:</p> <ul> <li>Principled exploration via temperature \\(\\lambda\\)</li> <li>Natural regularization via kinetic penalty</li> <li>Gradient-based action discovery (no fixed action sets needed)</li> </ul> <p>Theoretical depth:</p> <ul> <li>Connects RL to 300+ years of physics and optimal control</li> <li>Provides a unified framework (discrete, continuous, hybrid actions)</li> <li>Opens path to advanced techniques (natural gradients, Riemannian optimization)</li> </ul> <p>Next steps:</p> <ul> <li>Chapter 07: RF-SARSA \u2014 How to learn \\(Q^+\\) via temporal differences</li> <li>Quantum-Inspired Chapter 09 \u2014 Path integrals and Feynman's formulation</li> </ul>"},{"location":"GRL0/tutorials/03a-least-action-principle/#further-reading","title":"Further Reading","text":"<p>Path Integral Control:</p> <ul> <li>Kappen, H. J. (2005). \"Path integrals and symmetry breaking for optimal control theory.\" Journal of Statistical Mechanics.</li> <li>Todorov, E. (2009). \"Efficient computation of optimal actions.\" PNAS.</li> <li>Theodorou, E., Buchli, J., &amp; Schaal, S. (2010). \"A generalized path integral control approach to reinforcement learning.\" JMLR.</li> </ul> <p>Variational Principles:</p> <ul> <li>Goldstein, H., Poole, C., &amp; Safko, J. (2002). Classical Mechanics (3<sup>rd</sup> ed.), Chapter 2.</li> <li>Landau, L. D., &amp; Lifshitz, E. M. (1976). Mechanics (3<sup>rd</sup> ed.), Chapter 2.</li> </ul> <p>Natural Gradients &amp; Policy Optimization:</p> <ul> <li>Amari, S. (1998). \"Natural gradient works efficiently in learning.\" Neural Computation.</li> <li>Schulman, J., et al. (2015). \"Trust region policy optimization.\" ICML.</li> </ul> <p>\u2190 Back to Chapter 03: Energy and Fitness | Next: Chapter 04 \u2192</p> <p>Related: Quantum-Inspired Chapter 09 - Path Integrals</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/","title":"Chapter 4: The Reinforcement Field","text":"<p>Purpose: Understand GRL's central object \u2014 the reinforcement field Prerequisites: Chapter 3 (Energy and Fitness) Key Concepts: Functional field, RKHS gradient, policy as geometry, continuous-time dynamics</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#introduction","title":"Introduction","text":"<p>We've built up to this point:</p> <ul> <li>Chapter 1: Actions as parameters, augmented space, particles, kernels</li> <li>Chapter 2: RKHS as the mathematical home for our value function</li> <li>Chapter 3: Fitness and energy as two views of the same landscape</li> </ul> <p>Now we can finally define GRL's central object: the reinforcement field.</p> <p>This chapter addresses a critical conceptual point that the original paper left implicit: the reinforcement field is not a classical vector field. It is a functional field defined through RKHS geometry. Understanding this distinction is essential for correctly implementing and extending GRL.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#1-what-the-original-paper-said","title":"1. What the Original Paper Said","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#the-original-definition","title":"The Original Definition","text":"<p>The original GRL paper defined:</p> <p>\"A reinforcement field is a vector field in Hilbert space established by one or more kernels through their linear combination as a representation for the fitness function, where each of the kernel centers around a particular augmented state vector.\"</p> <p>This definition is correct but potentially misleading. The phrase \"vector field\" might suggest arrows in Euclidean space. That's not what GRL means.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#whats-right-about-it","title":"What's Right About It","text":"<p>The definition correctly identifies:</p> <ul> <li>The field is induced by kernels</li> <li>It is constructed from linear combinations</li> <li>It lives in a Hilbert space</li> <li>The kernel centers are experience particles</li> </ul>"},{"location":"GRL0/tutorials/04-reinforcement-field/#what-needs-clarification","title":"What Needs Clarification","text":"<p>The word \"vector\" is ambiguous. It could mean:</p> <ol> <li>A geometric arrow in \\(\\mathbb{R}^n\\) (classical vector field)</li> <li>An element of a Hilbert space (a function)</li> </ol> <p>GRL uses meaning (2). The \"vectors\" in the reinforcement field are functions, not arrows.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#2-two-meanings-of-vector","title":"2. Two Meanings of \"Vector\"","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#meaning-a-geometric-vectors","title":"Meaning A: Geometric Vectors","text":"<p>In physics and calculus, a vector field assigns an arrow to each point:</p> \\[ \\mathbf{v}: \\mathbb{R}^n \\to \\mathbb{R}^n \\] <p>At each point \\(x\\), there's a vector \\(\\mathbf{v}(x)\\) with magnitude and direction. Think of wind velocity at each point in space.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#meaning-b-hilbert-space-vectors","title":"Meaning B: Hilbert Space Vectors","text":"<p>In functional analysis, a vector is any element of a vector space. Functions are vectors:</p> <p>$$</p> <p>f \\in \\mathcal{H}_k $$</p> <p>\"Vector\" means an object that can be added and scaled, with an inner product structure.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#grl-uses-meaning-b","title":"GRL Uses Meaning B","text":"<p>In GRL, when we say \"vector field,\" we mean:</p> <p>A field that assigns Hilbert space elements (functions) to points, not Euclidean arrows.</p> <p>This is a fundamental distinction.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#3-the-functional-field","title":"3. The Functional Field","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#definition","title":"Definition","text":"<p>A functional field is:</p> <p>A field whose values are induced by derivatives of functionals in an RKHS, not by coordinate-wise vector components.</p> <p>More precisely:</p> <ul> <li>The value function \\(Q^+\\) lives in RKHS \\(\\mathcal{H}_k\\)</li> <li>Its gradient \\(\\nabla Q^+\\) is defined via the RKHS inner product</li> <li>This gradient is the Riesz representer of a functional derivative</li> </ul> <p>(See Chapter 4a: Riesz Representer for a detailed explanation of what this means and why it matters.)</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#the-strengthened-definition","title":"The Strengthened Definition","text":"<p>Definition (Reinforcement Field). A reinforcement field is a vector field whose vectors are elements of a reproducing kernel Hilbert space. Specifically, it is the functional gradient of a scalar value/energy functional over augmented state-action space. The field is induced by the RKHS inner product and constructed from experience particles through kernel superposition.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#key-clarification","title":"Key Clarification","text":"<p>\"Vector\" refers to an element of a Hilbert space (a function), not a geometric arrow in Euclidean space.</p> <p>With this clarification, the original definition becomes precise and powerful.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#4-gradient-structure-in-rkhs","title":"4. Gradient Structure in RKHS","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#value-function-representation","title":"Value Function Representation","text":"<p>The value function is represented as:</p> \\[ Q^+(z) = \\sum_{i=1}^N w_i \\, k(z, z_i) \\] <p>where \\(z_i\\) are particle locations and \\(w_i\\) are weights.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#functional-gradient","title":"Functional Gradient","text":"<p>The gradient of \\(Q^+\\) at any point \\(z\\) is:</p> <p>$$</p> <p>\\nabla_z Q^+(z) = \\sum_{i=1}^N w_i \\, \\nabla_z k(z, z_i) $$</p> <p>This gradient is:</p> <ul> <li>A superposition of kernel gradients centered at particles</li> <li>Globally smooth (inherited from kernel smoothness)</li> <li>Nonlocal (every particle contributes, weighted by kernel)</li> <li>Data-adaptive (shaped by accumulated experience)</li> </ul>"},{"location":"GRL0/tutorials/04-reinforcement-field/#why-this-is-different","title":"Why This Is Different","text":"<p>In classical vector calculus, \\(\\nabla f(x)\\) is computed locally using partial derivatives. The result depends only on \\(f\\)'s behavior near \\(x\\).</p> <p>In RKHS, \\(\\nabla Q^+(z)\\) depends on all particles, weighted by their kernel distance from \\(z\\). It's a global, nonlocal object.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#5-the-field-in-action-space","title":"5. The Field in Action Space","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#policy-learning-equations","title":"Policy Learning Equations","text":"<p>The original paper presents two key equations for policy learning:</p> <p>Discrete update (Equation 12): $$</p> <p>\\theta_{t+1} = \\theta_t + \\eta \\, \\frac{\\partial Q^+(s_t, \\theta)}{\\partial \\theta} \\Big|_{\\theta=\\theta_t} $$</p> <p>This updates action parameters by moving toward higher value.</p> <p>Continuous-time flow (Equation 13): $$</p> <p>\\frac{d\\theta}{dt} = \\nabla_\\theta Q^+(s(t), \\theta(t)) $$</p> <p>Policy learning becomes flow in the reinforcement field.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#correct-interpretation","title":"Correct Interpretation","text":"<p>These equations should be read as:</p> <p>The policy evolves along the steepest ascent direction of a functional defined in RKHS, evaluated at the current augmented state.</p> <p>The trajectory is not \"gradient ascent in \\(\\mathbb{R}^d\\)\" \u2014 it is gradient flow in the geometry induced by the kernel.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#6-why-functional-field-is-the-right-name","title":"6. Why \"Functional Field\" Is the Right Name","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#three-properties-hidden-by-vector-field","title":"Three Properties Hidden by \"Vector Field\"","text":"<p>Calling it a functional field emphasizes:</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#61-defined-in-function-space-not-coordinate-space","title":"6.1 Defined in Function Space, Not Coordinate Space","text":"<p>The geometry comes from the kernel, not Euclidean coordinates. Two configurations are \"close\" if the kernel says so, regardless of coordinate distance.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#62-directions-are-induced-not-specified","title":"6.2 Directions Are Induced, Not Specified","text":"<p>The field direction emerges from the RKHS inner product structure. It's computed from functional geometry, not assigned arbitrarily.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#63-generalizes-to-infinite-dimensions","title":"6.3 Generalizes to Infinite Dimensions","text":"<p>States, actions, or operators can live in function spaces, manifolds, or operator spaces \u2014 without changing the definition. A classical vector field wouldn't survive this generalization.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#7-policy-as-geometry","title":"7. Policy as Geometry","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#the-central-insight","title":"The Central Insight","text":"<p>The policy is not a function to be learned. It is a trajectory induced by the reinforcement field geometry.</p> <p>In standard RL, we learn a policy \\(\\pi_\\phi(a|s)\\) by optimizing parameters \\(\\phi\\).</p> <p>In GRL, there is no separate policy function. Instead:</p> <ul> <li>We learn the reinforcement field \\(Q^+\\)</li> <li>The policy emerges from navigating this field</li> <li>Action selection is geometric: move toward high-value regions</li> </ul>"},{"location":"GRL0/tutorials/04-reinforcement-field/#implications","title":"Implications","text":"Standard RL GRL Learn \\(\\pi_\\phi(a\\|s)\\) Learn \\(Q^+(s, \\theta)\\) Policy is explicit Policy is implicit Optimize policy parameters Shape value landscape Sample from distribution Navigate geometry"},{"location":"GRL0/tutorials/04-reinforcement-field/#8-energy-view-of-the-field","title":"8. Energy View of the Field","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#conversion-to-energy","title":"Conversion to Energy","text":"<p>In energy terms (Chapter 3), the reinforcement field becomes a force field:</p> \\[ E(z) = -Q^+(z) \\] <p>$$</p> <p>\\mathbf{F}(z) = -\\nabla E(z) = \\nabla Q^+(z) $$</p> <p>The force points toward low energy (high value).</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#langevin-dynamics","title":"Langevin Dynamics","text":"<p>Adding stochasticity for exploration:</p> <p>$$</p> <p>d\\theta_t = -\\nabla_\\theta E(s_t, \\theta_t) \\, dt + \\sqrt{2\\beta^{-1}} \\, dW_t $$</p> <p>This is Langevin dynamics in the augmented space \u2014 a principled way to add exploration while respecting the energy landscape.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#connection-to-diffusion-models","title":"Connection to Diffusion Models","text":"<p>The reinforcement field gradient is analogous to the score function in diffusion models:</p> <p>$$</p> <p>\\nabla \\log p(z) \\propto -\\nabla E(z) = \\nabla Q^+(z) $$</p> <p>This opens connections to:</p> <ul> <li>Diffusion-based policy learning</li> <li>Score matching for value functions</li> <li>Denoising approaches to control</li> </ul>"},{"location":"GRL0/tutorials/04-reinforcement-field/#9-what-the-field-encodes","title":"9. What the Field Encodes","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#three-in-one","title":"Three-in-One","text":"<p>The reinforcement field simultaneously encodes:</p> Aspect Encoded By Value estimation The scalar function \\(Q^+(z)\\) Generalization Kernel-mediated similarity Policy structure Geometry (gradients, level sets) <p>These are not separate objects \u2014 they're aspects of one unified field.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#field-quantities","title":"Field Quantities","text":"Quantity Meaning \\(Q^+(z)\\) Value at configuration \\(z\\) \\(\\nabla Q^+(z)\\) Direction of improvement \\(\\|\\nabla Q^+(z)\\|\\) Rate of improvement Level set \\(\\{z: Q^+(z) = c\\}\\) Iso-value surface Local maximum of \\(Q^+\\) Optimal action (for given state)"},{"location":"GRL0/tutorials/04-reinforcement-field/#10-example-2d-navigation","title":"10. Example: 2D Navigation","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#setup","title":"Setup","text":"<p>Consider a simple domain:</p> <ul> <li>State \\(s = (x, y) \\in \\mathbb{R}^2\\): position</li> <li>Action \\(\\theta = (F_x, F_y) \\in \\mathbb{R}^2\\): force to apply</li> <li>Augmented state \\(z = (x, y, F_x, F_y) \\in \\mathbb{R}^4\\)</li> </ul>"},{"location":"GRL0/tutorials/04-reinforcement-field/#particles-from-experience","title":"Particles from Experience","text":"<p>After exploration, suppose we have particles:</p> <ul> <li>\\((z_1, w_1)\\): Near goal, high weight (good experience)</li> <li>\\((z_2, w_2)\\): Hit obstacle, low/negative weight (bad experience)</li> <li>\\((z_3, w_3)\\): Random wandering, moderate weight</li> </ul>"},{"location":"GRL0/tutorials/04-reinforcement-field/#the-field","title":"The Field","text":"<p>The reinforcement field:</p> \\[ Q^+(z) = w_1 k(z, z_1) + w_2 k(z, z_2) + w_3 k(z, z_3) \\] <p>creates a landscape over 4D augmented space.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#policy","title":"Policy","text":"<p>At a new state \\(s\\), the policy queries: \"For which \\(\\theta\\) is \\(Q^+(s, \\theta)\\) highest?\"</p> <p>The answer comes from the field geometry:</p> <ul> <li>Near the goal: \\(\\theta\\) similar to past successful actions</li> <li>Near obstacles: Avoid \\(\\theta\\) similar to past failures</li> <li>Unknown regions: Low confidence, uncertainty-driven exploration</li> </ul>"},{"location":"GRL0/tutorials/04-reinforcement-field/#11-comparison-to-classical-concepts","title":"11. Comparison to Classical Concepts","text":"Classical Concept GRL Reinforcement Field Vector field in \\(\\mathbb{R}^n\\) Functional field in RKHS Pointwise gradient Riesz representer of functional derivative Euclidean distance Kernel-induced similarity Policy as function \\(\\pi: S \\to A\\) Policy as trajectory/flow in field Potential function Value functional \\(Q^+ \\in \\mathcal{H}_k\\)"},{"location":"GRL0/tutorials/04-reinforcement-field/#12-summary","title":"12. Summary","text":""},{"location":"GRL0/tutorials/04-reinforcement-field/#the-core-insight","title":"The Core Insight","text":"<p>Reinforcement in GRL is not a scalar signal, but a geometry.</p> <p>Policies are not functions to be learned, but trajectories induced by that geometry.</p> <p>This transforms RL from a function-fitting problem into a geometric control problem in function space.</p>"},{"location":"GRL0/tutorials/04-reinforcement-field/#definition-recap","title":"Definition Recap","text":"<p>The reinforcement field is:</p> <ol> <li>The functional gradient of value/energy in RKHS</li> <li>A superposition of kernel gradients at particles</li> <li>A nonlocal, smooth field over augmented space</li> <li>The substrate on which policy emerges as flow</li> </ol>"},{"location":"GRL0/tutorials/04-reinforcement-field/#why-it-matters","title":"Why It Matters","text":"<ul> <li>Provides a unified object for value, generalization, and policy</li> <li>Naturally extends to operators, infinite dimensions, stochastic dynamics</li> <li>Connects to EBMs, diffusion models, optimal control</li> <li>Geometrizes reinforcement learning</li> </ul>"},{"location":"GRL0/tutorials/04-reinforcement-field/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>The reinforcement field is a functional field, not a classical vector field</li> <li>\"Vector\" means Hilbert-space element (function), not geometric arrow</li> <li>The field is the functional gradient of \\(Q^+\\) in RKHS</li> <li>Gradients are superpositions of kernel gradients at all particles</li> <li>Policy = geometry: Trajectories emerge from navigating the field</li> <li>The energy view gives force field for dynamics</li> <li>This perspective connects GRL to diffusion models and optimal control</li> </ol>"},{"location":"GRL0/tutorials/04-reinforcement-field/#next-steps","title":"Next Steps","text":"<p>In Chapter 5: Particle Memory, we'll explore:</p> <ul> <li>How particles represent experience</li> <li>The MemoryUpdate algorithm</li> <li>Particle operations: add, merge, prune</li> <li>Managing memory over time</li> </ul> <p>Related: Chapter 3: Energy and Fitness, Chapter 5: Particle Memory</p> <p>Last Updated: January 11, 2026</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/","title":"Chapter 4a: The Riesz Representer \u2014 Gradients in Function Space","text":"<p>Supplement to Chapter 4: Reinforcement Field</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#why-this-matters","title":"Why This Matters","text":"<p>In Chapter 4, we said:</p> <p>\"The gradient \\(\\nabla Q^+\\) is the Riesz representer of a functional derivative.\"</p> <p>But what does that mean? And why is it important for GRL?</p> <p>This chapter unpacks the Riesz representation theorem\u2014the mathematical machinery that lets us talk about \"gradients\" in infinite-dimensional function spaces.</p> <p>Prerequisites: Chapter 2 (RKHS Foundations)</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-problem-what-is-a-gradient-in-function-space","title":"The Problem: What Is a \"Gradient\" in Function Space?","text":""},{"location":"GRL0/tutorials/04a-riesz-representer/#in-finite-dimensions-familiar-territory","title":"In Finite Dimensions (Familiar Territory)","text":"<p>In normal calculus, if you have a scalar function:</p> \\[f(x) = x_1^2 + 2x_2\\] <p>The gradient is a vector:</p> \\[\\nabla f = \\begin{bmatrix} 2x_1 \\\\ 2 \\end{bmatrix}\\] <p>Interpretation: The gradient tells you the direction of steepest increase at each point.</p> <p>How we use it: Inner product with a direction vector \\(v\\):</p> \\[\\langle \\nabla f, v \\rangle = 2x_1 v_1 + 2 v_2\\] <p>This gives the directional derivative of \\(f\\) in direction \\(v\\).</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#in-infinite-dimensions-function-space","title":"In Infinite Dimensions (Function Space)","text":"<p>Now suppose you have a functional\u2014a function that takes functions as input and returns a scalar:</p> \\[L[f] = \\int_0^1 f(x)^2 \\, dx\\] <p>Example: If \\(f(x) = x\\), then:</p> \\[L[f] = \\int_0^1 x^2 \\, dx = \\frac{1}{3}\\] <p>Question: What is the \"gradient\" of \\(L\\) at \\(f\\)?</p> <p>Problem: There's no finite-dimensional vector space here. Functions live in an infinite-dimensional space. What does \\(\\nabla L\\) even mean?</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-idea-represent-the-derivative-as-a-function","title":"The Idea: Represent the Derivative as a Function","text":"<p>The key insight from functional analysis:</p> <p>Instead of computing a gradient vector, find the unique function \\(g\\) that represents the derivative via inner product.</p> <p>Specifically, we want to find \\(g\\) such that for any \"direction\" (test function) \\(h\\):</p> \\[\\frac{d}{d\\epsilon} L[f + \\epsilon h] \\bigg|_{\\epsilon=0} = \\langle g, h \\rangle\\] <p>This \\(g\\) is called the Riesz representer of the derivative.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-riesz-representation-theorem","title":"The Riesz Representation Theorem","text":""},{"location":"GRL0/tutorials/04a-riesz-representer/#statement-informal","title":"Statement (Informal)","text":"<p>In a Hilbert space, every continuous linear functional can be represented as an inner product with a unique element of that space.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#statement-formal","title":"Statement (Formal)","text":"<p>Let \\(\\mathcal{H}\\) be a Hilbert space, and let \\(\\phi: \\mathcal{H} \\to \\mathbb{R}\\) be a continuous linear functional. Then there exists a unique element \\(g \\in \\mathcal{H}\\) such that:</p> \\[\\phi(f) = \\langle g, f \\rangle_{\\mathcal{H}} \\quad \\text{for all } f \\in \\mathcal{H}\\] <p>We call \\(g\\) the Riesz representer of \\(\\phi\\).</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#why-this-is-profound","title":"Why This Is Profound","text":"<p>This theorem says:</p> <ol> <li>Functionals are functions: Any linear operation on functions can be represented by a specific function</li> <li>Derivatives live in the same space: The derivative of a functional is itself an element of the Hilbert space</li> <li>Inner products encode everything: All you need is the inner product structure</li> </ol>"},{"location":"GRL0/tutorials/04a-riesz-representer/#example-1-point-evaluation-functional","title":"Example 1: Point Evaluation Functional","text":"<p>Let's start simple.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-functional","title":"The Functional","text":"<p>In an RKHS \\(\\mathcal{H}_k\\) over domain \\(\\mathcal{X}\\), define:</p> \\[\\phi_x(f) = f(x)\\] <p>This functional evaluates \\(f\\) at point \\(x\\).</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-riesz-representer","title":"The Riesz Representer","text":"<p>By the Riesz representation theorem, there exists \\(g_x \\in \\mathcal{H}_k\\) such that:</p> \\[f(x) = \\langle g_x, f \\rangle_{\\mathcal{H}_k}\\] <p>Question: What is \\(g_x\\)?</p> <p>Answer: It's the kernel section \\(k(x, \\cdot)\\)!</p> <p>By the reproducing property:</p> \\[f(x) = \\langle f, k(x, \\cdot) \\rangle_{\\mathcal{H}_k}\\] <p>Interpretation: The function \\(k(x, \\cdot)\\) represents the operation \"evaluate at \\(x\\).\"</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#example-2-derivative-of-a-functional","title":"Example 2: Derivative of a Functional","text":"<p>Let's compute an actual derivative.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-functional_1","title":"The Functional","text":"<p>Consider:</p> \\[L[f] = \\int_0^1 f(x)^2 \\, dx\\] <p>We want the derivative at \\(f_0(x) = x\\).</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#computing-the-directional-derivative","title":"Computing the Directional Derivative","text":"<p>For any test function \\(h\\):</p> \\[\\frac{d}{d\\epsilon} L[f_0 + \\epsilon h] \\bigg|_{\\epsilon=0} = \\frac{d}{d\\epsilon} \\int_0^1 (f_0 + \\epsilon h)^2 \\, dx \\bigg|_{\\epsilon=0}\\] \\[= \\int_0^1 2 f_0(x) h(x) \\, dx = 2 \\int_0^1 x \\cdot h(x) \\, dx\\]"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-riesz-representer_1","title":"The Riesz Representer","text":"<p>We need \\(g\\) such that:</p> \\[\\int_0^1 x \\cdot h(x) \\, dx = \\langle g, h \\rangle_{L^2}\\] <p>In \\(L^2[0,1]\\) with the standard inner product:</p> \\[\\langle g, h \\rangle = \\int_0^1 g(x) h(x) \\, dx\\] <p>Answer: \\(g(x) = 2x\\)</p> <p>Interpretation: The \"gradient\" of \\(L\\) at \\(f_0\\) is the function \\(g(x) = 2f_0(x) = 2x\\).</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#example-3-gradient-of-the-grl-value-functional","title":"Example 3: Gradient of the GRL Value Functional","text":"<p>Now let's connect to GRL.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-setup","title":"The Setup","text":"<p>In GRL, the value function is:</p> \\[Q^+(z) = \\sum_i w_i k(z_i, z)\\] <p>where \\(Q^+ \\in \\mathcal{H}_k\\) (the RKHS induced by kernel \\(k\\)).</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-functional_2","title":"The Functional","text":"<p>Consider the functional that evaluates \\(Q^+\\) at a query point \\(z_0\\):</p> \\[\\phi_{z_0}(Q^+) = Q^+(z_0)\\]"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-directional-derivative","title":"The Directional Derivative","text":"<p>For any \"direction\" \\(h \\in \\mathcal{H}_k\\):</p> \\[\\frac{d}{d\\epsilon} (Q^+ + \\epsilon h)(z_0) \\bigg|_{\\epsilon=0} = h(z_0)\\] <p>By the reproducing property:</p> \\[h(z_0) = \\langle h, k(z_0, \\cdot) \\rangle_{\\mathcal{H}_k}\\]"},{"location":"GRL0/tutorials/04a-riesz-representer/#the-riesz-representer_2","title":"The Riesz Representer","text":"<p>The gradient of the functional \\(\\phi_{z_0}\\) is:</p> \\[\\nabla \\phi_{z_0} = k(z_0, \\cdot)\\] <p>Interpretation: The \"direction\" of steepest increase for the value function at \\(z_0\\) is the kernel section \\(k(z_0, \\cdot)\\).</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#why-this-matters-for-grl","title":"Why This Matters for GRL","text":""},{"location":"GRL0/tutorials/04a-riesz-representer/#1-gradients-are-functions-not-vectors","title":"1. Gradients Are Functions, Not Vectors","text":"<p>In GRL, when we talk about the \"gradient\" \\(\\nabla Q^+\\), we mean:</p> <p>The unique function in \\(\\mathcal{H}_k\\) that represents how \\(Q^+\\) changes in response to perturbations.</p> <p>This gradient is not a finite-dimensional vector\u2014it's an element of the infinite-dimensional RKHS.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#2-the-reinforcement-field-is-a-gradient-field","title":"2. The Reinforcement Field Is a Gradient Field","text":"<p>The reinforcement field is defined as:</p> \\[\\mathbf{F}(z) = \\nabla_z Q^+(z)\\] <p>Using the Riesz representation, this gradient is:</p> \\[\\nabla_z Q^+(z) = \\sum_i w_i \\nabla_z k(z_i, z)\\] <p>Each \\(\\nabla_z k(z_i, z)\\) is itself a Riesz representer\u2014the function that represents how \\(k(z_i, \\cdot)\\) changes at \\(z\\).</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#3-inner-products-compute-directional-derivatives","title":"3. Inner Products Compute Directional Derivatives","text":"<p>When we compute:</p> \\[\\langle \\nabla Q^+, h \\rangle\\] <p>We're computing the directional derivative of \\(Q^+\\) in direction \\(h\\).</p> <p>This is how the agent \"probes\" the value landscape to decide which direction (action) to take.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#4-policy-inference-via-gradients","title":"4. Policy Inference via Gradients","text":"<p>In GRL, policy inference involves finding the direction in augmented space that maximizes value:</p> \\[\\theta^* = \\arg\\max_\\theta Q^+(s, \\theta)\\] <p>This is equivalent to following the gradient (Riesz representer) of \\(Q^+\\) in the action parameter space.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#notation-summary","title":"Notation Summary","text":"<p>Let's clarify all the notation we've used:</p> Symbol Meaning \\(\\mathcal{H}\\) A Hilbert space (e.g., RKHS) \\(\\phi: \\mathcal{H} \\to \\mathbb{R}\\) A linear functional (maps functions to scalars) \\(g \\in \\mathcal{H}\\) The Riesz representer of \\(\\phi\\) \\(\\langle g, f \\rangle\\) Inner product in \\(\\mathcal{H}\\) \\(L[f]\\) A functional that takes \\(f\\) and returns a scalar \\(\\nabla L\\) The Riesz representer of the derivative of \\(L\\) \\(k(x, \\cdot)\\) Kernel section (function of the second argument) \\(Q^+ \\in \\mathcal{H}_k\\) The reinforcement field (value function in RKHS) \\(\\nabla Q^+\\) The Riesz representer of the value functional derivative"},{"location":"GRL0/tutorials/04a-riesz-representer/#visual-intuition","title":"Visual Intuition","text":""},{"location":"GRL0/tutorials/04a-riesz-representer/#finite-dimensions","title":"Finite Dimensions","text":"<p>In \\(\\mathbb{R}^2\\):</p> <pre><code>Gradient:    \u2207f = [2x\u2081, 2]  \u2190 a vector\n\nDirection:   v = [v\u2081, v\u2082]   \u2190 another vector\n\nDirectional derivative: \u27e8\u2207f, v\u27e9 = 2x\u2081v\u2081 + 2v\u2082\n</code></pre>"},{"location":"GRL0/tutorials/04a-riesz-representer/#infinite-dimensions-rkhs","title":"Infinite Dimensions (RKHS)","text":"<p>In \\(\\mathcal{H}_k\\):</p> <pre><code>Gradient:    \u2207Q\u207a = \u03a3\u1d62 w\u1d62 \u2207k(z\u1d62, \u00b7)  \u2190 a function\n\nDirection:   h \u2208 \u210b\u2096                \u2190 another function\n\nDirectional derivative: \u27e8\u2207Q\u207a, h\u27e9 = \u222b \u2207Q\u207a(z) h(z) d\u03bc(z)\n</code></pre> <p>Key insight: Same structure, different space!</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#example-4-computing-a-concrete-gradient","title":"Example 4: Computing a Concrete Gradient","text":"<p>Let's compute an explicit example with a Gaussian kernel.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#setup","title":"Setup","text":"<p>Gaussian RBF kernel:</p> \\[k(z, z') = \\exp\\left(-\\frac{\\|z - z'\\|^2}{2\\sigma^2}\\right)\\] <p>Value function at a single particle:</p> \\[Q^+(z) = w_1 k(z_1, z) = w_1 \\exp\\left(-\\frac{\\|z - z_1\\|^2}{2\\sigma^2}\\right)\\]"},{"location":"GRL0/tutorials/04a-riesz-representer/#computing-the-gradient","title":"Computing the Gradient","text":"<p>The gradient with respect to \\(z\\) is:</p> \\[\\nabla_z Q^+(z) = w_1 \\nabla_z \\exp\\left(-\\frac{\\|z - z_1\\|^2}{2\\sigma^2}\\right)\\] <p>Using the chain rule:</p> \\[\\nabla_z \\exp\\left(-\\frac{\\|z - z_1\\|^2}{2\\sigma^2}\\right) = \\exp\\left(-\\frac{\\|z - z_1\\|^2}{2\\sigma^2}\\right) \\cdot \\left(-\\frac{z - z_1}{\\sigma^2}\\right)\\] <p>So:</p> \\[\\nabla_z Q^+(z) = -\\frac{w_1}{\\sigma^2} (z - z_1) \\exp\\left(-\\frac{\\|z - z_1\\|^2}{2\\sigma^2}\\right)\\]"},{"location":"GRL0/tutorials/04a-riesz-representer/#interpretation","title":"Interpretation","text":"<ul> <li>Magnitude: Largest near \\(z_1\\), decays with distance</li> <li>Direction: Points from \\(z_1\\) toward \\(z\\) (if \\(w_1 &gt; 0\\), repulsive gradient)</li> <li>Sign: If \\(w_1 &gt; 0\\), gradient points away from particle; if \\(w_1 &lt; 0\\), toward particle</li> </ul> <p>For GRL policy inference: The agent follows the gradient to move toward high-value regions (positive particles attract, negative particles repel).</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#practical-implications-for-grl","title":"Practical Implications for GRL","text":""},{"location":"GRL0/tutorials/04a-riesz-representer/#1-gradients-are-computable","title":"1. Gradients Are Computable","text":"<p>Because RKHS gradients have Riesz representers, we can compute them explicitly:</p> \\[\\nabla Q^+(z) = \\sum_i w_i \\nabla_z k(z_i, z)\\] <p>No need for finite differences or backpropagation\u2014the gradient is analytical.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#2-gradients-guide-policy","title":"2. Gradients Guide Policy","text":"<p>The policy at state \\(s\\) can be computed by finding:</p> \\[\\theta^* = \\arg\\max_\\theta Q^+(s, \\theta)\\] <p>This is equivalent to following the gradient in parameter space:</p> \\[\\theta^* = \\theta_0 + \\alpha \\nabla_\\theta Q^+(s, \\theta_0)\\]"},{"location":"GRL0/tutorials/04a-riesz-representer/#3-gradients-enable-continuous-optimization","title":"3. Gradients Enable Continuous Optimization","text":"<p>Because gradients exist and are smooth (for smooth kernels), we can use gradient-based optimization for action selection\u2014even though the \"action space\" is infinite-dimensional!</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#4-functional-derivatives-are-well-defined","title":"4. Functional Derivatives Are Well-Defined","text":"<p>The Riesz representation theorem guarantees that all the functional derivatives we use in GRL (value derivatives, policy gradients, energy gradients) are well-defined elements of the RKHS.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#common-misconceptions","title":"Common Misconceptions","text":""},{"location":"GRL0/tutorials/04a-riesz-representer/#misconception-1-the-gradient-is-a-vector","title":"Misconception 1: \"The Gradient Is a Vector\"","text":"<p>Reality: In RKHS, the gradient is a function\u2014an element of the same Hilbert space.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#misconception-2-rkhs-gradients-are-approximations","title":"Misconception 2: \"RKHS Gradients Are Approximations\"","text":"<p>Reality: RKHS gradients are exact\u2014they are the Riesz representers defined by the inner product structure.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#misconception-3-we-need-to-discretize-to-compute-gradients","title":"Misconception 3: \"We Need to Discretize to Compute Gradients\"","text":"<p>Reality: For analytic kernels (RBF, Mat\u00e9rn, polynomial), gradients have closed-form expressions.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#misconception-4-functional-derivatives-are-esoteric","title":"Misconception 4: \"Functional Derivatives Are Esoteric\"","text":"<p>Reality: They're just the infinite-dimensional version of ordinary derivatives, made rigorous by the Riesz representation theorem.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#summary","title":"Summary","text":"Finite Dimensions Infinite Dimensions (RKHS) Gradient is a vector \\(\\nabla f \\in \\mathbb{R}^n\\) Gradient is a function \\(\\nabla L \\in \\mathcal{H}\\) Inner product: \\(\\langle \\nabla f, v \\rangle\\) Inner product: \\(\\langle \\nabla L, h \\rangle_{\\mathcal{H}}\\) Directional derivative via dot product Directional derivative via RKHS inner product Gradient descent in \\(\\mathbb{R}^n\\) Gradient descent in \\(\\mathcal{H}\\) <p>The Riesz representation theorem says: These are the same structure, just in different spaces.</p>"},{"location":"GRL0/tutorials/04a-riesz-representer/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Riesz Representer = Gradient in Function Space</li> <li>Every linear functional has a unique function that represents it via inner product</li> <li> <p>This function is the \"gradient\"</p> </li> <li> <p>RKHS Makes Gradients Tractable</p> </li> <li>Reproducing property: \\(f(x) = \\langle f, k(x, \\cdot) \\rangle\\)</li> <li> <p>Kernel sections \\(k(x, \\cdot)\\) are Riesz representers of point evaluations</p> </li> <li> <p>GRL's Reinforcement Field Is a Gradient Field</p> </li> <li>\\(\\nabla Q^+\\) is the Riesz representer of value function changes</li> <li> <p>Policy inference follows this gradient to maximize value</p> </li> <li> <p>Notation:</p> </li> <li>\\(\\nabla Q^+\\): The gradient (a function in RKHS)</li> <li>\\(\\langle \\nabla Q^+, h \\rangle\\): Directional derivative in direction \\(h\\)</li> <li>\\(\\nabla_z k(z_i, z)\\): Gradient of kernel (computable analytically)</li> </ol>"},{"location":"GRL0/tutorials/04a-riesz-representer/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/tutorials/04a-riesz-representer/#within-this-tutorial","title":"Within This Tutorial","text":"<ul> <li>Chapter 2: RKHS Foundations \u2014 Inner products and reproducing property</li> <li>Chapter 4: Reinforcement Field \u2014 The gradient field in GRL</li> <li>Chapter 5: Particle Memory \u2014 How particles induce the value function</li> </ul>"},{"location":"GRL0/tutorials/04a-riesz-representer/#advanced-topics","title":"Advanced Topics","text":"<ul> <li>Quantum-Inspired: Wavefunction Interpretation \u2014 State vectors vs. coordinate representations</li> </ul>"},{"location":"GRL0/tutorials/04a-riesz-representer/#mathematical-references","title":"Mathematical References","text":"<ul> <li>Riesz Representation Theorem:</li> <li>Rudin, W. (1991). Functional Analysis. McGraw-Hill. (Chapter 6)</li> <li> <p>Reed &amp; Simon (1980). Functional Analysis. Academic Press.</p> </li> <li> <p>RKHS and Reproducing Property:</p> </li> <li>Berlinet &amp; Thomas-Agnan (2004). Reproducing Kernel Hilbert Spaces in Probability and Statistics. Springer.</li> <li> <p>Steinwart &amp; Christmann (2008). Support Vector Machines. Springer.</p> </li> <li> <p>Calculus of Variations:</p> </li> <li>Gelfand &amp; Fomin (1963). Calculus of Variations. Dover.</li> </ul>"},{"location":"GRL0/tutorials/04a-riesz-representer/#next-steps","title":"Next Steps","text":"<p>In Chapter 6: MemoryUpdate Algorithm, we'll see how the Riesz representer structure enables efficient updates to the reinforcement field as new particles are added to memory.</p> <p>Spoiler: Because gradients have explicit representations, we can compute value function updates analytically!</p> <p>Last Updated: January 12, 2026</p>"},{"location":"GRL0/tutorials/05-particle-memory/","title":"Chapter 5: Particle Memory","text":"<p>Purpose: Understand how GRL represents and stores experience Prerequisites: Chapter 4 (Reinforcement Field) Key Concepts: Particles as functional basis, memory as belief state, kernel association, energy landscape</p>"},{"location":"GRL0/tutorials/05-particle-memory/#introduction","title":"Introduction","text":"<p>In Chapters 1-4, we've built up the mathematical foundation:</p> <ul> <li>Augmented space \\((s, \\theta)\\)</li> <li>RKHS as the function space</li> <li>Energy/fitness landscapes</li> <li>The reinforcement field</li> </ul> <p>Now we address a fundamental question: How does GRL store and use experience?</p> <p>Traditional RL uses replay buffers \u2014 collections of transitions \\((s, a, r, s')\\) that are sampled for training. GRL takes a radically different approach: particle memory.</p> <p>This chapter explains:</p> <ul> <li>What particles represent mathematically</li> <li>How particles create the reinforcement field</li> <li>Why this is more than a replay buffer</li> <li>The deep connection to belief states</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#1-what-is-a-particle","title":"1. What is a Particle?","text":""},{"location":"GRL0/tutorials/05-particle-memory/#the-basic-definition","title":"The Basic Definition","text":"<p>A particle is a weighted point in augmented state-action space:</p> \\[ \\omega = (z, w) = ((s, \\theta), w) \\] <p>where:</p> <ul> <li>\\(z = (s, \\theta)\\): Location in augmented space</li> <li>\\(w \\in \\mathbb{R}\\): Weight (influence on the field)</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#the-collection","title":"The Collection","text":"<p>The agent maintains a particle memory:</p> <p>$$</p> <p>\\Omega = {(z_1, w_1), (z_2, w_2), \\ldots, (z_N, w_N)} $$</p> <p>This ensemble of weighted particles is the agent's complete experience representation.</p>"},{"location":"GRL0/tutorials/05-particle-memory/#2-particles-vs-replay-buffer","title":"2. Particles vs. Replay Buffer","text":""},{"location":"GRL0/tutorials/05-particle-memory/#surface-similarity","title":"Surface Similarity","text":"<p>Both store information about past experiences. But the similarities end there.</p>"},{"location":"GRL0/tutorials/05-particle-memory/#deep-differences","title":"Deep Differences","text":"Replay Buffer Particle Memory Stores transitions \\((s, a, r, s')\\) Stores weighted points \\((z, w)\\) Used for sampling mini-batches Used for function representation Purpose: re-experience past data Purpose: construct energy landscape Size: Fixed capacity, FIFO or priority Size: Dynamic, with merging/pruning Query: Random sample Query: Kernel-weighted evaluation Semantics: \"What happened?\" Semantics: \"What do I believe?\" <p>Key insight: Replay buffer is a database. Particle memory is a belief state.</p>"},{"location":"GRL0/tutorials/05-particle-memory/#3-particles-as-basis-functions","title":"3. Particles as Basis Functions","text":""},{"location":"GRL0/tutorials/05-particle-memory/#the-mathematical-role","title":"The Mathematical Role","text":"<p>Recall the reinforcement field from Chapter 4:</p> \\[ Q^+(z) = \\sum_{i=1}^N w_i \\, k(z, z_i) \\] <p>Each particle \\((z_i, w_i)\\) contributes a \"bump\" to this landscape:</p> <ul> <li>Location \\(z_i\\): Where the bump is centered</li> <li>Weight \\(w_i\\): Amplitude and sign of the bump</li> <li>Kernel \\(k(\\cdot, z_i)\\): Shape of the bump</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#particles-are-the-function","title":"Particles ARE the Function","text":"<p>The particles don't just influence the value function \u2014 they define it completely. The function exists only through the particles.</p> <p>The particle memory is the value function in its nonparametric representation.</p> <p>This is like how:</p> <ul> <li>A polynomial is defined by its coefficients</li> <li>A Fourier series is defined by its frequency components</li> <li>A neural network is defined by its weights</li> </ul> <p>Particles are the \"parameters\" of the reinforcement field \u2014 but they're not learned in the usual sense. They're accumulated from experience.</p>"},{"location":"GRL0/tutorials/05-particle-memory/#4-memory-as-belief-state","title":"4. Memory as Belief State","text":""},{"location":"GRL0/tutorials/05-particle-memory/#beyond-data-storage","title":"Beyond Data Storage","text":"<p>Particle memory represents more than \"what happened.\" It represents what the agent believes about:</p> <ul> <li>Which configurations are valuable</li> <li>How experience generalizes</li> <li>What uncertainties exist</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#three-types-of-belief","title":"Three Types of Belief","text":"<p>1. Value Belief</p> <p>\"I believe configuration \\(z\\) has value proportional to \\(Q^+(z)\\).\"</p> <p>This belief emerges from the weighted sum of particle contributions.</p> <p>2. Structural Belief</p> <p>\"I believe similar configurations have similar values.\"</p> <p>This belief is encoded in the kernel function \\(k(\\cdot, \\cdot)\\).</p> <p>3. Uncertainty Belief</p> <p>\"I am uncertain about regions far from any particle.\"</p> <p>Where particles are sparse, the field is weak \u2014 signaling uncertainty.</p>"},{"location":"GRL0/tutorials/05-particle-memory/#5-how-particles-create-the-landscape","title":"5. How Particles Create the Landscape","text":""},{"location":"GRL0/tutorials/05-particle-memory/#local-influence","title":"Local Influence","text":"<p>Each particle creates a local \"energy well\" or \"hill\":</p> <p>For positive weight \\(w_i &gt; 0\\) (good experience):</p> <ul> <li>In fitness view: Creates a peak (desirable region)</li> <li>In energy view: Creates a valley (attractor)</li> </ul> <p>For negative weight \\(w_i &lt; 0\\) (bad experience):</p> <ul> <li>In fitness view: Creates a valley (undesirable)</li> <li>In energy view: Creates a hill (repeller)</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#kernel-control","title":"Kernel Control","text":"<p>The kernel determines the range of influence:</p> \\[ k(z, z_i) = \\exp\\left(-\\frac{\\|z - z_i\\|^2}{2\\ell^2}\\right) \\] <ul> <li>Small lengthscale \\(\\ell\\): Narrow, localized influence</li> <li>Large lengthscale \\(\\ell\\): Broad, far-reaching influence</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#superposition","title":"Superposition","text":"<p>The full landscape is the superposition of all particle contributions:</p> <p>$$</p> <p>Q^+(z) = \\underbrace{w_1 k(z, z_1)}{\\text{particle 1}} + \\underbrace{w_2 k(z, z_2)} $$}} + \\cdots + \\underbrace{w_N k(z, z_N)}_{\\text{particle N}</p> <p>Multiple particles can reinforce each other (constructive interference) or cancel (destructive interference).</p>"},{"location":"GRL0/tutorials/05-particle-memory/#6-particle-operations","title":"6. Particle Operations","text":"<p>Particle memory is not static. It supports dynamic operations:</p>"},{"location":"GRL0/tutorials/05-particle-memory/#61-add","title":"6.1 Add","text":"<p>Insert a new particle \\((z_{\\text{new}}, w_{\\text{new}})\\) based on recent experience.</p> <p>When: After each interaction with the environment</p> <p>Effect: Immediately reshapes the reinforcement field</p>"},{"location":"GRL0/tutorials/05-particle-memory/#62-query","title":"6.2 Query","text":"<p>Evaluate the reinforcement field at a point \\(z\\):</p> <p>$$</p> <p>Q^+(z) = \\sum_i w_i k(z, z_i) $$</p> <p>When: During action selection or policy evaluation</p> <p>Complexity: \\(O(N)\\) \u2014 must sum over all particles</p>"},{"location":"GRL0/tutorials/05-particle-memory/#63-merge","title":"6.3 Merge","text":"<p>Combine similar particles to prevent unbounded growth.</p> <p>When: Periodically or when memory exceeds capacity</p> <p>Criterion: If \\(k(z_i, z_j) &gt; \\tau\\), merge into single particle</p> <p>Result: Reduces \\(N\\) while preserving approximate field shape</p>"},{"location":"GRL0/tutorials/05-particle-memory/#64-prune","title":"6.4 Prune","text":"<p>Remove particles with low influence.</p> <p>When: During memory management</p> <p>Criterion: If \\(|w_i|\\) is very small or particle is isolated</p> <p>Result: Improves computational efficiency</p>"},{"location":"GRL0/tutorials/05-particle-memory/#65-update","title":"6.5 Update","text":"<p>Modify particle weights based on new reinforcement signals.</p> <p>When: During TD-style updates (RF-SARSA)</p> <p>Effect: Reshapes the energy landscape to reflect new evidence</p>"},{"location":"GRL0/tutorials/05-particle-memory/#7-memory-as-a-functional-ensemble","title":"7. Memory as a Functional Ensemble","text":""},{"location":"GRL0/tutorials/05-particle-memory/#not-points-in-space","title":"Not Points in Space","text":"<p>Particles are not just \"samples from a distribution.\" They are:</p> <p>Basis elements of a function-space representation</p>"},{"location":"GRL0/tutorials/05-particle-memory/#the-ensemble-view","title":"The Ensemble View","text":"<p>The particle ensemble collectively defines:</p> <ul> <li>A value function</li> <li>A belief state</li> <li>An energy landscape</li> <li>An implicit policy</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#particle-interactions","title":"Particle Interactions","text":"<p>Particles don't exist in isolation. They interact through:</p> <p>Kernel overlap: Nearby particles reinforce or interfere</p> <p>Collective influence: Far-away particles still contribute (though weakly)</p> <p>Emergent structure: Clusters of particles create \"basins\" in the landscape</p>"},{"location":"GRL0/tutorials/05-particle-memory/#8-connection-to-pomdp-belief-states","title":"8. Connection to POMDP Belief States","text":""},{"location":"GRL0/tutorials/05-particle-memory/#the-pomdp-perspective","title":"The POMDP Perspective","text":"<p>In a Partially Observable Markov Decision Process (POMDP), the agent maintains a belief state \\(b(s)\\) \u2014 a probability distribution over possible states.</p> <p>GRL's particle memory plays an analogous role, but in function space rather than probability space.</p>"},{"location":"GRL0/tutorials/05-particle-memory/#the-mapping","title":"The Mapping","text":"POMDP Concept GRL Analog Belief state \\(b(s)\\) Particle ensemble \\(\\Omega\\) Probability distribution Energy functional \\(Q^+ \\in \\mathcal{H}_k\\) Support of belief Regions with particles Belief update MemoryUpdate algorithm Action from belief Query field, navigate landscape"},{"location":"GRL0/tutorials/05-particle-memory/#why-this-matters","title":"Why This Matters","text":"<p>Viewing particle memory as a belief state explains why:</p> <ul> <li>GRL naturally handles uncertainty</li> <li>Exploration emerges from sparse particles</li> <li>No explicit observation model is needed</li> <li>The agent can reason under ambiguity</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#9-memory-management","title":"9. Memory Management","text":""},{"location":"GRL0/tutorials/05-particle-memory/#the-growth-problem","title":"The Growth Problem","text":"<p>Without management, memory grows indefinitely:</p> <ul> <li>New particles added at each step</li> <li>\\(N\\) increases linearly with experience</li> <li>Query cost \\(O(N)\\) becomes prohibitive</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#management-strategies","title":"Management Strategies","text":"<p>1. Fixed Capacity</p> <p>Maintain at most \\(N_{\\max}\\) particles. When full:</p> <ul> <li>Merge similar particles</li> <li>Prune low-weight particles</li> <li>Replace least-influential particles</li> </ul> <p>2. Adaptive Precision</p> <p>Keep high resolution (many particles) in important regions, low resolution elsewhere.</p> <p>3. Hierarchical Memory</p> <p>Organize particles into clusters or tree structures for efficient queries.</p> <p>4. Inducing Points</p> <p>Select a subset of \"representative\" particles, approximate others.</p>"},{"location":"GRL0/tutorials/05-particle-memory/#10-particle-semantics","title":"10. Particle Semantics","text":""},{"location":"GRL0/tutorials/05-particle-memory/#what-does-a-particle-mean","title":"What Does a Particle Mean?","text":"<p>A particle \\((z_i, w_i)\\) with:</p> <ul> <li>\\(z_i = (s_i, \\theta_i)\\): \"In state \\(s_i\\), action \\(\\theta_i\\) was relevant\"</li> <li>\\(w_i &gt; 0\\): \"This configuration led to positive reinforcement\"</li> <li>\\(w_i &lt; 0\\): \"This configuration led to negative reinforcement\"</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#what-does-kernel-similarity-mean","title":"What Does Kernel Similarity Mean?","text":"<p>\\(k(z, z_i) &gt; 0.5\\) means:</p> <ul> <li>\"Configuration \\(z\\) is similar enough to \\(z_i\\) that evidence from \\(z_i\\) applies to \\(z\\)\"</li> <li>\"The experience at \\(z_i\\) generalizes to \\(z\\)\"</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#what-does-the-field-value-mean","title":"What Does the Field Value Mean?","text":"<p>\\(Q^+(z)\\) is:</p> <ul> <li>The weighted evidence from all particles about configuration \\(z\\)</li> <li>The agent's best guess of value at \\(z\\)</li> <li>The fitness/energy level at that point in augmented space</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#11-example-2d-navigation","title":"11. Example: 2D Navigation","text":""},{"location":"GRL0/tutorials/05-particle-memory/#setup","title":"Setup","text":"<ul> <li>State: \\((x, y)\\) position</li> <li>Action: \\((F_x, F_y)\\) force</li> <li>Augmented: \\(z = (x, y, F_x, F_y) \\in \\mathbb{R}^4\\)</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#initial-particles","title":"Initial Particles","text":"Particle Location Weight Meaning \\(\\omega_1\\) \\((5, 5, 1, 0)\\) \\(+10\\) Good: move right near goal \\(\\omega_2\\) \\((2, 3, -1, 0)\\) \\(-5\\) Bad: moving left hit obstacle \\(\\omega_3\\) \\((0, 0, 0, 0.5)\\) \\(+2\\) OK: small upward force at start"},{"location":"GRL0/tutorials/05-particle-memory/#the-field","title":"The Field","text":"<p>At a new configuration \\(z = (4, 4, 0.8, 0.1)\\):</p> \\[ Q^+(z) = 10 \\cdot k(z, z_1) - 5 \\cdot k(z, z_2) + 2 \\cdot k(z, z_3) \\] <p>If \\(z\\) is close to \\(z_1\\) (near goal, rightward force), \\(k(z, z_1)\\) is high, so \\(Q^+(z)\\) will be positive (good).</p> <p>If \\(z\\) is far from all particles, all kernel values are small, so \\(Q^+(z) \\approx 0\\) (uncertain).</p>"},{"location":"GRL0/tutorials/05-particle-memory/#policy-inference","title":"Policy Inference","text":"<p>To decide action in state \\((4, 4)\\):</p> <ul> <li>Query field for various \\(\\theta\\): \\(Q^+((4, 4), \\theta)\\)</li> <li>Find \\(\\theta\\) that maximizes \\(Q^+\\)</li> <li>Result: Choose action similar to \\(\\theta_1 = (1, 0)\\) (move right)</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#12-summary","title":"12. Summary","text":""},{"location":"GRL0/tutorials/05-particle-memory/#core-concepts","title":"Core Concepts","text":"Concept Meaning Particle \\((z, w)\\) Weighted point in augmented space Particle memory \\(\\Omega\\) Ensemble representing belief state Basis function role Each particle is a kernel-centered function Reinforcement field Superposition of particle contributions Memory operations Add, query, merge, prune, update"},{"location":"GRL0/tutorials/05-particle-memory/#key-insights","title":"Key Insights","text":"<ol> <li>Particles ARE the value function \u2014 they define \\(Q^+\\) completely</li> <li>Memory IS a belief state \u2014 not just data storage</li> <li>Kernel similarity defines generalization \u2014 how experience spreads</li> <li>Particles interact \u2014 creating emergent landscape structure</li> <li>Dynamic memory \u2014 grows, shrinks, and reshapes over time</li> </ol>"},{"location":"GRL0/tutorials/05-particle-memory/#why-this-matters_1","title":"Why This Matters","text":"<ul> <li>Nonparametric: No fixed architecture, adapts to experience</li> <li>Interpretable: Can visualize and understand particle contributions</li> <li>Uncertainty-aware: Sparse particles = high uncertainty</li> <li>Compositional: Particles can be organized hierarchically</li> </ul>"},{"location":"GRL0/tutorials/05-particle-memory/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Particle \\((z, w)\\): Weighted point in augmented space, a basis function</li> <li>Memory \\(\\Omega\\): Ensemble of particles, defines the reinforcement field</li> <li>Not a replay buffer: Memory is a functional representation, not data</li> <li>Belief state: Particles encode what the agent believes about value</li> <li>Dynamic operations: Add, query, merge, prune, update particles</li> <li>Landscape creation: Particles collectively shape the energy landscape</li> <li>POMDP connection: Memory plays the role of belief in function space</li> </ol>"},{"location":"GRL0/tutorials/05-particle-memory/#next-steps","title":"Next Steps","text":"<p>In Chapter 6: MemoryUpdate Algorithm, we'll explore:</p> <ul> <li>How particles are added from new experiences</li> <li>The four conceptual operations of MemoryUpdate</li> <li>Why MemoryUpdate is a belief-state transition operator</li> <li>Connection to Bayesian updating in RKHS</li> </ul> <p>Related: Chapter 4: Reinforcement Field, Chapter 6: MemoryUpdate</p> <p>Last Updated: January 11, 2026</p>"},{"location":"GRL0/tutorials/06-memory-update/","title":"Chapter 6: MemoryUpdate \u2014 The Agent's Belief Transition","text":"<p>Purpose: Understand how GRL agents evolve their understanding of the world Prerequisites: Chapters 4 (Reinforcement Field), 5 (Particle Memory) Key Concepts: Belief update, particle evolution, kernel association, functional memory</p>"},{"location":"GRL0/tutorials/06-memory-update/#why-this-chapter-matters","title":"Why This Chapter Matters","text":"<p>In Chapter 5, we learned that memory is a functional representation\u2014a weighted set of experience particles that induce a reinforcement field.</p> <p>But how does this representation change as the agent experiences new things?</p> <p>This chapter introduces Algorithm 1: MemoryUpdate\u2014the core mechanism that:</p> <ul> <li>Converts new experiences into particles</li> <li>Associates them with existing knowledge</li> <li>Reshapes the energy landscape</li> <li>Maintains a tractable particle set</li> </ul> <p>Critical insight:</p> <p>MemoryUpdate is not just \"memory management\"\u2014it is the belief-state transition operator of GRL, expressed in RKHS rather than probability space.</p> <p>Everything downstream (policy inference, reinforcement propagation, soft transitions, POMDP) emerges from this single operation.</p>"},{"location":"GRL0/tutorials/06-memory-update/#the-problem-how-should-an-agent-update-its-beliefs","title":"The Problem: How Should an Agent Update Its Beliefs?","text":""},{"location":"GRL0/tutorials/06-memory-update/#traditional-rl-approach","title":"Traditional RL Approach","text":"<p>In standard RL, when the agent observes \\((s_t, a_t, r_t, s_{t+1})\\), it updates:</p> <ol> <li> <p>Value function via Bellman backup:    $\\(Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)]\\)$</p> </li> <li> <p>Experience buffer (if using replay):    $\\(\\text{buffer} \\leftarrow \\text{buffer} \\cup \\{(s, a, r, s')\\}\\)$</p> </li> <li> <p>Policy (if policy gradient):    $\\(\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot A(s, a)\\)$</p> </li> </ol> <p>These are three separate operations on three separate structures.</p>"},{"location":"GRL0/tutorials/06-memory-update/#grl-approach","title":"GRL Approach","text":"<p>In GRL, when the agent observes \\((s_t, \\theta_t, r_t)\\) (remember: actions are parameters \\(\\theta\\)), it performs one operation:</p> \\[\\text{MemoryUpdate}: \\mathcal{M} \\rightarrow \\mathcal{M}'\\] <p>This single operation simultaneously:</p> <ul> <li>Updates the agent's belief about the value landscape</li> <li>Incorporates new experience into memory</li> <li>Reshapes the policy (implicitly, via the field)</li> <li>Maintains uncertainty representation</li> </ul> <p>How? By operating at the functional level in RKHS, not the parametric level.</p>"},{"location":"GRL0/tutorials/06-memory-update/#what-memoryupdate-does-conceptual-view","title":"What MemoryUpdate Does (Conceptual View)","text":"<p>Before diving into the algorithm, let's understand its four core operations:</p>"},{"location":"GRL0/tutorials/06-memory-update/#1-particle-instantiation","title":"1. Particle Instantiation","text":"<p>What: Convert experience \\((s_t, \\theta_t, r_t)\\) into an augmented state-action particle</p> \\[z_{new} = (s_t, \\theta_t)\\] <p>Why: This particle is a hypothesis about what matters\u2014a point in augmented space where something significant happened.</p> <p>Not: Just storing a tuple. The particle becomes a functional basis element in RKHS.</p>"},{"location":"GRL0/tutorials/06-memory-update/#2-geometric-association","title":"2. Geometric Association","text":"<p>What: Compute how similar the new experience is to existing particles</p> \\[a_i = k(z_{new}, z_i) \\quad \\text{for each } z_i \\in \\mathcal{M}\\] <p>where \\(k(\\cdot, \\cdot)\\) is the RKHS kernel over augmented space.</p> <p>Why: This determines:</p> <ul> <li>How much the new experience confirms existing beliefs</li> <li>How far its influence should spread in the landscape</li> <li>Which particles should be adjusted in response</li> </ul> <p>Not: Nearest-neighbor lookup. Association is soft, global, and geometry-aware.</p>"},{"location":"GRL0/tutorials/06-memory-update/#3-energy-functional-maintenance","title":"3. Energy Functional Maintenance","text":"<p>What: Assign a weight \\(w_{new}\\) to the new particle, and optionally adjust weights of associated particles</p> \\[w_{new} = g(r_t, \\{a_i\\})\\] <p>where \\(g(\\cdot)\\) maps reinforcement evidence to an energy contribution.</p> <p>Why: Particle weights determine the shape of the energy landscape:</p> \\[E(z) = -Q^+(z) = -\\sum_i w_i k(z_i, z)\\] <p>Adjusting weights reshapes this landscape to reflect new evidence.</p> <p>Not: Direct value update. We're modifying the functional representation, not tabular entries.</p>"},{"location":"GRL0/tutorials/06-memory-update/#4-memory-management","title":"4. Memory Management","text":"<p>What: Prune, merge, or decay particles to maintain bounded complexity</p> <p>Why: Without this, the particle set grows unboundedly and computational cost explodes.</p> <p>How: - Attenuate particles with small \\(|w_i|\\) (low evidence) - Merge particles with high mutual similarity (redundant) - Discard particles below relevance threshold (forgetting)</p> <p>Not: Simple replay buffer eviction. This is functional regularization + model selection.</p>"},{"location":"GRL0/tutorials/06-memory-update/#algorithm-1-memoryupdate-formal-specification","title":"Algorithm 1: MemoryUpdate (Formal Specification)","text":"<p>Now let's see the actual algorithm with precise notation.</p>"},{"location":"GRL0/tutorials/06-memory-update/#input","title":"Input","text":"Symbol Meaning Type \\(\\mathcal{M}\\) Current particle memory \\(\\{(z_i, w_i)\\}_{i=1}^N\\) \\(z_i = (s_i, \\theta_i)\\) Augmented state-action particle \\(z_i \\in \\mathcal{Z} = \\mathcal{S} \\times \\Theta\\) \\(w_i\\) Particle weight (energy contribution) \\(w_i \\in \\mathbb{R}\\) \\((s_t, \\theta_t, r_t)\\) New experience tuple \\((s_t, \\theta_t) \\in \\mathcal{Z}, r_t \\in \\mathbb{R}\\) \\(k(\\cdot, \\cdot)\\) RKHS kernel over augmented space \\(k: \\mathcal{Z} \\times \\mathcal{Z} \\to \\mathbb{R}\\)"},{"location":"GRL0/tutorials/06-memory-update/#output","title":"Output","text":"Symbol Meaning \\(\\mathcal{M}'\\) Updated particle memory"},{"location":"GRL0/tutorials/06-memory-update/#algorithm-steps","title":"Algorithm Steps","text":"<p>Step 1: Particle Construction</p> <p>Form the new augmented particle:</p> \\[z_{new} = (s_t, \\theta_t)\\] <p>Step 2: Kernel-Based Association</p> <p>For each existing particle \\(z_i \\in \\mathcal{M}\\), compute similarity:</p> \\[a_i = k(z_{new}, z_i)\\] <p>This creates an association vector:</p> \\[\\mathbf{a} = [a_1, a_2, \\ldots, a_N]^T\\] <p>Step 3: Energy Contribution Initialization</p> <p>Assign an initial weight to \\(z_{new}\\), proportional to the reinforcement signal:</p> \\[w_{new} = g(r_t, \\mathbf{a})\\] <p>Common choices for \\(g(\\cdot)\\):</p> <ol> <li>Direct reinforcement: \\(w_{new} = r_t\\)</li> <li>Normalized: \\(w_{new} = \\frac{r_t}{\\sum_i a_i + \\epsilon}\\)</li> <li>Novelty-weighted: \\(w_{new} = r_t \\cdot (1 - \\max_i a_i)\\)</li> </ol> <p>The paper uses option 1 (direct) for simplicity, but options 2-3 can improve performance.</p> <p>Step 4: Memory Integration</p> <p>Insert the new particle into memory:</p> \\[\\mathcal{M} \\leftarrow \\mathcal{M} \\cup \\{(z_{new}, w_{new})\\}\\] <p>At this point, the energy functional becomes:</p> \\[E(z) = -\\sum_{i=1}^{N} w_i k(z_i, z) - w_{new} k(z_{new}, z)\\] <p>Step 5: Association-Driven Adjustment (Optional)</p> <p>For particles with strong association (\\(a_i &gt; \\varepsilon\\)), adjust their weights:</p> \\[w_i \\leftarrow w_i + \\lambda \\cdot a_i \\cdot w_{new}\\] <p>where:</p> <ul> <li>\\(\\lambda \\in [0, 1]\\) controls propagation strength</li> <li>\\(a_i\\) determines how much particle \\(i\\) should respond</li> <li>\\(w_{new}\\) provides the signal to propagate</li> </ul> <p>Interpretation: New evidence propagates geometrically to related experiences, enabling generalization without Bellman backups.</p> <p>Step 6: Memory Regularization</p> <p>Apply one or more of the following:</p> <p>a) Weight decay:</p> \\[w_i \\leftarrow \\gamma_w \\cdot w_i \\quad \\text{for all } i\\] <p>where \\(\\gamma_w \\in (0, 1]\\) (e.g., 0.99).</p> <p>b) Particle merging:</p> <p>If \\(k(z_i, z_j) &gt; \\tau_{merge}\\) for some pair \\((i, j)\\):</p> \\[z_{merged} = \\frac{w_i z_i + w_j z_j}{w_i + w_j}, \\quad w_{merged} = w_i + w_j\\] <p>Remove \\((z_i, w_i)\\) and \\((z_j, w_j)\\); add \\((z_{merged}, w_{merged})\\).</p> <p>c) Particle pruning:</p> <p>Remove particles with \\(|w_i| &lt; \\tau_{prune}\\).</p> <p>Step 7: Return Updated Memory</p> \\[\\text{return } \\mathcal{M}'\\]"},{"location":"GRL0/tutorials/06-memory-update/#line-by-line-interpretation","title":"Line-by-Line Interpretation","text":"<p>Let's unpack what each step really means for the agent.</p>"},{"location":"GRL0/tutorials/06-memory-update/#step-1-particle-construction","title":"Step 1: Particle Construction","text":"<p>Plain language:</p> <p>\"This experience is a hypothesis about what actions matter in what states.\"</p> <p>Why particle form?</p> <p>Because in RKHS, every particle \\(z_i\\) induces a kernel section \\(k(z_i, \\cdot)\\)\u2014a function in \\(\\mathcal{H}_k\\) that carries influence throughout the augmented space.</p> <p>Geometric view:</p> <p>You're adding a \"probe\" into the value landscape\u2014a point that will shape the field around it.</p>"},{"location":"GRL0/tutorials/06-memory-update/#step-2-kernel-association","title":"Step 2: Kernel Association","text":"<p>Plain language:</p> <p>\"How compatible is this experience with what I already believe?\"</p> <p>Not asking: \"Is this the same state?\" or \"Is this the same action?\"</p> <p>Asking: \"How much does this experience resonate with existing knowledge?\"</p> <p>Example (Gaussian RBF kernel):</p> \\[a_i = \\exp\\left(-\\frac{\\|z_{new} - z_i\\|^2}{2\\sigma^2}\\right)\\] <ul> <li>If \\(z_{new} \\approx z_i\\): \\(a_i \\approx 1\\) (high similarity)</li> <li>If \\(z_{new}\\) far from \\(z_i\\): \\(a_i \\approx 0\\) (low similarity)</li> <li>Intermediate distances: \\(a_i \\in (0, 1)\\) (partial similarity)</li> </ul> <p>This is soft association\u2014every particle contributes, weighted by distance.</p>"},{"location":"GRL0/tutorials/06-memory-update/#step-3-energy-contribution-initialization","title":"Step 3: Energy Contribution Initialization","text":"<p>Plain language:</p> <p>\"How strongly should this experience shape the landscape?\"</p> <p>Not: \"What is the value of \\((s, a)\\)?\"</p> <p>But: \"How much evidence does this experience provide?\"</p> <p>Sign matters: - If \\(r_t &gt; 0\\): \\(w_{new} &gt; 0\\) \u2192 positive particle \u2192 attracts in field - If \\(r_t &lt; 0\\): \\(w_{new} &lt; 0\\) \u2192 negative particle \u2192 repels in field</p> <p>Magnitude matters: - Large \\(|r_t|\\): Strong influence - Small \\(|r_t|\\): Weak influence</p>"},{"location":"GRL0/tutorials/06-memory-update/#step-4-memory-integration","title":"Step 4: Memory Integration","text":"<p>Plain language:</p> <p>\"The agent's state of knowledge has changed.\"</p> <p>After this step, the energy functional is:</p> \\[E(z) = -\\sum_{i=1}^{N+1} w_i k(z_i, z)\\] <p>This is the new belief state of the agent\u2014a functional representation of all experiences so far.</p> <p>Key insight: Memory grows functionally, not discretely. The agent doesn't just \"remember more tuples\"\u2014it maintains a richer functional prior for policy inference.</p>"},{"location":"GRL0/tutorials/06-memory-update/#step-5-association-driven-adjustment","title":"Step 5: Association-Driven Adjustment","text":"<p>Plain language:</p> <p>\"New evidence doesn't just affect the new particle\u2014it propagates to related experiences.\"</p> <p>This is subtle and powerful.</p> <p>Example:</p> <p>Suppose:</p> <ul> <li>Agent has particle \\(z_1 = (s_1, \\theta_1)\\) with \\(w_1 = +2.0\\)</li> <li>New experience: \\(z_{new} = (s_1, \\theta_2)\\) with \\(r_t = +3.0\\)</li> <li>Association: \\(a_1 = k(z_{new}, z_1) = 0.8\\) (high similarity)</li> </ul> <p>After Step 5:</p> \\[w_1 \\leftarrow 2.0 + \\lambda \\cdot 0.8 \\cdot 3.0 = 2.0 + 2.4\\lambda\\] <p>If \\(\\lambda = 0.5\\), then \\(w_1 = 3.2\\).</p> <p>Interpretation: The new positive experience at \\(z_{new}\\) reinforces the nearby particle \\(z_1\\), even though they have different action parameters.</p> <p>This is how GRL generalizes without Bellman backups\u2014evidence spreads through kernel geometry.</p>"},{"location":"GRL0/tutorials/06-memory-update/#step-6-memory-regularization","title":"Step 6: Memory Regularization","text":"<p>Plain language:</p> <p>\"Keep the particle set tractable and adaptive.\"</p> <p>Why each sub-operation matters:</p> <p>a) Weight decay: - Implements forgetting (older evidence matters less) - Prevents weight explosion - Allows adaptation in non-stationary environments</p> <p>b) Particle merging: - Removes redundancy (two particles in nearly the same location) - Maintains functional expressiveness without wasting particles - Natural form of model compression</p> <p>c) Particle pruning: - Removes low-evidence particles (noise or outliers) - Bounds computational cost - Implements Occam's razor (prefer simpler representations)</p>"},{"location":"GRL0/tutorials/06-memory-update/#worked-example-1d-navigation","title":"Worked Example: 1D Navigation","text":"<p>Let's see MemoryUpdate in action with a concrete example.</p>"},{"location":"GRL0/tutorials/06-memory-update/#setup","title":"Setup","text":"<ul> <li>State space: \\(\\mathcal{S} = [0, 10]\\) (1D position)</li> <li>Action space: \\(\\Theta = [-1, +1]\\) (velocity parameter)</li> <li>Kernel: Gaussian RBF, \\(k(z, z') = \\exp(-\\|z - z'\\|^2 / 2)\\)</li> <li>Goal: \\(s = 10\\) (reward \\(r = +10\\))</li> <li>Current memory: \\(\\mathcal{M} = \\{(z_1, w_1), (z_2, w_2)\\}\\)</li> </ul> <p>where:</p> <ul> <li>\\(z_1 = (8.0, +0.5)\\), \\(w_1 = +5.0\\)</li> <li>\\(z_2 = (3.0, -0.3)\\), \\(w_2 = -2.0\\)</li> </ul>"},{"location":"GRL0/tutorials/06-memory-update/#new-experience","title":"New Experience","text":"<p>Agent at \\(s_t = 9.0\\), takes action \\(\\theta_t = +0.8\\), reaches goal, receives \\(r_t = +10.0\\).</p>"},{"location":"GRL0/tutorials/06-memory-update/#step-by-step-execution","title":"Step-by-Step Execution","text":"<p>Step 1: Particle Construction</p> \\[z_{new} = (9.0, 0.8)\\] <p>Step 2: Kernel Association</p> <p>Compute similarity to existing particles:</p> \\[a_1 = k(z_{new}, z_1) = k\\Big((9.0, 0.8), (8.0, 0.5)\\Big)\\] <p>Distance:</p> \\[\\|z_{new} - z_1\\| = \\sqrt{(9.0 - 8.0)^2 + (0.8 - 0.5)^2} = \\sqrt{1.0 + 0.09} = 1.044\\] <p>Similarity:</p> \\[a_1 = \\exp(-1.044^2 / 2) = \\exp(-0.545) \\approx 0.58\\] <p>Similarly:</p> \\[\\|z_{new} - z_2\\| = \\sqrt{(9.0 - 3.0)^2 + (0.8 + 0.3)^2} = \\sqrt{36 + 1.21} = 6.10\\] \\[a_2 = \\exp(-6.10^2 / 2) = \\exp(-18.6) \\approx 0.0\\] <p>Association vector: \\(\\mathbf{a} = [0.58, 0.0]^T\\)</p> <p>Interpretation: The new experience is similar to \\(z_1\\) (both near the goal with positive actions), but dissimilar to \\(z_2\\) (far from goal, negative action).</p> <p>Step 3: Energy Contribution</p> <p>Using direct reinforcement:</p> \\[w_{new} = r_t = +10.0\\] <p>Step 4: Memory Integration</p> \\[\\mathcal{M} \\leftarrow \\{(z_1, +5.0), (z_2, -2.0), (z_{new}, +10.0)\\}\\] <p>Energy functional:</p> \\[E(z) = -5.0 \\cdot k(z, z_1) + 2.0 \\cdot k(z, z_2) - 10.0 \\cdot k(z, z_{new})\\] <p>(Recall: \\(E = -Q^+\\), so negative weights attract, positive repel in energy terms.)</p> <p>Step 5: Association-Driven Adjustment (with \\(\\lambda = 0.5\\))</p> <p>Only \\(z_1\\) has significant association (\\(a_1 = 0.58 &gt; \\varepsilon\\)):</p> \\[w_1 \\leftarrow 5.0 + 0.5 \\cdot 0.58 \\cdot 10.0 = 5.0 + 2.9 = 7.9\\] <p>Updated memory:</p> \\[\\mathcal{M} = \\{(z_1, +7.9), (z_2, -2.0), (z_{new}, +10.0)\\}\\] <p>Interpretation: The new positive experience reinforces \\(z_1\\) because they're geometrically close\u2014generalization via kernel association!</p> <p>Step 6: Memory Regularization (simplified)</p> <p>Apply weight decay with \\(\\gamma_w = 0.99\\):</p> \\[w_1 \\leftarrow 0.99 \\cdot 7.9 = 7.82$$ $$w_2 \\leftarrow 0.99 \\cdot (-2.0) = -1.98$$ $$w_{new} \\leftarrow 0.99 \\cdot 10.0 = 9.90\\] <p>Final memory:</p> \\[\\mathcal{M}' = \\{(z_1, +7.82), (z_2, -1.98), (z_{new}, +9.90)\\}\\]"},{"location":"GRL0/tutorials/06-memory-update/#what-changed","title":"What Changed?","text":"<p>Before MemoryUpdate:</p> <p>Energy landscape had:</p> <ul> <li>Moderate attraction near \\((8.0, 0.5)\\) (weight \\(+5.0\\))</li> <li>Weak repulsion near \\((3.0, -0.3)\\) (weight \\(-2.0\\))</li> </ul> <p>After MemoryUpdate:</p> <p>Energy landscape has:</p> <ul> <li>Stronger attraction near \\((8.0, 0.5)\\) (weight \\(+7.82\\)) \u2190 reinforced by new evidence</li> <li>Still weak repulsion near \\((3.0, -0.3)\\) (weight \\(-1.98\\)) \u2190 unchanged (no association)</li> <li>Strong new attraction near \\((9.0, 0.8)\\) (weight \\(+9.90\\)) \u2190 new high-value region</li> </ul> <p>Policy implication: Agent will now strongly prefer positive velocities when near the goal (generalization from one good experience to a neighborhood!).</p>"},{"location":"GRL0/tutorials/06-memory-update/#why-memoryupdate-is-instrumental-not-just-representational","title":"Why MemoryUpdate Is Instrumental (Not Just Representational)","text":"<p>Earlier we said MemoryUpdate is the belief-state transition operator. Let's unpack why it's instrumental for everything downstream.</p>"},{"location":"GRL0/tutorials/06-memory-update/#1-memoryupdate-enables-policy-inference","title":"1. MemoryUpdate Enables Policy Inference","text":"<p>The policy is defined as:</p> \\[\\pi(\\theta | s) \\propto \\exp(\\beta \\cdot Q^+(s, \\theta))\\] <p>But \\(Q^+\\) is determined by particle memory:</p> \\[Q^+(s, \\theta) = \\sum_i w_i k((s, \\theta), z_i)\\] <p>So changing \\(\\mathcal{M}\\) directly changes the policy\u2014no separate policy update needed.</p> <p>Example from above: After the update, \\(\\pi(\\theta | s=9)\\) will strongly favor \\(\\theta \\approx +0.8\\) because \\(w_{new} = +9.90\\) creates high \\(Q^+\\) there.</p>"},{"location":"GRL0/tutorials/06-memory-update/#2-memoryupdate-implements-reinforcement-propagation","title":"2. MemoryUpdate Implements Reinforcement Propagation","text":"<p>In traditional RL, reinforcement propagates via Bellman backups:</p> \\[Q(s, a) \\gets r + \\gamma \\max_{a'} Q(s', a')\\] <p>In GRL, reinforcement propagates via kernel association (Step 5):</p> \\[w_i \\leftarrow w_i + \\lambda \\cdot a_i \\cdot w_{new}\\] <p>Advantages: - Soft: Propagation strength depends on similarity \\(a_i\\) - Global: All associated particles update simultaneously - Geometry-aware: Propagation respects the RKHS metric</p>"},{"location":"GRL0/tutorials/06-memory-update/#3-memoryupdate-induces-soft-state-transitions","title":"3. MemoryUpdate Induces Soft State Transitions","text":"<p>Classical RL assumes deterministic or explicitly stochastic state transitions:</p> \\[s_{t+1} \\sim P(\\cdot | s_t, a_t)\\] <p>GRL has no explicit transition model. Instead, transitions are implicitly soft due to kernel overlap.</p> <p>How?</p> <p>When the agent observes \\((s_t, \\theta_t, r_t)\\), it creates particle \\(z_t = (s_t, \\theta_t)\\). This particle associates with multiple existing particles via \\(a_i = k(z_t, z_i)\\), each representing a different \"hypothesis\" about what state-action is relevant.</p> <p>The agent maintains belief over multiple hypotheses simultaneously\u2014emergent uncertainty from geometry!</p>"},{"location":"GRL0/tutorials/06-memory-update/#4-memoryupdate-is-the-pomdp-belief-update","title":"4. MemoryUpdate Is the POMDP Belief Update","text":"<p>In a POMDP, belief updates follow:</p> \\[b'(s') = \\frac{O(o | s') \\sum_s T(s' | s, a) b(s)}{\\text{normalizer}}\\] <p>In GRL, particle memory \\(\\mathcal{M}\\) is the belief state:</p> \\[b(z) \\propto \\sum_i w_i k(z, z_i)\\] <p>MemoryUpdate transitions:</p> \\[b_t(z) \\xrightarrow{\\text{MemoryUpdate}} b_{t+1}(z)\\] <p>But instead of Bayesian filtering over a discrete state space, we're doing functional belief evolution in RKHS.</p> <p>Key insight: POMDP structure emerges for free from the particle representation\u2014no need to specify observation or transition models!</p>"},{"location":"GRL0/tutorials/06-memory-update/#connection-to-other-grl-components","title":"Connection to Other GRL Components","text":"<p>Let's see how MemoryUpdate connects to the rest of the GRL framework.</p>"},{"location":"GRL0/tutorials/06-memory-update/#memoryupdate-reinforcement-field","title":"MemoryUpdate + Reinforcement Field","text":"<p>Chapter 4: The reinforcement field is:</p> \\[Q^+(z) = \\sum_i w_i k(z_i, z)\\] <p>MemoryUpdate modifies: - Particle locations \\(\\{z_i\\}\\) (by adding new particles) - Particle weights \\(\\{w_i\\}\\) (by adjusting in response to new evidence)</p> <p>Result: The field reshapes to reflect new experience\u2014peaks form in high-value regions, valleys in low-value regions.</p>"},{"location":"GRL0/tutorials/06-memory-update/#memoryupdate-particle-memory","title":"MemoryUpdate + Particle Memory","text":"<p>Chapter 5: Memory is a functional representation of experience.</p> <p>MemoryUpdate implements: - Dynamic basis expansion: Adding new particles = adding new basis functions - Weight adjustment: Changing the functional prior - Basis compression: Merging/pruning to maintain tractability</p> <p>Result: Memory stays expressive (can represent complex fields) yet bounded (doesn't explode).</p>"},{"location":"GRL0/tutorials/06-memory-update/#memoryupdate-rf-sarsa","title":"MemoryUpdate + RF-SARSA","text":"<p>Chapter 7 (next): RF-SARSA provides the reinforcement signal \\(r_t\\) that enters MemoryUpdate.</p> <p>Two-layer system: 1. RF-SARSA (outer loop): Decides what reinforcement signal to send 2. MemoryUpdate (inner loop): Reshapes the field in response</p> <p>Analogy: RF-SARSA is the \"teacher\" providing feedback; MemoryUpdate is the \"learner\" updating beliefs.</p>"},{"location":"GRL0/tutorials/06-memory-update/#common-misconceptions","title":"Common Misconceptions","text":""},{"location":"GRL0/tutorials/06-memory-update/#misconception-1-memoryupdate-is-just-experience-replay","title":"Misconception 1: \"MemoryUpdate Is Just Experience Replay\"","text":"<p>Reality: Experience replay stores tuples and samples them for training. MemoryUpdate converts experiences into functional basis elements that directly shape the value landscape.</p> <p>Key difference: Replay is passive storage; MemoryUpdate is active belief evolution.</p>"},{"location":"GRL0/tutorials/06-memory-update/#misconception-2-kernel-association-is-just-nearest-neighbor","title":"Misconception 2: \"Kernel Association Is Just Nearest-Neighbor\"","text":"<p>Reality: Kernel association is:</p> <ul> <li>Soft: All particles contribute, weighted by similarity</li> <li>Global: Associations computed with all particles simultaneously</li> <li>Differentiable: Enables smooth generalization</li> </ul> <p>Nearest-neighbor is:</p> <ul> <li>Hard: Only the closest point matters</li> <li>Local: No information about the broader landscape</li> <li>Discontinuous: Small changes in input cause jumps</li> </ul>"},{"location":"GRL0/tutorials/06-memory-update/#misconception-3-memoryupdate-only-affects-memory","title":"Misconception 3: \"MemoryUpdate Only Affects Memory\"","text":"<p>Reality: MemoryUpdate simultaneously affects:</p> <ul> <li>The value landscape (via particle weights)</li> <li>The policy (via \\(Q^+(s, \\theta)\\))</li> <li>Belief uncertainty (via particle diversity)</li> <li>Generalization (via kernel propagation)</li> </ul> <p>It's the state transition operator of the agent.</p>"},{"location":"GRL0/tutorials/06-memory-update/#misconception-4-step-5-association-driven-adjustment-is-optional","title":"Misconception 4: \"Step 5 (Association-Driven Adjustment) Is Optional\"","text":"<p>Truth: Step 5 is optional in implementation, but it's conceptually central to GRL.</p> <p>Why: Without Step 5, GRL reduces to kernel regression with independent samples. With Step 5, GRL becomes a belief propagation system where evidence spreads through geometry.</p> <p>Recommendation: Include Step 5 for best performance, especially in continuous domains where generalization is critical.</p>"},{"location":"GRL0/tutorials/06-memory-update/#practical-considerations","title":"Practical Considerations","text":""},{"location":"GRL0/tutorials/06-memory-update/#1-computational-complexity","title":"1. Computational Complexity","text":"<p>Per update cost:</p> <ul> <li>Step 2 (Association): \\(O(N)\\) kernel evaluations</li> <li>Step 5 (Adjustment): \\(O(N)\\) weight updates (if all \\(a_i &gt; \\varepsilon\\))</li> <li>Step 6 (Regularization): \\(O(N^2)\\) for merging (pairwise comparison), \\(O(N)\\) for pruning</li> </ul> <p>Total: \\(O(N)\\) for association + adjustment, \\(O(N^2)\\) for merging.</p> <p>Optimization: - Use sparse association: Only update particles with \\(a_i &gt; \\varepsilon\\) - Use KD-trees or ball trees to find high-association particles quickly - Merge periodically (every \\(K\\) steps) rather than every update</p>"},{"location":"GRL0/tutorials/06-memory-update/#2-hyperparameters","title":"2. Hyperparameters","text":"Parameter Meaning Typical Range Effect of Increasing \\(\\sigma\\) (kernel bandwidth) Similarity scale \\([0.1, 10]\\) More particles associate (smoother field) \\(\\lambda\\) (propagation) Step 5 coupling strength \\([0, 1]\\) Stronger evidence propagation \\(\\gamma_w\\) (decay) Weight decay rate \\([0.95, 1.0]\\) Faster forgetting \\(\\tau_{merge}\\) Merge threshold \\([0.8, 0.99]\\) More aggressive merging \\(\\tau_{prune}\\) Prune threshold \\([0.01, 0.5]\\) More aggressive pruning <p>Tuning guidance: - Start with \\(\\sigma \\approx \\text{characteristic state-action distance}\\) - Use \\(\\lambda = 0.5\\) as default (moderate propagation) - Set \\(\\gamma_w = 0.99\\) for stationary environments, \\(0.95\\) for non-stationary - Tune \\(\\tau_{merge}\\) and \\(\\tau_{prune}\\) based on memory budget</p>"},{"location":"GRL0/tutorials/06-memory-update/#3-implementation-tips","title":"3. Implementation Tips","text":"<p>Efficient kernel computation:</p> <p>For Gaussian RBF:</p> <pre><code>def kernel(z1, z2, sigma=1.0):\n    dist_sq = np.sum((z1 - z2)**2)\n    return np.exp(-dist_sq / (2 * sigma**2))\n</code></pre> <p>Sparse association:</p> <pre><code>def update_memory(memory, z_new, r_t, epsilon=0.1, lambda_prop=0.5):\n    # Step 2: Compute associations\n    associations = [kernel(z_new, z_i) for z_i, w_i in memory]\n\n    # Step 3: Initialize weight\n    w_new = r_t  # Direct reinforcement\n\n    # Step 4: Add to memory\n    memory.append((z_new, w_new))\n\n    # Step 5: Update associated particles (sparse)\n    for i, (a_i, (z_i, w_i)) in enumerate(zip(associations, memory[:-1])):\n        if a_i &gt; epsilon:\n            memory[i] = (z_i, w_i + lambda_prop * a_i * w_new)\n\n    return memory\n</code></pre> <p>Vectorized version:</p> <pre><code>import numpy as np\n\ndef update_memory_vectorized(Z, W, z_new, r_t, sigma=1.0, lambda_prop=0.5):\n    \"\"\"\n    Z: (N, d) array of particle locations\n    W: (N,) array of particle weights\n    z_new: (d,) new particle location\n    r_t: scalar reward\n    \"\"\"\n    # Step 2: Compute all associations at once\n    dists_sq = np.sum((Z - z_new[None, :])**2, axis=1)\n    A = np.exp(-dists_sq / (2 * sigma**2))  # (N,) association vector\n\n    # Step 3 + 4: Add new particle\n    w_new = r_t\n    Z = np.vstack([Z, z_new[None, :]])\n    W = np.append(W, w_new)\n\n    # Step 5: Update associated particles\n    W[:-1] += lambda_prop * A * w_new\n\n    return Z, W\n</code></pre>"},{"location":"GRL0/tutorials/06-memory-update/#visualization-field-evolution","title":"Visualization: Field Evolution","text":"<p>Let's see how the reinforcement field evolves through MemoryUpdate.</p>"},{"location":"GRL0/tutorials/06-memory-update/#initial-state-2-particles","title":"Initial State (2 particles)","text":"<pre><code>Particle memory:\n  z_1 = (8.0, +0.5), w_1 = +5.0\n  z_2 = (3.0, -0.3), w_2 = -2.0\n\nField Q^+(s, \u03b8):\n      \u03b8\n    1 |           \u2295 (peak near z_1)\n    0 |------------------------\n   -1 |     \u2296 (valley near z_2)\n      |________________________\n      0    3    6    9    10   s\n</code></pre> <p>Legend: - \u2295 High value region (positive particle) - \u2296 Low value region (negative particle)</p>"},{"location":"GRL0/tutorials/06-memory-update/#after-new-experience-3-particles","title":"After New Experience (3 particles)","text":"<p>New experience: \\((s=9, \u03b8=0.8, r=+10)\\)</p> <pre><code>Particle memory:\n  z_1 = (8.0, +0.5), w_1 = +7.9  \u2190 reinforced!\n  z_2 = (3.0, -0.3), w_2 = -2.0  \u2190 unchanged\n  z_new = (9.0, +0.8), w_new = +10.0  \u2190 new peak!\n\nField Q^+(s, \u03b8):\n      \u03b8\n    1 |          \u2295\u2295 (stronger peak, two nearby particles)\n    0 |------------------------\n   -1 |     \u2296\n      |________________________\n      0    3    6    9    10   s\n</code></pre> <p>What changed: - Peak near goal strengthened (two particles now: \\(z_1\\) and \\(z_{new}\\)) - Peak widened (kernel overlap creates broader attraction) - Policy will strongly prefer \\(\\theta \\approx 0.5\\)-\\(0.8\\) when \\(s \\approx 8\\)-\\(9\\)</p>"},{"location":"GRL0/tutorials/06-memory-update/#summary","title":"Summary","text":""},{"location":"GRL0/tutorials/06-memory-update/#memoryupdate-is","title":"MemoryUpdate Is...","text":"What It Is What It Does Belief-state transition operator Updates agent's functional representation of the world RKHS operation Adds basis functions, adjusts weights in function space Generalization mechanism Spreads evidence through kernel geometry Functional regularizer Maintains bounded, expressive particle sets"},{"location":"GRL0/tutorials/06-memory-update/#key-equations","title":"Key Equations","text":"<p>Energy functional after update:</p> \\[E(z) = -\\sum_{i \\in \\mathcal{M}'} w_i k(z_i, z)\\] <p>Association vector:</p> \\[a_i = k(z_{new}, z_i)\\] <p>Association-driven weight update:</p> \\[w_i \\leftarrow w_i + \\lambda \\cdot a_i \\cdot w_{new}\\]"},{"location":"GRL0/tutorials/06-memory-update/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>MemoryUpdate is not \"just memory\"</li> <li>It's the agent's state transition operator</li> <li> <p>It simultaneously affects value, policy, and belief</p> </li> <li> <p>Kernel association enables soft generalization</p> </li> <li>Evidence spreads to geometrically similar experiences</li> <li> <p>No Bellman backups needed</p> </li> <li> <p>Particles are functional bases</p> </li> <li>Adding a particle = adding a basis function to RKHS</li> <li> <p>Adjusting weights = reshaping the field</p> </li> <li> <p>Regularization is essential</p> </li> <li>Without pruning/merging, complexity explodes</li> <li> <p>Implements forgetting and model selection</p> </li> <li> <p>MemoryUpdate enables everything downstream</p> </li> <li>Policy inference: via \\(Q^+(s, \\theta)\\) induced by particles</li> <li>Soft transitions: via kernel overlap</li> <li>POMDP: via functional belief representation</li> </ol>"},{"location":"GRL0/tutorials/06-memory-update/#looking-ahead","title":"Looking Ahead","text":"<p>In the next chapter, we'll introduce RF-SARSA\u2014the algorithm that determines what reinforcement signal to send to MemoryUpdate.</p> <p>Spoiler: RF-SARSA implements temporal-difference learning in RKHS, enabling the agent to bootstrap value estimates from the reinforcement field itself.</p> <p>Together, MemoryUpdate and RF-SARSA form a two-layer learning system:</p> <ul> <li>RF-SARSA: Computes TD error in function space</li> <li>MemoryUpdate: Reshapes the field in response</li> </ul> <p>This is how GRL learns without explicit value function approximation or policy gradients!</p>"},{"location":"GRL0/tutorials/06-memory-update/#further-reading","title":"Further Reading","text":""},{"location":"GRL0/tutorials/06-memory-update/#within-this-tutorial","title":"Within This Tutorial","text":"<ul> <li>Chapter 2: RKHS Foundations \u2014 Kernel inner products and function spaces</li> <li>Chapter 4: Reinforcement Field \u2014 The functional landscape shaped by particles</li> <li>Chapter 4a: Riesz Representer \u2014 Gradients in function space</li> <li>Chapter 5: Particle Memory \u2014 Memory as functional representation</li> </ul>"},{"location":"GRL0/tutorials/06-memory-update/#next-chapters","title":"Next Chapters","text":"<ul> <li>Chapter 6a: Advanced Memory Dynamics (supplement) \u2014 Practical improvements beyond hard thresholds</li> <li>Chapter 7: RF-SARSA (next) \u2014 Temporal-difference learning in RKHS</li> <li>Chapter 8: Soft State Transitions \u2014 Emergent uncertainty from kernel geometry</li> <li>Chapter 9: POMDP Interpretation \u2014 Belief-based view of GRL</li> </ul>"},{"location":"GRL0/tutorials/06-memory-update/#advanced-topics","title":"Advanced Topics","text":"<p>For principled alternatives to hard threshold \\(\\tau\\):</p> <p>Chapter 06a: Advanced Memory Dynamics \u2192</p> <p>Practical improvements:</p> <ul> <li>Top-k Adaptive Neighbors \u2014 Density-aware, no global threshold</li> <li>Surprise-Gated Consolidation \u2014 Data-driven, bounded memory growth</li> <li>Hybrid Approach \u2014 Combines both methods</li> <li>Code examples and decision guides</li> </ul> <p>For full theoretical treatment:</p> <ul> <li>Chapter 07: Learning Beyond GP \u2014 Alternative learning mechanisms</li> <li>Chapter 08: Memory Dynamics \u2014 Formation, consolidation, retrieval operators</li> </ul>"},{"location":"GRL0/tutorials/06-memory-update/#original-paper","title":"Original Paper","text":"<ul> <li>Section IV-A: Experience Association and Particle Evolution</li> <li>Algorithm 1: MemoryUpdate (original specification)</li> </ul> <p>arXiv: Generalized Reinforcement Learning: Experience Particles, Action Operator, Reinforcement Field, Memory Association, and Decision Concepts</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/","title":"Chapter 06a: Advanced Memory Dynamics (Supplement)","text":"<p>Prerequisites: Chapter 06: MemoryUpdate Status: Practical guide to principled memory management Reading time: ~45 minutes</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#overview","title":"Overview","text":"<p>In Chapter 06, we learned Algorithm 1: MemoryUpdate from the original paper. It uses a hard threshold \\(\\tau\\) for experience association:</p> \\[\\text{Associate particles } i \\text{ and } j \\text{ if } k(z_i, z_j) &gt; \\tau\\] <p>The problem: This threshold is:</p> <ul> <li>Brittle: Sensitive to \\(\\tau\\) choice</li> <li>Global: Same \\(\\tau\\) everywhere (doesn't adapt to density)</li> <li>Manual: Requires tuning, not data-driven</li> </ul> <p>This chapter presents two practical improvements that address these limitations:</p> <ol> <li>Top-k Adaptive Neighbors: Density-aware, no global threshold</li> <li>Surprise-Gated Consolidation: Data-driven, biologically inspired</li> </ol> <p>Both are straightforward to implement and provide immediate benefits over the baseline.</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Why Go Beyond Hard Thresholds?</li> <li>Method 1: Top-k Adaptive Neighbors</li> <li>Method 2: Surprise-Gated Consolidation</li> <li>Combining Both Methods</li> <li>When to Use Which?</li> <li>Implementation Notes</li> <li>Further Reading</li> </ol>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#1-why-go-beyond-hard-thresholds","title":"1. Why Go Beyond Hard Thresholds?","text":""},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#the-hard-threshold-problem","title":"The Hard Threshold Problem","text":"<p>Algorithm 1 uses: <pre><code>for i in range(len(memory)):\n    if kernel(z_new, z_i) &gt; tau:  # Hard threshold\n        memory[i].w += lambda_prop * kernel(z_new, z_i) * w_new\n</code></pre></p> <p>Issues:</p> <p>Issue 1: Density Variation</p> <p>In dense regions (many particles):</p> <ul> <li>Many particles exceed \\(\\tau\\) \u2192 lots of associations</li> <li>Computation expensive</li> <li>Redundant updates</li> </ul> <p>In sparse regions (few particles):</p> <ul> <li>Few particles exceed \\(\\tau\\) \u2192 few associations</li> <li>Under-generalization</li> <li>Slow learning</li> </ul> <p>One \\(\\tau\\) can't handle both!</p> <p>Issue 2: Sensitivity</p> <p>Small changes in \\(\\tau\\) cause large changes in behavior:</p> <ul> <li>\\(\\tau\\) too high: No associations, no generalization</li> <li>\\(\\tau\\) too low: Everything associates, no selectivity</li> <li>\"Just right\" \\(\\tau\\) is different for each environment</li> </ul> <p>Manual tuning is tedious and fragile.</p> <p>Issue 3: Not Data-Driven</p> <p>\\(\\tau\\) doesn't consider:</p> <ul> <li>Prediction error (is this experience surprising?)</li> <li>Particle importance (is this a critical memory?)</li> <li>Learning stage (early vs. late training)</li> </ul> <p>We can do better with adaptive, data-driven criteria.</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#what-makes-a-good-alternative","title":"What Makes a Good Alternative?","text":"<p>Desirable properties:</p> <ol> <li>Adaptive: Adjusts to local density automatically</li> <li>Data-driven: Uses TD-error, novelty, or other signals</li> <li>Simple: Easy to implement and understand</li> <li>Efficient: No significant computational overhead</li> <li>Stable: Robust to hyperparameter choices</li> </ol> <p>The two methods below satisfy these criteria.</p> <p></p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#2-method-1-top-k-adaptive-neighbors","title":"2. Method 1: Top-k Adaptive Neighbors","text":""},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#the-idea","title":"The Idea","text":"<p>Instead of global threshold \\(\\tau\\):</p> <p>Associate each particle with its k nearest neighbors (by kernel similarity)</p> <p>Per-particle threshold: \\(\\tau_i\\) = similarity to \\(k\\)-th nearest neighbor</p> <p>Effect:</p> <ul> <li>Dense regions: High \\(\\tau_i\\) (many close neighbors)</li> <li>Sparse regions: Low \\(\\tau_i\\) (few close neighbors)</li> <li>Self-normalizing across density variations</li> </ul>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#algorithm","title":"Algorithm","text":"<p>For new experience \\((z_{\\text{new}}, w_{\\text{new}})\\):</p> <ol> <li> <p>Compute similarities to all existing particles:    $\\(a_i = k(z_{\\text{new}}, z_i) \\quad \\text{for } i = 1, \\ldots, N\\)$</p> </li> <li> <p>Sort similarities in descending order</p> </li> <li> <p>Select top-k particles (k nearest neighbors)</p> </li> <li> <p>Update only these k particles:    $\\(w_i \\leftarrow w_i + \\lambda_{\\text{prop}} \\cdot a_i \\cdot w_{\\text{new}} \\quad \\text{for } i \\in \\text{top-k}\\)$</p> </li> <li> <p>Add new particle: \\((z_{\\text{new}}, w_{\\text{new}})\\) to memory</p> </li> </ol>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#code-example","title":"Code Example","text":"<pre><code>import numpy as np\n\ndef memory_update_topk(memory, z_new, w_new, kernel, k=5, lambda_prop=0.5):\n    \"\"\"\n    MemoryUpdate with top-k adaptive neighbors.\n\n    Args:\n        memory: List of (z_i, w_i) tuples\n        z_new: New augmented state (s, theta)\n        w_new: New weight (typically reward or TD target)\n        kernel: Kernel function k(z, z')\n        k: Number of neighbors to update\n        lambda_prop: Propagation strength\n\n    Returns:\n        Updated memory\n    \"\"\"\n    if len(memory) == 0:\n        # First particle\n        return [(z_new, w_new)]\n\n    # Compute similarities to all existing particles\n    similarities = []\n    for i, (z_i, w_i) in enumerate(memory):\n        sim = kernel(z_new, z_i)\n        similarities.append((i, sim))\n\n    # Sort by similarity (descending)\n    similarities.sort(key=lambda x: x[1], reverse=True)\n\n    # Select top-k neighbors\n    top_k_indices = [idx for idx, sim in similarities[:k]]\n\n    # Update top-k neighbors\n    for i in top_k_indices:\n        z_i, w_i = memory[i]\n        sim = similarities[i][1]  # Get similarity\n        memory[i] = (z_i, w_i + lambda_prop * sim * w_new)\n\n    # Add new particle\n    memory.append((z_new, w_new))\n\n    return memory\n</code></pre>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#example-1d-navigation","title":"Example: 1D Navigation","text":"<p>Setup:</p> <ul> <li>State space: \\(s \\in [0, 10]\\)</li> <li>Action space: \\(\\theta \\in \\{-1, +1\\}\\) (left/right)</li> <li>Kernel: RBF with bandwidth \\(\\sigma = 1.0\\)</li> </ul> <p>Scenario: Agent explores, creating particles at:</p> <ul> <li>Dense region: \\(s = \\{5.0, 5.1, 5.2, 5.3, 5.4\\}\\) (5 particles)</li> <li>Sparse region: \\(s = \\{1.0, 9.0\\}\\) (2 particles)</li> </ul> <p>New experience: \\(s = 5.25\\), \\(\\theta = +1\\), \\(r = 1.0\\)</p> <p>Hard Threshold (\\(\\tau = 0.5\\)):</p> <p>Particles that exceed \\(\\tau\\):</p> <ul> <li>All 5 particles in dense region (wasteful!)</li> <li>0 particles in sparse region</li> </ul> <p>Top-k (\\(k = 3\\)):</p> <p>Top-3 neighbors (by similarity):</p> <ol> <li>\\(s = 5.2\\) (closest)</li> <li>\\(s = 5.3\\) (second closest)</li> <li>\\(s = 5.1\\) (third closest)</li> </ol> <p>Only these 3 are updated \u2014 efficient and appropriate!</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#advantages","title":"Advantages","text":"<p>\u2705 Density-adaptive: Automatically adjusts to local structure</p> <p>\u2705 No tuning: \\(k\\) is more intuitive than \\(\\tau\\) (number of neighbors)</p> <p>\u2705 Efficient: Fixed computational cost (\\(O(N \\log k)\\) if using heap)</p> <p>\u2705 Stable: Small changes in \\(k\\) don't drastically change behavior</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#choosing-k","title":"Choosing \\(k\\)","text":"<p>Rule of thumb:</p> <ul> <li>\\(k = 5\\): Good default for most environments</li> <li>\\(k = 3\\): More selective (sparse updates)</li> <li>\\(k = 10\\): More diffuse (broad generalization)</li> </ul> <p>Heuristic: \\(k \\approx\\) number of neighbors within typical kernel bandwidth</p> <p>Practical: Try \\(k \\in \\{3, 5, 10\\}\\), pick based on performance</p> <p></p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#3-method-2-surprise-gated-consolidation","title":"3. Method 2: Surprise-Gated Consolidation","text":""},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#the-idea_1","title":"The Idea","text":"<p>Key insight from neuroscience: </p> <ul> <li>Surprising experiences (high prediction error) \u2192 stored distinctly</li> <li>Predictable experiences (low prediction error) \u2192 consolidated into existing memories</li> </ul> <p>In GRL terms:</p> \\[\\text{Surprise}(z_t) = |Q^+(z_t) - y_t|\\] <p>where \\(y_t = r_t + \\gamma \\max_a Q^+(s_{t+1}, a)\\) is the TD target.</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#algorithm_1","title":"Algorithm","text":"<p>For new experience \\((z_t, y_t)\\):</p> <ol> <li> <p>Query current field: \\(\\hat{Q} = Q^+(z_t)\\)</p> </li> <li> <p>Compute surprise: \\(\\text{surprise} = |\\hat{Q} - y_t|\\)</p> </li> <li> <p>Decision:</p> </li> </ol> <p>If surprise \\(&gt; \\tau_{\\text{surprise}}\\): (Novel/unexpected)    - Create new particle: \\((z_t, y_t)\\)    - Do NOT update neighbors</p> <p>If surprise \\(\\leq \\tau_{\\text{surprise}}\\): (Familiar/predictable)    - Update neighbors via association    - Do NOT create new particle (consolidate into existing)</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#why-this-works","title":"Why This Works","text":"<p>High surprise (large TD-error):</p> <ul> <li>Current memory is wrong about this experience</li> <li>Storing distinctly preserves this information for learning</li> <li>Avoids corrupting existing memories</li> </ul> <p>Low surprise (small TD-error):</p> <ul> <li>Current memory is already accurate</li> <li>Consolidating compresses redundant information</li> <li>Saves memory space</li> </ul> <p>Result: Memory grows only when needed (for surprising events)</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#code-example_1","title":"Code Example","text":"<pre><code>def memory_update_surprise_gated(memory, z_new, r_t, s_next, \n                                  kernel, gamma=0.99, \n                                  tau_surprise=0.5, lambda_prop=0.5):\n    \"\"\"\n    MemoryUpdate with surprise-gated consolidation.\n\n    Args:\n        memory: List of (z_i, w_i) tuples\n        z_new: New augmented state (s, theta)\n        r_t: Reward\n        s_next: Next state\n        kernel: Kernel function\n        gamma: Discount factor\n        tau_surprise: Surprise threshold\n        lambda_prop: Propagation strength\n\n    Returns:\n        Updated memory\n    \"\"\"\n    # Compute TD target\n    Q_next = max([query_field(memory, (s_next, a), kernel) \n                  for a in action_space])\n    y_t = r_t + gamma * Q_next\n\n    # Query current estimate\n    Q_current = query_field(memory, z_new, kernel)\n\n    # Compute surprise\n    surprise = abs(Q_current - y_t)\n\n    if surprise &gt; tau_surprise:\n        # High surprise: Store distinctly\n        memory.append((z_new, y_t))\n        print(f\"High surprise ({surprise:.2f}): New particle created\")\n    else:\n        # Low surprise: Consolidate into neighbors\n        for i, (z_i, w_i) in enumerate(memory):\n            sim = kernel(z_new, z_i)\n            if sim &gt; 0.1:  # Small threshold for efficiency\n                memory[i] = (z_i, w_i + lambda_prop * sim * y_t)\n        print(f\"Low surprise ({surprise:.2f}): Consolidated\")\n\n    return memory\n\n\ndef query_field(memory, z, kernel):\n    \"\"\"Query the reinforcement field at z.\"\"\"\n    return sum(w_i * kernel(z, z_i) for z_i, w_i in memory)\n</code></pre>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#example-gridworld-navigation","title":"Example: GridWorld Navigation","text":"<p>Setup:</p> <ul> <li>5\u00d75 GridWorld, goal at (4, 4), reward = +10</li> <li>Agent has seen goal region before (memory exists)</li> </ul> <p>Scenario 1: Agent at (3, 4), moves right, reaches goal</p> <ul> <li>Current estimate: \\(Q^+ \\approx 9.5\\) (accurate)</li> <li>Actual: \\(r + \\gamma Q^+ = 10 + 0 = 10\\)</li> <li>Surprise: \\(|9.5 - 10| = 0.5\\) (low)</li> <li>Action: Consolidate into existing memories</li> </ul> <p>Scenario 2: Agent discovers shortcut (new optimal path)</p> <ul> <li>Current estimate: \\(Q^+ \\approx 2.0\\) (outdated)</li> <li>Actual: \\(r + \\gamma Q^+ = 0 + 0.99 \\times 9.5 = 9.4\\) (much better!)</li> <li>Surprise: \\(|2.0 - 9.4| = 7.4\\) (high!)</li> <li>Action: Create new particle (preserve discovery)</li> </ul>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#advantages_1","title":"Advantages","text":"<p>\u2705 Data-driven: Uses TD-error, not manual threshold</p> <p>\u2705 Memory-efficient: Only grows when necessary</p> <p>\u2705 Biologically plausible: Mirrors human memory consolidation</p> <p>\u2705 Exploration-friendly: High-surprise regions naturally explored more</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#choosing-tau_textsurprise","title":"Choosing \\(\\tau_{\\text{surprise}}\\)","text":"<p>Rule of thumb:</p> <ul> <li>\\(\\tau_{\\text{surprise}} = 0.5 \\times \\text{reward scale}\\)</li> <li>If rewards are \\(\\in [0, 10]\\): try \\(\\tau_{\\text{surprise}} = 0.5\\)</li> <li>If rewards are \\(\\in [-1, +1]\\): try \\(\\tau_{\\text{surprise}} = 0.1\\)</li> </ul> <p>Heuristic: Start high (store most things), decrease over time (consolidate more)</p> <p>Adaptive: Use running average of TD-errors to set \\(\\tau_{\\text{surprise}}\\) dynamically</p> <p></p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#4-combining-both-methods","title":"4. Combining Both Methods","text":"<p>Best of both worlds: Use surprise-gating for formation, top-k for propagation.</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#hybrid-algorithm","title":"Hybrid Algorithm","text":"<pre><code>def memory_update_hybrid(memory, z_new, r_t, s_next, kernel,\n                          k=5, gamma=0.99, tau_surprise=0.5, \n                          lambda_prop=0.5):\n    \"\"\"\n    Hybrid MemoryUpdate: surprise-gating + top-k.\n\n    Formation: Surprise-gated (create new particle if surprise &gt; tau)\n    Propagation: Top-k neighbors (update k nearest)\n    \"\"\"\n    # Compute TD target\n    Q_next = max([query_field(memory, (s_next, a), kernel) \n                  for a in action_space])\n    y_t = r_t + gamma * Q_next\n\n    # Query current estimate\n    Q_current = query_field(memory, z_new, kernel)\n    surprise = abs(Q_current - y_t)\n\n    # === FORMATION (surprise-gated) ===\n    should_create_new = (surprise &gt; tau_surprise) or (len(memory) == 0)\n\n    # === PROPAGATION (top-k) ===\n    if len(memory) &gt; 0:\n        # Compute similarities\n        similarities = [(i, kernel(z_new, z_i)) \n                        for i, (z_i, w_i) in enumerate(memory)]\n        similarities.sort(key=lambda x: x[1], reverse=True)\n\n        # Update top-k neighbors\n        for i, sim in similarities[:k]:\n            z_i, w_i = memory[i]\n            memory[i] = (z_i, w_i + lambda_prop * sim * y_t)\n\n    # === ADD NEW (if needed) ===\n    if should_create_new:\n        memory.append((z_new, y_t))\n\n    return memory\n</code></pre>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#why-this-combination-works","title":"Why This Combination Works","text":"<p>Surprise-gating (formation):</p> <ul> <li>Controls when to add particles</li> <li>Memory grows only for important experiences</li> <li>Bounded memory growth</li> </ul> <p>Top-k (propagation):</p> <ul> <li>Controls how to update neighbors</li> <li>Efficient, density-adaptive</li> <li>Stable generalization</li> </ul> <p>Together:</p> <ul> <li>Selective formation + efficient propagation</li> <li>Bounded memory + good generalization</li> <li>Works across diverse environments</li> </ul> <p></p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#5-when-to-use-which","title":"5. When to Use Which?","text":""},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#method-comparison","title":"Method Comparison","text":"Method Pros Cons Best For Hard Threshold (baseline) Simple, interpretable Brittle, not adaptive Initial prototypes, uniform density Top-k Adaptive Density-adaptive, stable Still grows unbounded Variable density environments Surprise-Gating Memory-efficient, data-driven Requires TD-error Online learning, lifelong tasks Hybrid (Top-k + Surprise) Best of both Slightly more complex Production systems, resource-constrained"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#decision-tree","title":"Decision Tree","text":"<pre><code>Are you resource-constrained (memory/compute)?\n\u251c\u2500 YES: Use Surprise-Gating or Hybrid\n\u2514\u2500 NO: \n    \u2502\n    Is your environment density-varying?\n    \u251c\u2500 YES: Use Top-k or Hybrid\n    \u2514\u2500 NO: Hard threshold is fine (keep it simple)\n</code></pre>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#recommendations-by-use-case","title":"Recommendations by Use Case","text":"<p>Prototyping / Research:</p> <ul> <li>Start with Hard Threshold (baseline)</li> <li>If memory grows too large \u2192 add Surprise-Gating</li> <li>If performance varies by region \u2192 add Top-k</li> </ul> <p>Production / Robotics:</p> <ul> <li>Use Hybrid (surprise + top-k)</li> <li>Bounded memory, stable performance</li> <li>Tune \\(k\\) and \\(\\tau_{\\text{surprise}}\\) on validation set</li> </ul> <p>Lifelong Learning:</p> <ul> <li>Use Surprise-Gating (required for bounded memory)</li> <li>Consider adding Decay (old particles fade)</li> <li>Periodically Prune low-importance particles</li> </ul> <p></p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#6-implementation-notes","title":"6. Implementation Notes","text":""},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#computational-complexity","title":"Computational Complexity","text":"Method Per-Update Cost Memory Cost Hard Threshold \\(O(N)\\) \\(O(N)\\) (unbounded) Top-k \\(O(N + k \\log k)\\) \\(O(N)\\) (unbounded) Surprise-Gating \\(O(N)\\) \\(O(N_{\\text{surprise}})\\) (bounded!) Hybrid \\(O(N + k \\log k)\\) \\(O(N_{\\text{surprise}})\\) (bounded) <p>Note: All methods are \\(O(N)\\) in existing memory size. Top-k adds small \\(O(k \\log k)\\) overhead for sorting.</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#optimization-tricks","title":"Optimization Tricks","text":"<p>1. Efficient Top-k with Heap</p> <p>Instead of sorting all similarities:</p> <pre><code>import heapq\n\ndef get_topk_neighbors(similarities, k):\n    \"\"\"Get top-k largest similarities efficiently.\"\"\"\n    # Use max-heap (negate values for min-heap)\n    return heapq.nlargest(k, similarities, key=lambda x: x[1])\n</code></pre> <p>Reduces sorting from \\(O(N \\log N)\\) to \\(O(N + k \\log k)\\).</p> <p>2. Adaptive \\(\\tau_{\\text{surprise}}\\)</p> <p>Use running statistics:</p> <pre><code>class AdaptiveSurpriseThreshold:\n    def __init__(self, initial_tau=0.5, percentile=75):\n        self.tau = initial_tau\n        self.td_errors = []\n        self.percentile = percentile\n\n    def update(self, td_error):\n        \"\"\"Update threshold based on recent TD-errors.\"\"\"\n        self.td_errors.append(abs(td_error))\n        if len(self.td_errors) &gt; 100:\n            self.td_errors.pop(0)  # Keep last 100\n\n        # Set tau to 75th percentile of recent TD-errors\n        self.tau = np.percentile(self.td_errors, self.percentile)\n\n    def should_create_new(self, surprise):\n        return surprise &gt; self.tau\n</code></pre> <p>Effect: \\(\\tau_{\\text{surprise}}\\) adapts to environment scale automatically.</p> <p>3. Periodic Pruning</p> <p>Prevent unbounded growth (even with surprise-gating):</p> <pre><code>def prune_memory(memory, max_size=1000, kernel=None):\n    \"\"\"Remove least important particles if memory exceeds max_size.\"\"\"\n    if len(memory) &lt;= max_size:\n        return memory\n\n    # Compute importance scores (e.g., weight magnitude)\n    importance = [(i, abs(w_i)) for i, (z_i, w_i) in enumerate(memory)]\n    importance.sort(key=lambda x: x[1], reverse=True)\n\n    # Keep top max_size particles\n    keep_indices = set(i for i, score in importance[:max_size])\n    memory = [p for i, p in enumerate(memory) if i in keep_indices]\n\n    return memory\n</code></pre> <p>When to prune: After every episode or every N steps.</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#hyperparameter-guidelines","title":"Hyperparameter Guidelines","text":"Parameter Default Range Notes \\(k\\) (top-k) 5 3-10 Higher \u2192 more generalization \\(\\tau_{\\text{surprise}}\\) 0.5 0.1-1.0 Scale with reward magnitude \\(\\lambda_{\\text{prop}}\\) 0.5 0.1-1.0 Same as baseline \\(\\gamma\\) (discount) 0.99 0.9-0.999 Task-dependent <p>Meta-learning: Learn these from distribution of tasks (see Chapter 08 for details).</p> <p></p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#7-further-reading","title":"7. Further Reading","text":""},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#within-this-tutorial-series","title":"Within This Tutorial Series","text":"<p>Part I Tutorials: - Chapter 05: Particle Memory \u2014 Foundations of particle representation - Chapter 06: MemoryUpdate \u2014 Algorithm 1 baseline</p> <p>Quantum-Inspired Extensions (for full theory):</p> <ul> <li>Chapter 07: Learning Beyond GP \u2014 Alternative learning mechanisms (online SGD, sparse methods, MoE, neural networks)</li> <li>Chapter 08: Memory Dynamics \u2014 Formation, consolidation, retrieval operators with full theoretical treatment</li> </ul>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#additional-methods-not-covered-here","title":"Additional Methods Not Covered Here","text":"<p>For even more advanced approaches, see Chapter 08:</p> <p>Soft Association (no threshold):</p> <ul> <li>Temperature-controlled: \\(\\alpha_{ij} = \\text{softmax}(\\gamma k(z_i, z_j))\\)</li> <li>Differentiable, smooth</li> </ul> <p>MDL Consolidation:</p> <ul> <li>Minimize: \\(\\text{TD-error}(Q^+) + \\lambda |\\Omega|\\)</li> <li>Principled compression via information theory</li> <li>More complex to implement</li> </ul> <p>Memory Type Tags:</p> <ul> <li>Factual (never forget)</li> <li>Experiential (normal decay)</li> <li>Working (fast decay)</li> </ul>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#related-literature","title":"Related Literature","text":"<p>Adaptive Memory in RL: - Schaul et al. (2015). \"Prioritized Experience Replay.\" ICLR. - Isele &amp; Cosgun (2018). \"Selective Experience Replay for Lifelong Learning.\" AAAI.</p> <p>Neuroscience of Memory: - McClelland et al. (1995). \"Why There Are Complementary Learning Systems in the Hippocampus and Neocortex.\" - Dudai (2004). \"The Neurobiology of Consolidations, Or, How Stable Is the Engram?\" Annual Review of Psychology.</p> <p>Information-Theoretic Learning: - Rissanen (1978). \"Modeling by Shortest Data Description.\" Automatica.</p>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#summary","title":"Summary","text":""},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Hard thresholds are brittle \u2014 density-adaptive and data-driven alternatives are better</p> </li> <li> <p>Top-k Adaptive Neighbors \u2014 Simple, practical improvement</p> </li> <li>No global threshold</li> <li>Self-normalizing across density</li> <li> <p>\\(k = 5\\) is a good default</p> </li> <li> <p>Surprise-Gated Consolidation \u2014 Data-driven memory growth</p> </li> <li>High TD-error \u2192 new particle</li> <li>Low TD-error \u2192 consolidate</li> <li> <p>Bounded memory growth</p> </li> <li> <p>Hybrid approach is best for production \u2014 combines both benefits</p> </li> <li> <p>Implementation is straightforward \u2014 code examples provided, minimal overhead</p> </li> </ol>"},{"location":"GRL0/tutorials/06a-advanced-memory-dynamics/#next-steps","title":"Next Steps","text":"<p>Immediate:</p> <ul> <li> Replace hard threshold in your MemoryUpdate with top-k</li> <li> Measure memory growth and performance</li> <li> Compare to baseline (Algorithm 1)</li> </ul> <p>Advanced:</p> <ul> <li> Implement surprise-gating</li> <li> Add adaptive \\(\\tau_{\\text{surprise}}\\)</li> <li> Periodic pruning for long-running agents</li> </ul> <p>Theoretical Depth:</p> <ul> <li> Read Chapter 08 for operator formalism</li> <li> Explore MDL consolidation</li> <li> Study memory type differentiation</li> </ul> <p>Last Updated: January 14, 2026 Next: Chapter 07: RF-SARSA Algorithm (planned)</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/","title":"Chapter 7: RF-SARSA (Functional TD Learning)","text":"<p>Purpose: Understand how GRL learns the reinforcement field through temporal-difference updates Prerequisites: Chapters 4-6 (Reinforcement Field, Particle Memory, MemoryUpdate) Key Concepts: RF-SARSA algorithm, two-layer learning, functional TD, field reshaping, belief-conditioned control</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#introduction","title":"Introduction","text":"<p>We've built up the conceptual foundations:</p> <ul> <li>The reinforcement field \\(Q^+(z)\\) as a functional object in RKHS (Chapter 4)</li> <li>Experience particles as basis elements (Chapter 5)</li> <li>MemoryUpdate as belief evolution (Chapter 6)</li> </ul> <p>But we haven't yet addressed the fundamental question: How does the agent learn \\(Q^+\\) from experience?</p> <p>This chapter introduces RF-SARSA (Reinforcement Field SARSA), the core learning algorithm. The name might suggest \"SARSA with kernels,\" but this would be misleading. RF-SARSA is fundamentally different:</p> Classical SARSA RF-SARSA (GRL) Updates Scalar \\(Q(s,a)\\) values Particle weights defining global functional field Learning target State-action value Geometry of the entire landscape Policy \\(\\arg\\max_a Q(s,a)\\) Field-based inference via energy minimization Generalization Via function approximation Geometric propagation through kernels <p>Core insight: RF-SARSA doesn't learn Q-values\u2014it reshapes the energy landscape from which actions are inferred.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#1-what-makes-rf-sarsa-different","title":"1. What Makes RF-SARSA Different?","text":""},{"location":"GRL0/tutorials/07-rf-sarsa/#11-classical-sarsa-a-reminder","title":"1.1 Classical SARSA: A Reminder","text":"<p>Standard SARSA (State-Action-Reward-State-Action) updates Q-values using the temporal difference (TD) error:</p> \\[Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[r + \\gamma Q(s', a') - Q(s, a)\\right]\\] <p>Interpretation:</p> <ul> <li>Target: \\(r + \\gamma Q(s', a')\\) (one-step return using actual next action)</li> <li>Error: Difference between target and current estimate</li> <li>Update: Move current estimate toward target</li> </ul> <p>Key property: On-policy learning (learns about the policy being executed).</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#12-rf-sarsa-functional-td-in-rkhs","title":"1.2 RF-SARSA: Functional TD in RKHS","text":"<p>RF-SARSA maintains the TD learning principle but applies it to basis functions in RKHS, not to table entries.</p> <p>What changes:</p> <ol> <li>Primary updates: Particle weights \\(w_i\\) that define the field \\(Q^+(z) = \\sum_i w_i k(z, z_i)\\)</li> <li>Geometric propagation: TD signal affects not just one location but a neighborhood (via kernel)</li> <li>Belief representation: The field \\(Q^+\\) (equivalently, particle memory \\(\\Omega\\)) is the agent's state</li> <li>Action inference: Policy emerges from field queries, not direct optimization</li> </ol> <p>Critical point: RF-SARSA is not learning a policy\u2014it's reshaping a landscape from which policy is inferred.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#13-two-learning-processes-at-different-levels","title":"1.3 Two Learning Processes at Different Levels","text":"<p>RF-SARSA couples two learning processes:</p> <p>Primitive Layer (Discrete):</p> <ul> <li>Maintains \\(Q(s, a)\\) estimates over discrete state-action pairs</li> <li>Uses standard SARSA updates</li> <li>Provides temporal grounding (immediate feedback from environment)</li> <li>Supplies reinforcement signals (TD errors)</li> </ul> <p>Field Layer (Continuous):</p> <ul> <li>Generalizes estimates over continuous augmented space \\((s, \\theta)\\)</li> <li>Uses Gaussian Process Regression (GPR) for interpolation</li> <li>Performs policy inference exclusively</li> <li>Receives reinforcement through experience particles</li> </ul> <p>Key relationship:</p> <ul> <li>Primitive layer \u2192 generates TD evidence</li> <li>Field layer \u2192 performs action inference</li> <li>MemoryUpdate \u2192 connects them (Algorithm 1)</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#2-the-rf-sarsa-algorithm-informal-overview","title":"2. The RF-SARSA Algorithm (Informal Overview)","text":"<p>Before the formal specification, here's the conceptual flow:</p> <p>Initialization:</p> <ol> <li>Initialize kernel hyperparameters \\(\\theta\\) (e.g., via ARD on initial particles)</li> <li>Initialize primitive \\(Q(s, a)\\) arbitrarily</li> <li>Initialize particle memory \\(\\Omega\\) (possibly empty)</li> </ol> <p>Each episode:</p> <ol> <li>Start in state \\(s_0\\)</li> </ol> <p>Each step:</p> <ol> <li> <p>Field-based action inference:</p> </li> <li> <p>For each candidate action \\(a^{(i)}\\), form augmented state \\(z^{(i)} = (s, a^{(i)})\\)</p> </li> <li>Query field: \\(Q^+(z^{(i)}) = \\mathbb{E}[Q^+ \\mid \\Omega, k, \\theta]\\) via GPR</li> <li> <p>Select action via policy: \\(a \\sim \\pi(a \\mid s)\\) based on \\(Q^+\\) values</p> </li> <li> <p>Environment interaction:</p> </li> <li> <p>Execute \\(a\\), observe reward \\(r\\) and next state \\(s'\\)</p> </li> <li> <p>Next action inference:</p> </li> <li> <p>Repeat step 2 for state \\(s'\\) to select \\(a'\\)</p> </li> <li> <p>Primitive SARSA update:</p> </li> <li> <p>Compute TD error: \\(\\delta = r + \\gamma Q(s', a') - Q(s, a)\\)</p> </li> <li> <p>Update: \\(Q(s, a) \\leftarrow Q(s, a) + \\alpha \\delta\\)</p> </li> <li> <p>Particle reinforcement (Algorithm 1: MemoryUpdate):</p> </li> <li> <p>Form particle: \\(\\omega = (z, Q(s, a))\\) where \\(z = (s, a)\\)</p> </li> <li> <p>Update memory: \\(\\Omega \\leftarrow \\text{MemoryUpdate}(\\omega, \\delta, k, \\tau, \\Omega)\\)</p> </li> <li> <p>Advance: \\(s \\leftarrow s'\\), \\(a \\leftarrow a'\\)</p> </li> </ol> <p>Periodic:</p> <ul> <li>Every \\(T\\) steps, re-estimate kernel hyperparameters \\(\\theta\\) via ARD</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#3-formal-specification","title":"3. Formal Specification","text":""},{"location":"GRL0/tutorials/07-rf-sarsa/#31-notation","title":"3.1 Notation","text":"<p>Environment:</p> <ul> <li>\\(s \\in \\mathcal{S}\\): primitive environment state</li> <li>\\(a^{(i)} \\in \\mathcal{A}\\): discrete action (\\(i = 1, \\ldots, n\\))</li> <li>\\(r \\in \\mathbb{R}\\): reward</li> <li>\\(\\gamma \\in [0, 1]\\): discount factor</li> </ul> <p>Parametric action representation:</p> <ul> <li>\\(M\\): parametric action model</li> <li>\\(f_{A^+}: a^{(i)} \\mapsto x_a^{(i)} \\in \\mathbb{R}^{d_a}\\): action encoder</li> <li>\\(f_A: x_a \\mapsto a\\): action decoder</li> </ul> <p>Augmented space:</p> <ul> <li>\\(x_s \\in \\mathbb{R}^{d_s}\\): state features</li> <li>\\(x_a \\in \\mathbb{R}^{d_a}\\): action parameters</li> <li>\\(z = (x_s, x_a) \\in \\mathbb{R}^{d_s + d_a}\\): augmented state-action point</li> </ul> <p>Value functions:</p> <ul> <li>\\(Q(s, a)\\): primitive action-value (base SARSA learner)</li> <li>\\(Q^+(z)\\): field-based value estimate (via GPR on \\(\\Omega\\))</li> <li>\\(E(z) := -Q^+(z)\\): energy (lower is better)</li> </ul> <p>Memory and kernel:</p> <ul> <li>\\(\\Omega = \\{(z_i, q_i)\\}_{i=1}^N\\): particle memory (GPR training set)</li> <li>\\(k(z, z'; \\theta)\\): kernel function with hyperparameters \\(\\theta\\)</li> <li>\\(\\tau \\in [0, 1]\\): association threshold</li> </ul> <p>Parameters:</p> <ul> <li>\\(\\alpha\\): SARSA learning rate</li> <li>\\(T\\): ARD update period</li> <li>\\(\\beta\\): policy temperature (for Boltzmann policy)</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#32-algorithm-rf-sarsa","title":"3.2 Algorithm: RF-SARSA","text":"<p>Inputs:</p> <ul> <li>Kernel function \\(k(\\cdot, \\cdot; \\theta)\\)</li> <li>Parametric action model \\(M\\)</li> <li>ARD update period \\(T\\)</li> <li>Initial particle memory \\(\\Omega_0\\)</li> <li>Association threshold \\(\\tau\\)</li> <li>Learning rate \\(\\alpha\\), discount \\(\\gamma\\), policy temperature \\(\\beta\\)</li> </ul> <p>Initialization:</p> <ol> <li> <p>Estimate kernel hyperparameters:</p> </li> <li> <p>If \\(\\Omega_0 \\neq \\emptyset\\): Run ARD on \\(\\Omega_0\\) to get \\(\\theta\\)</p> </li> <li> <p>Else: Initialize \\(\\theta = \\theta_0\\) from prior</p> </li> <li> <p>Initialize primitive Q-function:</p> </li> <li> <p>\\(Q(s, a) \\leftarrow 0\\) for all \\((s, a)\\) (or small random values)</p> </li> <li> <p>Initialize particle memory:</p> </li> <li> <p>\\(\\Omega \\leftarrow \\Omega_0\\)</p> </li> <li> <p>Initialize step counter:</p> </li> <li> <p>\\(t_{\\text{ARD}} \\leftarrow 0\\)</p> </li> </ol> <p>For each episode:</p> <ol> <li> <p>Initialize state:</p> </li> <li> <p>Observe initial state \\(s_0\\)</p> </li> <li> <p>Set \\(s \\leftarrow s_0\\)</p> </li> <li> <p>Initial action selection:</p> </li> <li> <p>For each \\(a^{(i)} \\in \\mathcal{A}\\):</p> <ul> <li>Encode: \\(x_a^{(i)} \\leftarrow f_{A^+}(a^{(i)})\\)</li> <li>Form augmented: \\(z^{(i)} \\leftarrow (x_s(s), x_a^{(i)})\\)</li> <li>Field query: \\(Q^+(z^{(i)}) \\leftarrow \\text{GPR-Predict}(z^{(i)}; \\Omega, k, \\theta)\\)</li> <li>Policy: \\(a \\sim \\pi(\\cdot \\mid s)\\) where \\(\\pi(a^{(i)} \\mid s) \\propto \\exp(\\beta Q^+(z^{(i)}))\\)</li> </ul> </li> </ol> <p>For each step of episode:</p> <ol> <li> <p>Periodic kernel hyperparameter update:</p> </li> <li> <p>If \\(t_{\\text{ARD}} \\bmod T = 0\\):</p> <ul> <li>Re-run ARD on \\(\\Omega\\) to update \\(\\theta\\)</li> <li>(Optional) Increase \\(T\\) to reduce update frequency over time</li> <li>\\(t_{\\text{ARD}} \\leftarrow t_{\\text{ARD}} + 1\\)</li> </ul> </li> <li> <p>Environment interaction:</p> </li> <li> <p>Execute action \\(a\\)</p> </li> <li> <p>Observe reward \\(r\\) and next state \\(s'\\)</p> </li> <li> <p>Next action inference (field query):</p> </li> <li> <p>For each \\(a'^{(j)} \\in \\mathcal{A}\\):</p> <ul> <li>Encode: \\(x_a'^{(j)} \\leftarrow f_{A^+}(a'^{(j)})\\)</li> <li>Form augmented: \\(z'^{(j)} \\leftarrow (x_s(s'), x_a'^{(j)})\\)</li> <li>Field query: \\(Q^+(z'^{(j)}) \\leftarrow \\text{GPR-Predict}(z'^{(j)}; \\Omega, k, \\theta)\\)</li> <li>Policy: \\(a' \\sim \\pi(\\cdot \\mid s')\\) where \\(\\pi(a'^{(j)} \\mid s') \\propto \\exp(\\beta Q^+(z'^{(j)}))\\)</li> </ul> </li> <li> <p>Primitive SARSA update (generates TD evidence):</p> <ul> <li>Compute TD error:   $\\(\\delta \\leftarrow r + \\gamma Q(s', a') - Q(s, a)\\)$</li> <li>Update primitive Q-function:   $\\(Q(s, a) \\leftarrow Q(s, a) + \\alpha \\delta\\)$</li> <li>Store updated value: \\(q \\leftarrow Q(s, a)\\)</li> </ul> </li> <li> <p>Particle reinforcement (Algorithm 1: MemoryUpdate):</p> <ul> <li>Form experience particle:   $\\(\\omega \\leftarrow (z, q) \\quad \\text{where} \\quad z = (x_s(s), f_{A^+}(a))\\)$</li> <li>Update particle memory:   $\\(\\Omega \\leftarrow \\text{MemoryUpdate}(\\omega, \\delta, k, \\tau, \\Omega)\\)$   (See Chapter 6 for MemoryUpdate details)</li> </ul> </li> <li> <p>State-action transition:</p> <ul> <li>\\(s \\leftarrow s'\\), \\(a \\leftarrow a'\\)</li> </ul> </li> <li> <p>Termination check:</p> <ul> <li>If \\(s\\) is terminal, end episode</li> <li>Else, return to step 7</li> </ul> </li> </ol>"},{"location":"GRL0/tutorials/07-rf-sarsa/#33-helper-function-gpr-predict","title":"3.3 Helper Function: GPR-Predict","text":"<p>Gaussian Process Regression prediction:</p> <p>Given particle memory \\(\\Omega = \\{(z_i, q_i)\\}_{i=1}^N\\), kernel \\(k\\), and hyperparameters \\(\\theta\\), predict the field value at query point \\(z\\):</p> \\[Q^+(z) = \\sum_{i=1}^N \\alpha_i k(z, z_i; \\theta)\\] <p>where coefficients \\(\\alpha = (\\alpha_1, \\ldots, \\alpha_N)^\\top\\) solve:</p> \\[(K + \\sigma_n^2 I) \\alpha = q\\] <p>with:</p> <ul> <li>\\(K_{ij} = k(z_i, z_j; \\theta)\\): kernel matrix</li> <li>\\(q = (q_1, \\ldots, q_N)^\\top\\): stored values</li> <li>\\(\\sigma_n^2\\): noise variance (hyperparameter)</li> </ul> <p>In practice: Use efficient GPR libraries (e.g., GPyTorch, scikit-learn's <code>GaussianProcessRegressor</code>).</p> <p>Computational note: For large \\(N\\), use sparse GP approximations (e.g., inducing points, random features).</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#4-how-rf-sarsa-works-the-three-forces","title":"4. How RF-SARSA Works: The Three Forces","text":"<p>RF-SARSA succeeds by balancing three forces:</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#41-temporal-credit-assignment-primitive-sarsa","title":"4.1 Temporal Credit Assignment (Primitive SARSA)","text":"<p>Role: Ground the field in actual experienced returns.</p> <p>Mechanism: SARSA provides temporally accurate value estimates:</p> <ul> <li>TD error \\(\\delta = r + \\gamma Q(s', a') - Q(s, a)\\) measures prediction error</li> <li>Bootstrapping from \\(Q(s', a')\\) propagates future returns backward</li> <li>On-policy learning ensures values reflect the behavior policy</li> </ul> <p>Why this matters: Without temporal grounding, the field would have no connection to true returns\u2014it would generalize nonsense.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#42-geometric-generalization-gpr","title":"4.2 Geometric Generalization (GPR)","text":"<p>Role: Spread value information across similar configurations.</p> <p>Mechanism: Kernel similarity defines \"nearness\":</p> <ul> <li>\\(k(z, z')\\) large \u2192 \\(z\\) and \\(z'\\) are similar \u2192 should have similar \\(Q^+\\) values</li> <li>GP regression interpolates smoothly between particles</li> <li>Predictions come with uncertainty estimates (\\(\\sigma^2(z)\\))</li> </ul> <p>Why this matters: The agent visits a tiny fraction of augmented space\u2014generalization is essential for learning.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#43-adaptive-geometry-ard","title":"4.3 Adaptive Geometry (ARD)","text":"<p>Role: Learn which dimensions matter.</p> <p>Mechanism: Automatic Relevance Determination (ARD) adjusts kernel lengthscales:</p> <ul> <li>Large lengthscale \u2192 dimension is irrelevant \u2192 smooth over it</li> <li>Small lengthscale \u2192 dimension is critical \u2192 pay attention to variations</li> </ul> <p>Example: In a reaching task, gripper orientation might be irrelevant initially but critical when grasping\u2014ARD adapts.</p> <p>Why this matters: The agent doesn't know a priori which action parameters are important\u2014ARD discovers this from data.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#44-the-virtuous-cycle","title":"4.4 The Virtuous Cycle","text":"<p>These three forces create a virtuous cycle:</p> <pre><code>1. SARSA updates primitive Q(s,a) using TD \u2192 accurate local estimates\n2. Particles (z, Q(s,a)) added to \u03a9 \u2192 new training data\n3. GPR generalizes across \u03a9 \u2192 smooth field Q+(z)\n4. Policy queries Q+(z) \u2192 explores intelligently\n5. New experiences \u2192 refine SARSA estimates\n6. ARD adapts kernel \u2192 focuses on relevant dimensions\n\u2192 Loop back to step 1\n</code></pre> <p>Key insight: The field \\(Q^+\\) is not optimized\u2014it emerges from the interaction of these forces.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#5-connection-to-the-principle-of-least-action","title":"5. Connection to the Principle of Least Action","text":"<p>With Chapter 03a fresh in mind, we can now see RF-SARSA through the lens of physics.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#51-rf-sarsa-as-action-minimization","title":"5.1 RF-SARSA as Action Minimization","text":"<p>Recall from Chapter 03a that optimal trajectories minimize the action functional:</p> \\[S[\\tau] = \\int_0^T \\left[E(z_t) + \\frac{1}{2\\lambda}\\|\\dot{z}_t\\|^2\\right] dt\\] <p>RF-SARSA implements this:</p> <ol> <li>Energy term: \\(E(z) = -Q^+(z)\\) is learned via TD updates</li> <li>High \\(Q^+\\) \u2192 low energy \u2192 good configuration</li> <li> <p>SARSA signals reshape \\(Q^+\\) to reflect actual returns</p> </li> <li> <p>Kinetic term: \\(\\frac{1}{2\\lambda}\\|\\dot{z}\\|^2\\) is implicitly encoded by the kernel</p> </li> <li>Smooth kernels (e.g., RBF) penalize rapid changes</li> <li>Particles propagate weights to neighbors (Algorithm 1: MemoryUpdate)</li> <li> <p>Result: Smooth \\(Q^+\\) landscape</p> </li> <li> <p>Policy: Boltzmann distribution \\(\\pi \\propto \\exp(Q^+/\\lambda)\\) minimizes expected action</p> </li> <li>Temperature \\(\\lambda\\) controls exploration</li> <li>Sampling via Langevin dynamics (gradient flow on \\(Q^+\\))</li> </ol> <p>The deep connection: RF-SARSA is not an ad-hoc algorithm\u2014it's learning the energy landscape from which the least-action principle determines optimal trajectories!</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#52-why-this-matters-for-learning","title":"5.2 Why This Matters for Learning","text":"<p>Smoothness as regularization:</p> <ul> <li>RF-SARSA favors smooth \\(Q^+\\) fields (via kernel smoothness)</li> <li>Equivalent to penalizing rapid action changes (kinetic term)</li> <li>Natural Occam's razor: simple policies preferred</li> </ul> <p>Physical intuition:</p> <ul> <li>Particles are like \"mass distributions\" creating a potential landscape</li> <li>Agent follows \"trajectories\" that minimize action</li> <li>MemoryUpdate reshapes the landscape based on experience</li> </ul> <p>Modern perspective:</p> <ul> <li>This is energy-based learning (Chapter 3)</li> <li>Policy emerges from gradient flow on learned energy (Langevin dynamics)</li> <li>RF-SARSA anticipates modern score-based / diffusion-based RL methods</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#6-worked-example-1d-navigation-continued","title":"6. Worked Example: 1D Navigation (Continued)","text":"<p>Let's extend the 1D navigation example from Chapter 6 to show how RF-SARSA learns.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#61-problem-setup","title":"6.1 Problem Setup","text":"<p>Environment:</p> <ul> <li>State \\(s \\in [0, 10]\\): position on a line</li> <li>Goal: \\(s = 10\\) (reward \\(r = +10\\))</li> <li>Obstacle: region \\([4, 6]\\) (reward \\(r = -5\\) if entered)</li> <li>Actions: \\(a \\in \\{\\text{left}, \\text{right}\\}\\) with parametric \"step size\" \\(\\theta \\in [0, 1]\\)</li> <li>left: \\(s' = s - 2\\theta\\)</li> <li>right: \\(s' = s + 2\\theta\\)</li> </ul> <p>Augmented space: \\(z = (s, \\theta) \\in [0, 10] \\times [0, 1]\\)</p> <p>Kernel: RBF kernel \\(k(z, z') = \\exp(-\\|z - z'\\|^2 / (2\\ell^2))\\) with lengthscale \\(\\ell = 0.5\\)</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#62-initial-state-episode-1-step-1","title":"6.2 Initial State (Episode 1, Step 1)","text":"<p>Agent at \\(s = 2\\):</p> <p>Primitive Q-function (initialized to zero):</p> <ul> <li>\\(Q(2, \\text{left}) = 0\\)</li> <li>\\(Q(2, \\text{right}) = 0\\)</li> </ul> <p>Particle memory: \\(\\Omega = \\emptyset\\) (no particles yet)</p> <p>Field prediction (no particles \u2192 default prior mean = 0):</p> <ul> <li>\\(Q^+(2, \\text{left}, 0.5) = 0\\)</li> <li>\\(Q^+(2, \\text{right}, 0.5) = 0\\)</li> </ul> <p>Action selection (random, since all Q-values equal):</p> <ul> <li>Select \\(a = \\text{right}\\), \\(\\theta = 0.5\\)</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#63-first-experience","title":"6.3 First Experience","text":"<p>Execute action:</p> <ul> <li>\\(s' = 2 + 2(0.5) = 3\\), \\(r = 0\\) (no reward yet)</li> </ul> <p>Next action (random again):</p> <ul> <li>\\(a' = \\text{right}\\), \\(\\theta' = 0.5\\)</li> </ul> <p>SARSA update (assume \\(\\gamma = 0.9\\), \\(\\alpha = 0.1\\)): $\\(\\delta = 0 + 0.9 \\cdot 0 - 0 = 0\\)$ $\\(Q(2, \\text{right}) = 0 + 0.1 \\cdot 0 = 0\\)$</p> <p>Particle: \\(\\omega = ((2, 0.5), 0)\\) (augmented state, updated Q-value)</p> <p>MemoryUpdate:</p> <ul> <li>\\(\\Omega\\) is empty \u2192 create new particle</li> <li>\\(\\Omega = \\{((2, 0.5), 0)\\}\\)</li> </ul> <p>Field now: \\(Q^+(z) = 0 \\cdot k(z, (2, 0.5))\\) (still zero, but structure is building)</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#64-after-many-steps-goal-reached","title":"6.4 After Many Steps: Goal Reached","text":"<p>Episode 1 concludes: Agent reaches \\(s = 10\\) after 5 steps, receives \\(r = +10\\).</p> <p>Final step SARSA update: $\\(\\delta = 10 + 0 - 0 = 10 \\quad (\\text{terminal, so } Q(s', a') = 0)\\)$ $\\(Q(9, \\text{right}) = 0 + 0.1 \\cdot 10 = 1.0\\)$</p> <p>Particle: \\(\\omega = ((9, 0.5), 1.0)\\)</p> <p>MemoryUpdate: Adds particle to \\(\\Omega\\), propagates positive weight to neighbors.</p> <p>Key moment: Particles with positive values now exist near the goal!</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#65-episode-2-field-guided-exploration","title":"6.5 Episode 2: Field-Guided Exploration","text":"<p>Agent at \\(s = 2\\) again:</p> <p>Field prediction (now informed by particles):</p> <ul> <li>Particles exist at \\((9, 0.5)\\), \\((7, 0.5)\\), etc. with positive weights</li> <li>\\(Q^+(2, \\text{right}, 0.5)\\) &gt; \\(Q^+(2, \\text{left}, 0.5)\\) (right leads toward goal)</li> </ul> <p>Policy: Boltzmann with \\(\\beta = 1\\): $\\(\\pi(\\text{right} \\mid 2) = \\frac{e^{Q^+(2, \\text{right}, 0.5)}}{e^{Q^+(2, \\text{right}, 0.5)} + e^{Q^+(2, \\text{left}, 0.5)}}\\)$</p> <p>Result: Agent more likely to choose right (toward goal).</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#66-long-term-behavior","title":"6.6 Long-Term Behavior","text":"<p>After 100 episodes:</p> <ul> <li>Particle memory \\(\\Omega\\) contains ~500 particles</li> <li>Positive-value particles cluster in paths leading to goal</li> <li>Negative-value particles mark obstacle region</li> <li>ARD has learned: position \\(s\\) is critical, step size \\(\\theta\\) less so (larger lengthscale for \\(\\theta\\))</li> </ul> <p>Field \\(Q^+\\):</p> <ul> <li>High values: paths from any \\(s\\) toward goal, avoiding obstacle</li> <li>Low values: paths toward obstacle or away from goal</li> </ul> <p>Policy:</p> <ul> <li>Smooth, deterministic path from any start state to goal</li> <li>Naturally avoids obstacle (low \\(Q^+\\) region)</li> </ul> <p>This is emergence: The agent never explicitly computed an optimal path\u2014it emerged from RF-SARSA's three forces!</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#7-why-rf-sarsa-is-not-standard-sarsa-with-kernels","title":"7. Why RF-SARSA Is NOT Standard SARSA with Kernels","text":""},{"location":"GRL0/tutorials/07-rf-sarsa/#71-common-misconception","title":"7.1 Common Misconception","text":"<p>\u8bef interpretation: \"RF-SARSA is just SARSA using kernel function approximation instead of a table.\"</p> <p>Why this is wrong:</p> Aspect Kernel Function Approximation RF-SARSA Updates Global weight vector \\(w\\) via SGD Particle ensemble \\(\\Omega\\) via MemoryUpdate Prediction \\(Q(s,a) = w^\\top \\phi(s,a)\\) (linear) \\(Q^+(z) = \\sum_i \\alpha_i k(z, z_i)\\) (GPR) Memory Fixed basis \\(\\phi\\) Growing/evolving particle set Geometry Fixed feature space Adaptive (ARD on kernel) <p>Kernel FA learns a weight vector. RF-SARSA shapes a functional field.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#72-what-rf-sarsa-actually-is","title":"7.2 What RF-SARSA Actually Is","text":"<p>RF-SARSA is closer to:</p> <p>Galerkin methods (functional analysis):</p> <ul> <li>Approximate solutions in a finite-dimensional subspace of an infinite-dimensional space</li> <li>RF-SARSA: subspace spanned by kernel sections \\(k(z_i, \\cdot)\\)</li> </ul> <p>Interacting particle systems (statistical physics):</p> <ul> <li>Particles influence each other through pairwise interactions</li> <li>RF-SARSA: MemoryUpdate propagates weights through kernel interactions</li> </ul> <p>Energy-based models (modern ML):</p> <ul> <li>Learn energy function, sample from \\(p \\propto \\exp(-E)\\)</li> <li>RF-SARSA: Learn \\(E = -Q^+\\), policy is Boltzmann distribution</li> </ul> <p>Belief-state RL (POMDPs):</p> <ul> <li>Maintain belief over states, condition policy on belief</li> <li>RF-SARSA: \\(\\Omega\\) is belief representation, policy conditioned on \\(Q^+(\\cdot; \\Omega)\\)</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#8-relation-to-modern-rl-methods","title":"8. Relation to Modern RL Methods","text":"<p>RF-SARSA anticipated several ideas that became mainstream later:</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#81-kernel-temporal-difference-learning","title":"8.1 Kernel Temporal Difference Learning","text":"<p>Kernel TD (Engel et al., 2005):</p> <ul> <li>Apply TD learning in RKHS</li> <li>Use kernel trick for function approximation</li> </ul> <p>RF-SARSA connection:</p> <ul> <li>Also uses kernels for TD learning</li> <li>But operates in augmented space \\((s, \\theta)\\)</li> <li>Couples with particle-based memory management</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#82-energy-based-rl","title":"8.2 Energy-Based RL","text":"<p>Modern EBMs (e.g., Diffusion Q-learning, 2023):</p> <ul> <li>Represent policies/values as energy functions</li> <li>Sampling via Langevin dynamics</li> </ul> <p>RF-SARSA connection:</p> <ul> <li>Energy interpretation \\(E = -Q^+\\) (Chapter 3)</li> <li>Policy via Boltzmann distribution (Chapter 03a)</li> <li>Implicitly performs gradient flow on learned energy</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#83-model-based-rl-via-gps","title":"8.3 Model-Based RL via GPs","text":"<p>GP-based model learning (Deisenroth et al., 2015):</p> <ul> <li>Learn forward dynamics \\(p(s' \\mid s, a)\\) via GP</li> <li>Plan using learned model</li> </ul> <p>RF-SARSA connection:</p> <ul> <li>GP over augmented space provides implicit forward model</li> <li>Soft state transitions emerge from kernel similarity (Chapter 8, upcoming)</li> <li>No explicit dynamics model, but captures uncertainty</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#84-neural-processes","title":"8.4 Neural Processes","text":"<p>Neural Processes (Garnelo et al., 2018):</p> <ul> <li>Learn a distribution over functions from context set</li> <li>Condition predictions on context</li> </ul> <p>RF-SARSA connection:</p> <ul> <li>\\(\\Omega\\) is the context set (particle memory)</li> <li>\\(Q^+(z; \\Omega)\\) is the conditional prediction</li> <li>GPR is a (non-parametric) neural process!</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#9-implementation-notes","title":"9. Implementation Notes","text":""},{"location":"GRL0/tutorials/07-rf-sarsa/#91-choosing-hyperparameters","title":"9.1 Choosing Hyperparameters","text":"<p>Kernel lengthscale \\(\\ell\\):</p> <ul> <li>Too small: overfitting, no generalization</li> <li>Too large: over-smoothing, loss of detail</li> <li>Solution: Use ARD to learn per-dimension lengthscales</li> </ul> <p>Association threshold \\(\\tau\\):</p> <ul> <li>Too low: all particles associate, slow computation</li> <li>Too high: no association, memory explosion</li> <li>Rule of thumb: \\(\\tau \\approx 0.1\\) (10% correlation threshold)</li> </ul> <p>SARSA learning rate \\(\\alpha\\):</p> <ul> <li>Standard RL tuning: start \\(\\alpha = 0.1\\), decay over time</li> <li>Should be larger than typical Q-learning (on-policy is more stable)</li> </ul> <p>Policy temperature \\(\\beta\\):</p> <ul> <li>High \\(\\beta\\) (low temperature): greedy, exploitation</li> <li>Low \\(\\beta\\) (high temperature): stochastic, exploration</li> <li>Schedule: Exponential decay, e.g., \\(\\beta_t = \\beta_0 \\cdot 1.01^t\\)</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#92-computational-complexity","title":"9.2 Computational Complexity","text":"<p>Per-step costs:</p> <ol> <li>Field query (policy inference): \\(O(Nn)\\)</li> <li>\\(N\\): number of particles</li> <li>\\(n\\): number of discrete actions</li> <li> <p>GPR prediction: \\(O(N)\\) per query (after precomputing \\((K + \\sigma_n^2 I)^{-1}q\\))</p> </li> <li> <p>MemoryUpdate: \\(O(N)\\)</p> </li> <li>Compute associations: \\(O(N)\\)</li> <li> <p>Update weights: \\(O(|\\mathcal{N}|)\\) where \\(|\\mathcal{N}| \\ll N\\)</p> </li> <li> <p>ARD (every \\(T\\) steps): \\(O(N^3)\\)</p> </li> <li>Solve GP regression: \\(O(N^3)\\) (matrix inversion)</li> <li>Mitigation: Use sparse GP methods, or increase \\(T\\) over time</li> </ol> <p>Total: \\(O(Nn + N + N^3/T) \\approx O(Nn)\\) for reasonable \\(T\\), \\(N\\).</p> <p>Scaling: For large \\(N\\) (&gt;1000), use:</p> <ul> <li>Sparse GPs (inducing points)</li> <li>Random Fourier features</li> <li>Amortized inference (neural network)</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#93-sparse-gp-approximations","title":"9.3 Sparse GP Approximations","text":"<p>Problem: GP regression is \\(O(N^3)\\) in number of particles.</p> <p>Solution: Sparse GP (Qui\u00f1onero-Candela &amp; Rasmussen, 2005):</p> <ul> <li>Choose \\(M \\ll N\\) inducing points</li> <li>Approximate \\(Q^+\\) using only these points</li> <li>Reduces complexity to \\(O(M^2 N)\\)</li> </ul> <p>In RF-SARSA:</p> <ul> <li>Select inducing points from particle memory (e.g., k-means)</li> <li>Update inducing points periodically</li> </ul> <p>Implementation: Use GPyTorch or GPflow with <code>InducingPointStrategy</code>.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#94-python-pseudocode","title":"9.4 Python Pseudocode","text":"<pre><code>class RFSARSA:\n    def __init__(self, kernel, action_model, alpha=0.1, gamma=0.9, beta=1.0, tau=0.1, T_ard=100):\n        self.kernel = kernel  # e.g., RBF kernel\n        self.action_model = action_model  # encodes/decodes actions\n        self.alpha = alpha  # SARSA learning rate\n        self.gamma = gamma  # discount\n        self.beta = beta  # policy temperature\n        self.tau = tau  # association threshold\n        self.T_ard = T_ard  # ARD period\n\n        self.Q_table = {}  # primitive Q(s,a)\n        self.particles = []  # particle memory \u03a9 = [(z, q), ...]\n        self.t_ard = 0\n\n    def gpr_predict(self, z):\n        \"\"\"Predict Q+(z) using GPR on particle memory.\"\"\"\n        if len(self.particles) == 0:\n            return 0.0  # prior mean\n\n        # Kernel vector: k(z, z_i) for all particles\n        k_vec = np.array([self.kernel(z, p[0]) for p in self.particles])\n\n        # GPR prediction (assuming precomputed alpha coefficients)\n        return k_vec @ self.alpha_gpr\n\n    def policy(self, s, actions):\n        \"\"\"Boltzmann policy over actions based on Q+ field.\"\"\"\n        q_values = []\n        for a in actions:\n            x_a = self.action_model.encode(a)\n            z = np.concatenate([s, x_a])\n            q_plus = self.gpr_predict(z)\n            q_values.append(q_plus)\n\n        q_values = np.array(q_values)\n        probs = np.exp(self.beta * q_values)\n        probs /= probs.sum()\n\n        return np.random.choice(actions, p=probs)\n\n    def update(self, s, a, r, s_next, a_next):\n        \"\"\"RF-SARSA update: primitive SARSA + MemoryUpdate.\"\"\"\n        # Primitive SARSA update\n        q_current = self.Q_table.get((s, a), 0.0)\n        q_next = self.Q_table.get((s_next, a_next), 0.0)\n\n        delta = r + self.gamma * q_next - q_current\n        q_new = q_current + self.alpha * delta\n\n        self.Q_table[(s, a)] = q_new\n\n        # Form particle\n        x_a = self.action_model.encode(a)\n        z = np.concatenate([s, x_a])\n        particle = (z, q_new)\n\n        # MemoryUpdate (Algorithm 1)\n        self.particles = memory_update(\n            particle, delta, self.kernel, self.tau, self.particles\n        )\n\n        # Periodic ARD update\n        self.t_ard += 1\n        if self.t_ard % self.T_ard == 0:\n            self.update_kernel_hyperparameters()\n\n    def update_kernel_hyperparameters(self):\n        \"\"\"Run ARD to update kernel lengthscales.\"\"\"\n        # Extract (z, q) from particles\n        Z = np.array([p[0] for p in self.particles])\n        q = np.array([p[1] for p in self.particles])\n\n        # Fit GP with ARD (optimize lengthscales)\n        from sklearn.gaussian_process import GaussianProcessRegressor\n        from sklearn.gaussian_process.kernels import RBF\n\n        kernel = RBF(length_scale=np.ones(Z.shape[1]), length_scale_bounds=(1e-2, 1e2))\n        gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)\n        gp.fit(Z, q)\n\n        # Update kernel with learned lengthscales\n        self.kernel.set_lengthscales(gp.kernel_.length_scale)\n\n        # Recompute GPR coefficients\n        K = self.kernel.matrix(Z, Z)\n        self.alpha_gpr = np.linalg.solve(K + 1e-6 * np.eye(len(Z)), q)\n</code></pre>"},{"location":"GRL0/tutorials/07-rf-sarsa/#10-strengths-and-limitations","title":"10. Strengths and Limitations","text":""},{"location":"GRL0/tutorials/07-rf-sarsa/#101-strengths","title":"10.1 Strengths","text":"<p>1. Generalization in Continuous Action Spaces - Natural handling of parametric actions - Smooth interpolation across action parameters</p> <p>2. Uncertainty Quantification - GP provides variance \\(\\sigma^2(z)\\) (not shown above for simplicity) - Can guide exploration (e.g., UCB: \\(Q^+ + \\kappa \\sigma\\))</p> <p>3. Adaptive Metric Learning - ARD discovers relevant dimensions automatically - No manual feature engineering required</p> <p>4. Interpretability - Particles are interpretable (experienced configurations) - Field visualization shows value landscape</p> <p>5. Theoretical Grounding - Connection to RKHS theory (Chapters 2, 4) - Least action principle (Chapter 03a) - POMDP interpretation (Chapter 8, upcoming)</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#102-limitations","title":"10.2 Limitations","text":"<p>1. Computational Cost - \\(O(N^3)\\) for GP regression (ARD step) - Mitigated by sparse GPs, but still slower than deep RL</p> <p>2. Memory Growth - Particle memory grows over time - Needs pruning/consolidation strategies (Chapter 06a)</p> <p>3. Discrete Action Assumption - Algorithm as stated requires discrete action set for field queries - Solution: Continuous optimization via Langevin sampling (Chapter 03a)</p> <p>4. On-Policy Learning - SARSA is on-policy (learns about behavior policy) - Slower than off-policy (Q-learning, DQN) - Tradeoff: More stable, safer for risk-sensitive domains</p> <p>5. Hyperparameter Sensitivity - Requires tuning \\(\\alpha\\), \\(\\beta\\), \\(\\tau\\), \\(T\\) - ARD helps but doesn't eliminate manual tuning</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#11-summary","title":"11. Summary","text":""},{"location":"GRL0/tutorials/07-rf-sarsa/#111-what-rf-sarsa-does","title":"11.1 What RF-SARSA Does","text":"<p>RF-SARSA is not a policy learning algorithm. It is a functional reinforcement mechanism that:</p> <ol> <li>Grounds value estimates temporally via primitive SARSA (\\(Q(s,a)\\))</li> <li>Generalizes them spatially via GP regression over particles (\\(Q^+(z)\\))</li> <li>Propagates them geometrically via MemoryUpdate (Algorithm 1)</li> <li>Adapts the metric via ARD (kernel hyperparameter learning)</li> </ol> <p>Policy emerges as a consequence of this process (via field queries), not as its direct goal.</p>"},{"location":"GRL0/tutorials/07-rf-sarsa/#112-key-conceptual-insights","title":"11.2 Key Conceptual Insights","text":"<p>Two-layer architecture:</p> <ul> <li>Primitive layer (SARSA) \u2192 temporal grounding</li> <li>Field layer (GPR) \u2192 spatial generalization</li> </ul> <p>Belief-state interpretation:</p> <ul> <li>Particle memory \\(\\Omega\\) = agent's belief state</li> <li>MemoryUpdate = belief update operator</li> <li>Policy = belief-conditioned action inference</li> </ul> <p>Physics grounding:</p> <ul> <li>Energy landscape \\(E = -Q^+\\) learned from experience</li> <li>Boltzmann policy minimizes expected action (Chapter 03a)</li> <li>Smooth trajectories emerge from kinetic regularization (kernel smoothness)</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#113-connection-to-the-big-picture","title":"11.3 Connection to the Big Picture","text":"<p>Part I: Reinforcement Fields (where we are now):</p> <ul> <li>Chapter 4: What is the reinforcement field? (functional object in RKHS)</li> <li>Chapter 5: How is it represented? (particles as basis elements)</li> <li>Chapter 6: How does memory evolve? (MemoryUpdate as belief transition)</li> <li>Chapter 7 (this chapter): How is the field learned? (RF-SARSA as functional TD)</li> </ul> <p>Coming next:</p> <ul> <li>Chapter 8: What emerges from this? (soft state transitions, uncertainty)</li> <li>Chapter 9: How to interpret this? (POMDP view, belief-based control)</li> <li>Chapter 10: Putting it all together (complete GRL system)</li> </ul>"},{"location":"GRL0/tutorials/07-rf-sarsa/#12-key-takeaways","title":"12. Key Takeaways","text":"<ol> <li> <p>RF-SARSA couples two learning processes: primitive SARSA (temporal grounding) + GPR (spatial generalization)</p> </li> <li> <p>It's not SARSA with kernels: Updates particle ensemble, not weight vector; reshapes functional field, not table entries</p> </li> <li> <p>Three forces enable learning: temporal credit (SARSA), geometric generalization (GP), adaptive geometry (ARD)</p> </li> <li> <p>Policy is inferred, not learned: Field queries via GPR \u2192 Boltzmann sampling \u2192 actions</p> </li> <li> <p>Physics-grounded: Energy landscape from least action principle; smooth trajectories from kinetic regularization</p> </li> <li> <p>Belief-state formulation: \\(\\Omega\\) is belief, MemoryUpdate is belief update, policy is belief-conditioned</p> </li> <li> <p>Scalable with approximations: Sparse GPs, random features, or neural networks for large-scale problems</p> </li> <li> <p>Anticipated modern methods: Kernel TD, energy-based RL, GP-based model RL, neural processes</p> </li> </ol>"},{"location":"GRL0/tutorials/07-rf-sarsa/#13-further-reading","title":"13. Further Reading","text":"<p>Original RF-SARSA:</p> <ul> <li>Chiu &amp; Huber (2022), Section IV. arXiv:2208.04822</li> </ul> <p>Kernel Temporal Difference Learning:</p> <ul> <li>Engel, Y., Mannor, S., &amp; Meir, R. (2005). \"Reinforcement learning with Gaussian processes.\" ICML.</li> <li>Xu, X., et al. (2007). \"Kernel-based least squares policy iteration for reinforcement learning.\" IEEE TNNLS.</li> </ul> <p>Path Integral Control (connection to Least Action):</p> <ul> <li>Theodorou, E., Buchli, J., &amp; Schaal, S. (2010). \"A generalized path integral control approach to reinforcement learning.\" JMLR.</li> </ul> <p>Gaussian Processes for RL:</p> <ul> <li>Deisenroth, M. P., &amp; Rasmussen, C. E. (2011). \"PILCO: A model-based and data-efficient approach to policy search.\" ICML.</li> <li>Rasmussen, C. E., &amp; Williams, C. K. I. (2006). Gaussian Processes for Machine Learning. MIT Press.</li> </ul> <p>Energy-Based RL:</p> <ul> <li>Haarnoja, T., et al. (2017). \"Reinforcement learning with deep energy-based policies.\" ICML (SQL).</li> <li>Ajay, A., et al. (2023). \"Is conditional generative modeling all you need for decision making?\" ICLR (Diffusion-QL).</li> </ul> <p>\u2190 Back to Chapter 06a: Advanced Memory Dynamics | Next: Chapter 08 (Coming Soon) \u2192</p> <p>Related: Chapter 03a - Least Action Principle | Related: Chapter 06 - MemoryUpdate</p> <p>Last Updated: January 14, 2026</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/","title":"Chapter 07a: Beyond Discrete Actions \u2014 Continuous Policy Inference","text":"<p>Purpose: Address the discrete action bottleneck and explore fully continuous GRL formulations Prerequisites: Chapter 07 (RF-SARSA) Key Concepts: Continuous action spaces, Langevin sampling, actor-critic in RKHS, action discovery, learned embeddings</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#introduction","title":"Introduction","text":"<p>Chapter 7 presented RF-SARSA as the core learning algorithm for GRL. However, the algorithm as specified has a critical limitation that constrains its applicability:</p> <p>The Discrete Action Assumption: RF-SARSA requires a finite set of primitive actions \\(\\mathcal{A} = \\{a^{(1)}, \\ldots, a^{(n)}\\}\\) to query the field \\(Q^+(s, a^{(i)})\\) during policy inference.</p> <p>This creates two problems:</p> <ol> <li>Manual Action Design: Requires hand-crafted mapping \\(f_{A^+}: a^{(i)} \\mapsto x_a^{(i)}\\) from primitive actions to parametric representation</li> <li>Scalability: For high-dimensional action parameters \\(\\theta \\in \\mathbb{R}^{d_a}\\), enumerating all actions is intractable</li> </ol> <p>Example: In the 2D navigation domain (original GRL paper):</p> <ul> <li>Primitive actions: move in 12 directions (like a clock: 0\u00b0, 30\u00b0, 60\u00b0, ..., 330\u00b0)</li> <li>Manual mapping: each direction \u2192 angle \\(\\theta \\in [0, 2\\pi)\\)</li> <li>Limitation: What if optimal action is at angle \\(\\pi/7 \\approx 25.7\u00b0\\) (not in the discrete set)?</li> </ul> <p>This chapter explores solutions that eliminate discrete actions entirely, enabling fully continuous policy inference in parametric action spaces.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#1-the-problem-discrete-actions-as-a-bottleneck","title":"1. The Problem: Discrete Actions as a Bottleneck","text":""},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#11-where-discrete-actions-enter-rf-sarsa","title":"1.1 Where Discrete Actions Enter RF-SARSA","text":"<p>In Algorithm 2 (Chapter 7), discrete actions appear in two places:</p> <p>Step 6: Field-Based Action Evaluation <pre><code>For each a^(i) \u2208 A:\n    Form z^(i) = (s, x_a^(i))\n    Query Q+(z^(i)) via GPR\nSelect a via Boltzmann policy\n</code></pre></p> <p>Step 10: Primitive SARSA Update <pre><code>\u03b4 = r + \u03b3 Q(s', a') - Q(s, a)\nQ(s, a) \u2190 Q(s, a) + \u03b1 \u03b4\n</code></pre></p> <p>Both steps assume \\(a \\in \\mathcal{A}\\) is discrete.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#12-why-this-is-limiting","title":"1.2 Why This Is Limiting","text":"<p>Problem 1: Manual Feature Engineering</p> <p>The mapping \\(f_{A^+}: a \\mapsto x_a\\) must be designed by hand:</p> <ul> <li>2D navigation: direction \u2192 angle</li> <li>Robotic reaching: discrete waypoints \u2192 joint angles</li> <li>Continuous control: requires discretizing continuous action space</li> </ul> <p>This defeats the purpose of parametric actions\u2014you still need domain expertise!</p> <p>Problem 2: Curse of Dimensionality</p> <p>For high-dimensional actions \\(\\theta \\in \\mathbb{R}^{d_a}\\):</p> <ul> <li>Discretizing each dimension with \\(k\\) values \u2192 \\(k^{d_a}\\) primitive actions</li> <li>Example: \\(d_a = 10\\), \\(k = 10\\) \u2192 \\(10^{10}\\) actions (intractable!)</li> </ul> <p>Problem 3: Suboptimal Actions</p> <p>Optimal action might lie between discrete choices:</p> <ul> <li>Discrete set: \\(\\{30\u00b0, 45\u00b0, 60\u00b0\\}\\)</li> <li>Optimal: \\(42\u00b0\\) (not representable!)</li> </ul>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#13-the-sarsa-constraint","title":"1.3 The SARSA Constraint","text":"<p>Why does RF-SARSA use discrete actions? Because SARSA does.</p> <p>Original SARSA (Rummery &amp; Niranjan, 1994): $\\(Q(s, a) \\leftarrow Q(s, a) + \\alpha [r + \\gamma Q(s', a') - Q(s, a)]\\)$</p> <p>This assumes \\(a, a'\\) are indexable (discrete) so you can store \\(Q(s, a)\\) in a table or lookup structure.</p> <p>For continuous actions \\(\\theta \\in \\mathbb{R}^{d_a}\\), you can't index \\(Q(s, \\theta)\\) this way!</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#2-solution-1-continuous-sarsa-via-langevin-sampling","title":"2. Solution 1: Continuous SARSA via Langevin Sampling","text":"<p>Key insight: We don't need primitive actions\u2014we can sample directly from the continuous field \\(Q^+(s, \\theta)\\) using gradient-based sampling.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#21-langevin-dynamics-refresher-from-chapter-03a","title":"2.1 Langevin Dynamics Refresher (from Chapter 03a)","text":"<p>Recall from the least action principle that optimal actions follow gradient flow:</p> \\[\\theta_{t+1} = \\theta_t + \\epsilon \\nabla_\\theta Q^+(s, \\theta_t) + \\sqrt{2\\epsilon\\lambda} \\, \\xi_t\\] <p>where:</p> <ul> <li>\\(\\nabla_\\theta Q^+(s, \\theta)\\) = gradient of field w.r.t. action parameters (via Riesz representer, Chapter 04a)</li> <li>\\(\\lambda\\) = temperature (exploration)</li> <li>\\(\\xi_t \\sim \\mathcal{N}(0, I)\\) = Gaussian noise</li> </ul> <p>This is Langevin Monte Carlo sampling from the Boltzmann distribution \\(\\pi(\\theta | s) \\propto \\exp(Q^+(s, \\theta) / \\lambda)\\).</p> <p>Advantage: No discrete action set needed! Sample \\(\\theta\\) directly from the field.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#22-continuous-rf-sarsa-modified-algorithm","title":"2.2 Continuous RF-SARSA: Modified Algorithm","text":"<p>Replace discrete action enumeration with continuous sampling:</p> <p>Original (discrete) RF-SARSA: <pre><code>For each a^(i) \u2208 A:\n    Query Q+(s, a^(i))\nSelect a ~ Boltzmann(Q+)\n</code></pre></p> <p>Continuous RF-SARSA: <pre><code>Initialize \u03b8_0 randomly (or from heuristic)\nFor k = 1 to K_sample:\n    Compute \u2207_\u03b8 Q+(s, \u03b8_{k-1})  [via Riesz representer]\n    \u03b8_k \u2190 \u03b8_{k-1} + \u03b5 \u2207_\u03b8 Q+(s, \u03b8_{k-1}) + \u221a(2\u03b5\u03bb) \u03be_k\nReturn \u03b8_K\n</code></pre></p> <p>Key change: Action inference becomes gradient-based optimization on the field.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#23-how-to-update-q-without-discrete-actions","title":"2.3 How to Update Q Without Discrete Actions?","text":"<p>Problem: SARSA update requires \\(Q(s, a)\\) as a scalar, but now \\(\\theta\\) is continuous.</p> <p>Solution 1: Function Approximation (Standard Approach)</p> <p>Use a parametric function approximator (e.g., neural network): $\\(Q_w(s, \\theta)\\)$</p> <p>Update via gradient descent: $\\(w \\leftarrow w - \\alpha \\nabla_w [Q_w(s, \\theta) - (r + \\gamma Q_w(s', \\theta'))]^2\\)$</p> <p>But wait\u2014this is just deep RL! We've abandoned the particle-based field representation.</p> <p>Solution 2: Particle-Based Continuous SARSA (GRL Way)</p> <p>Keep the particle representation \\(Q^+(z) = \\sum_i w_i k(z, z_i)\\) but eliminate primitive \\(Q(s, a)\\) table.</p> <p>Modified update:</p> <ol> <li>Sample action: \\(\\theta \\sim \\pi(\\cdot | s)\\) via Langevin</li> <li>Execute: observe \\(r\\), \\(s'\\)</li> <li>Sample next action: \\(\\theta' \\sim \\pi(\\cdot | s')\\) via Langevin</li> <li>Compute TD target directly from field:    $\\(\\delta = r + \\gamma Q^+(s', \\theta') - Q^+(s, \\theta)\\)$</li> <li>Form particle: \\(\\omega = ((s, \\theta), r + \\gamma Q^+(s', \\theta'))\\) (bootstrap target, not table Q)</li> <li>MemoryUpdate: \\(\\Omega \\leftarrow \\text{MemoryUpdate}(\\omega, \\delta, k, \\tau, \\Omega)\\)</li> </ol> <p>Key difference: No primitive \\(Q(s, a)\\) table! TD target computed directly from field queries.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#24-algorithm-continuous-rf-sarsa","title":"2.4 Algorithm: Continuous RF-SARSA","text":"<p>Inputs: - Kernel \\(k(\\cdot, \\cdot; \\theta)\\) - Langevin step size \\(\\epsilon\\), temperature \\(\\lambda\\) - Number of Langevin steps \\(K_{\\text{sample}}\\) - TD learning rate \\(\\alpha\\) (for particle weight updates) - Discount \\(\\gamma\\), association threshold \\(\\tau\\)</p> <p>Initialization: - Particle memory \\(\\Omega \\leftarrow \\emptyset\\) - Kernel hyperparameters \\(\\theta\\) (via ARD or prior)</p> <p>For each episode:</p> <ol> <li>Observe initial state \\(s_0\\)</li> </ol> <p>For each step \\(t\\):</p> <ol> <li> <p>Action sampling via Langevin:</p> </li> <li> <p>Initialize \\(\\theta_0\\) randomly or from heuristic</p> </li> <li>For \\(k = 1, \\ldots, K_{\\text{sample}}\\):      $\\(\\nabla_\\theta Q^+(s_t, \\theta_{k-1}) = \\sum_{i=1}^N w_i \\nabla_\\theta k((s_t, \\theta_{k-1}), z_i)\\)$      $\\(\\theta_k \\leftarrow \\theta_{k-1} + \\epsilon \\nabla_\\theta Q^+(s_t, \\theta_{k-1}) + \\sqrt{2\\epsilon\\lambda} \\, \\xi_k\\)$</li> <li> <p>Set \\(\\theta_t \\leftarrow \\theta_{K_{\\text{sample}}}\\)</p> </li> <li> <p>Execute action:</p> </li> <li> <p>Execute \\(\\theta_t\\) in environment</p> </li> <li> <p>Observe \\(r_t\\), \\(s_{t+1}\\)</p> </li> <li> <p>Next action sampling:</p> </li> <li> <p>Repeat step 2 for \\(s_{t+1}\\) to get \\(\\theta_{t+1}\\)</p> </li> <li> <p>Field-based TD update:</p> </li> <li> <p>Query field: \\(Q_t^+ \\leftarrow Q^+(s_t, \\theta_t)\\), \\(Q_{t+1}^+ \\leftarrow Q^+(s_{t+1}, \\theta_{t+1})\\)</p> </li> <li>Compute TD error: \\(\\delta_t \\leftarrow r_t + \\gamma Q_{t+1}^+ - Q_t^+\\)</li> <li> <p>Form TD target: \\(y_t \\leftarrow r_t + \\gamma Q_{t+1}^+\\) (bootstrap from field)</p> </li> <li> <p>Particle reinforcement:</p> </li> <li> <p>Form particle: \\(\\omega_t \\leftarrow ((s_t, \\theta_t), y_t)\\)</p> </li> <li> <p>Update memory: \\(\\Omega \\leftarrow \\text{MemoryUpdate}(\\omega_t, \\delta_t, k, \\tau, \\Omega)\\)</p> </li> <li> <p>Periodic ARD:</p> </li> <li> <p>Every \\(T\\) steps, update kernel hyperparameters \\(\\theta\\) via ARD on \\(\\Omega\\)</p> </li> </ol> <p>Repeat until terminal.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#25-advantages-and-limitations","title":"2.5 Advantages and Limitations","text":"<p>\u2705 Advantages:</p> <ol> <li>No manual action mapping: \\(\\theta\\) is sampled directly from field</li> <li>Fully continuous: No discretization of action space</li> <li>Principled: Langevin sampling from Boltzmann distribution (Chapter 03a)</li> <li>Natural exploration: Temperature \\(\\lambda\\) controls stochasticity</li> </ol> <p>\u26a0\ufe0f Limitations:</p> <ol> <li>Gradient computation: Requires \\(\\nabla_\\theta k(z, z')\\) (analytic or autodiff)</li> <li>Langevin convergence: Need \\(K_{\\text{sample}}\\) steps per action (slower)</li> <li>Local optima: Gradient descent can get stuck (non-convex \\(Q^+\\))</li> <li>No primitive Q-table: Loses SARSA's tabular grounding</li> </ol> <p>When to use: High-dimensional continuous actions where discrete enumeration is impossible.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#3-solution-2-actor-critic-in-rkhs","title":"3. Solution 2: Actor-Critic in RKHS","text":"<p>Idea: Decouple policy (actor) from value function (critic), as in standard actor-critic methods.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#31-the-actor-critic-framework","title":"3.1 The Actor-Critic Framework","text":"<p>Actor: Parametric policy \\(\\pi_\\phi(\\theta | s)\\) - Could be Gaussian: \\(\\pi_\\phi(\\theta | s) = \\mathcal{N}(\\mu_\\phi(s), \\sigma_\\phi(s))\\) - Trained via policy gradient</p> <p>Critic: Value function \\(Q^+(s, \\theta)\\) in RKHS (as before) - Trained via TD learning (using particles)</p> <p>Advantage: Policy is flexible, efficient to sample; critic provides value estimates for learning.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#32-grl-actor-critic-algorithm","title":"3.2 GRL Actor-Critic Algorithm","text":"<p>Modifications to RF-SARSA:</p> <p>Action selection (no Langevin needed):</p> <ul> <li>Sample from actor: \\(\\theta_t \\sim \\pi_\\phi(\\cdot | s_t)\\)</li> <li>Fast sampling (single forward pass)</li> </ul> <p>Critic update (unchanged):</p> <ul> <li>Particle-based TD: \\(\\delta_t = r_t + \\gamma Q^+(s_{t+1}, \\theta_{t+1}) - Q^+(s_t, \\theta_t)\\)</li> <li>MemoryUpdate as before</li> </ul> <p>Actor update (policy gradient):</p> <ul> <li>Compute advantage: \\(A_t = Q^+(s_t, \\theta_t) - V(s_t)\\) where \\(V(s) = \\mathbb{E}_{\\theta \\sim \\pi_\\phi}[Q^+(s, \\theta)]\\)</li> <li>Update policy: \\(\\phi \\leftarrow \\phi + \\beta \\nabla_\\phi \\log \\pi_\\phi(\\theta_t | s_t) A_t\\)</li> </ul>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#33-pseudocode","title":"3.3 Pseudocode","text":"<pre><code>class GRL_ActorCritic:\n    def __init__(self, actor_net, kernel, alpha=0.1, beta=0.01, gamma=0.9, tau=0.1):\n        self.actor = actor_net  # Neural network: s \u2192 \u03bc(s), \u03c3(s)\n        self.kernel = kernel\n        self.particles = []  # Critic: particle memory \u03a9\n        self.alpha = alpha  # Critic learning rate\n        self.beta = beta  # Actor learning rate\n        self.gamma = gamma\n        self.tau = tau\n\n    def sample_action(self, s):\n        \"\"\"Sample action from actor policy.\"\"\"\n        mu, sigma = self.actor(s)\n        theta = np.random.normal(mu, sigma)\n        return theta, mu, sigma\n\n    def critic_predict(self, s, theta):\n        \"\"\"Predict Q+(s, \u03b8) via GPR on particles.\"\"\"\n        z = np.concatenate([s, theta])\n        if len(self.particles) == 0:\n            return 0.0\n        return gpr_predict(z, self.particles, self.kernel)\n\n    def update(self, s, theta, r, s_next, theta_next):\n        \"\"\"Actor-critic update.\"\"\"\n        # Critic TD update\n        Q_current = self.critic_predict(s, theta)\n        Q_next = self.critic_predict(s_next, theta_next)\n        delta = r + self.gamma * Q_next - Q_current\n\n        # Particle reinforcement (critic)\n        z = np.concatenate([s, theta])\n        y = r + self.gamma * Q_next  # TD target\n        particle = (z, y)\n        self.particles = memory_update(particle, delta, self.kernel, self.tau, self.particles)\n\n        # Actor policy gradient\n        # Advantage: A = Q(s,\u03b8) - V(s)\n        # For simplicity, use TD error as advantage (A \u2248 \u03b4)\n        log_prob = self.actor.log_prob(theta, s)  # log \u03c0_\u03c6(\u03b8|s)\n        actor_loss = -log_prob * delta  # Policy gradient\n\n        self.actor.update(actor_loss, self.beta)\n</code></pre>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#34-advantages-and-limitations","title":"3.4 Advantages and Limitations","text":"<p>\u2705 Advantages:</p> <ol> <li>Efficient sampling: No Langevin iterations, single forward pass</li> <li>Flexible policy: Can model complex distributions (multimodal, correlations)</li> <li>Standard framework: Leverages decades of actor-critic research</li> <li>Scalable: Neural networks handle high-dimensional states/actions</li> </ol> <p>\u26a0\ufe0f Limitations:</p> <ol> <li>Parametric policy: Loses non-parametric flexibility of particle-based field</li> <li>Two learning systems: Actor and critic can be unstable (common in AC methods)</li> <li>Hyperparameters: Requires tuning learning rates, network architectures</li> <li>Divergence risk: Policy and value can diverge without careful tuning</li> </ol> <p>When to use: High-dimensional, complex policies where sampling efficiency matters.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#4-solution-3-learned-action-embeddings","title":"4. Solution 3: Learned Action Embeddings","text":"<p>Idea: Instead of hand-designing \\(f_{A^+}: a \\mapsto x_a\\), learn the embedding jointly with the value function.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#41-the-embedding-problem","title":"4.1 The Embedding Problem","text":"<p>Original GRL: Requires manual mapping from primitive actions to parametric representation.</p> <p>Example (2D navigation):</p> <ul> <li>Primitive: \"move North\"</li> <li>Manual embedding: North \u2192 angle \\(\\theta = \\pi/2\\)</li> </ul> <p>Question: Can we learn this mapping automatically?</p> <p>Answer: Yes! Use contrastive learning or auto-encoder.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#42-contrastive-action-embedding","title":"4.2 Contrastive Action Embedding","text":"<p>Objective: Embed actions such that:</p> <ul> <li>Similar actions (similar outcomes) \u2192 close in embedding space</li> <li>Dissimilar actions \u2192 far apart</li> </ul> <p>Training:</p> <ol> <li>Collect transitions: \\((s, a, s')\\)</li> <li>Define similarity: \\(\\text{sim}(a, a') = \\exp(-\\|s' - s''\\|^2)\\) where \\(s', s''\\) are outcomes</li> <li>Learn embedding: \\(f_\\psi: a \\mapsto x_a\\) such that \\(\\|x_a - x_{a'}\\|^2 \\propto -\\log \\text{sim}(a, a')\\)</li> </ol> <p>Loss (InfoNCE-style): $\\(\\mathcal{L}_{\\text{embed}} = -\\log \\frac{\\exp(\\langle f_\\psi(a), f_\\psi(a^+) \\rangle / \\tau)}{\\sum_{a^-} \\exp(\\langle f_\\psi(a), f_\\psi(a^-) \\rangle / \\tau)}\\)$</p> <p>where \\(a^+\\) is a positive pair (similar action), \\(a^-\\) are negatives.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#43-joint-learning-embedding-value-function","title":"4.3 Joint Learning: Embedding + Value Function","text":"<p>Algorithm:</p> <ol> <li>Initialization: Random embedding \\(f_\\psi\\)</li> <li>Collect experience: \\((s, a, r, s')\\)</li> <li>Embed actions: \\(x_a \\leftarrow f_\\psi(a)\\), \\(x_{a'} \\leftarrow f_\\psi(a')\\)</li> <li>TD update: Standard RF-SARSA using embedded actions</li> <li>Embedding update: Contrastive loss based on \\((a, a')\\) pairs with similar outcomes</li> <li>Repeat</li> </ol> <p>Key insight: Embedding evolves to make TD learning easier\u2014actions with similar values cluster in embedding space!</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#44-advantages-and-limitations","title":"4.4 Advantages and Limitations","text":"<p>\u2705 Advantages:</p> <ol> <li>No manual design: Embedding learned from data</li> <li>Adaptive: Embedding adapts to task (reward structure)</li> <li>Discovers structure: May reveal latent action properties</li> </ol> <p>\u26a0\ufe0f Limitations:</p> <ol> <li>Requires experience: Need data to learn embedding</li> <li>Non-stationarity: Embedding changes as policy improves (moving target)</li> <li>Computational cost: Joint optimization is complex</li> </ol> <p>When to use: When action space structure is unknown, or manual embedding is infeasible.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#5-solution-4-hierarchical-action-discovery","title":"5. Solution 4: Hierarchical Action Discovery","text":"<p>Idea: Let the agent discover a discrete action set automatically by clustering in action parameter space.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#51-the-clustering-approach","title":"5.1 The Clustering Approach","text":"<p>Step 1: Start with continuous action parameter space \\(\\mathbb{R}^{d_a}\\)</p> <p>Step 2: After initial exploration, cluster observed action parameters:</p> <ul> <li>Use k-means, GMM, or DBSCAN on \\(\\{\\theta_i\\}\\) from particle memory</li> <li>Cluster centers become \"discovered actions\"</li> </ul> <p>Step 3: Use discovered actions as discrete set for RF-SARSA</p> <p>Step 4: Periodically re-cluster as policy evolves</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#52-algorithm-sketch","title":"5.2 Algorithm Sketch","text":"<pre><code>def discover_actions(particles, n_clusters=10):\n    \"\"\"Discover discrete actions via clustering.\"\"\"\n    # Extract action parameters from particle memory\n    theta_set = [z[len(state):] for (z, q) in particles]  # z = (s, \u03b8)\n\n    # Cluster in action space\n    from sklearn.cluster import KMeans\n    kmeans = KMeans(n_clusters=n_clusters)\n    kmeans.fit(theta_set)\n\n    # Cluster centers = discovered actions\n    discovered_actions = kmeans.cluster_centers_\n    return discovered_actions\n\n# In RF-SARSA loop:\nif episode % T_discovery == 0:\n    A_discovered = discover_actions(self.particles, n_clusters=10)\n    # Use A_discovered as discrete action set for next T_discovery episodes\n</code></pre>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#53-options-framework-connection","title":"5.3 Options Framework Connection","text":"<p>This is closely related to the options framework (Sutton et al., 1999):</p> <ul> <li>Options: Temporally extended actions (subpolicies)</li> <li>GRL version: Discovered action clusters become \"options\"</li> <li>Can learn hierarchical policies: high-level chooses option, low-level executes</li> </ul>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#54-advantages-and-limitations","title":"5.4 Advantages and Limitations","text":"<p>\u2705 Advantages:</p> <ol> <li>Automatic: No manual action design</li> <li>Data-driven: Discovers actions that matter for the task</li> <li>Hierarchical: Enables multi-level policies</li> </ol> <p>\u26a0\ufe0f Limitations:</p> <ol> <li>Requires exploration: Need diverse data to cluster well</li> <li>K selection: How many clusters? (hyperparameter)</li> <li>Non-stationary: Discovered actions change over time</li> </ol> <p>When to use: Complex action spaces where structure is unknown, or hierarchical RL is beneficial.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#6-solution-5-direct-optimization-on-the-field","title":"6. Solution 5: Direct Optimization on the Field","text":"<p>Most radical solution: Eliminate TD learning entirely! Directly optimize the field \\(Q^+\\) via gradient ascent on expected return.</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#61-field-based-policy-gradient","title":"6.1 Field-Based Policy Gradient","text":"<p>Objective: Maximize expected return $\\(J(\\Omega) = \\mathbb{E}_{\\tau \\sim \\pi_\\Omega}[\\sum_{t=0}^T r_t]\\)$</p> <p>where \\(\\pi_\\Omega\\) is the policy induced by field \\(Q^+(\\cdot; \\Omega)\\) (e.g., via Langevin sampling).</p> <p>Gradient: $\\(\\nabla_\\Omega J = \\mathbb{E}_{\\tau}[\\sum_{t=0}^T \\nabla_\\Omega \\log \\pi_\\Omega(\\theta_t | s_t) R_t]\\)$</p> <p>where \\(R_t = \\sum_{t'=t}^T r_{t'}\\) is the return from time \\(t\\).</p> <p>Update: Gradient ascent on particle memory:</p> <ul> <li>Add particles where policy needs reinforcement</li> <li>Remove particles where policy is suboptimal</li> <li>Adjust weights to increase expected return</li> </ul>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#62-challenges","title":"6.2 Challenges","text":"<p>Problem 1: Computing \\(\\nabla_\\Omega \\log \\pi_\\Omega(\\theta | s)\\) is non-trivial\u2014policy is implicit (via GPR + Langevin).</p> <p>Problem 2: High variance (standard policy gradient issue).</p> <p>Solution: Use score matching or diffusion models to learn \\(\\nabla_\\theta \\log \\pi(\\theta | s)\\) directly.</p> <p>This is an active research direction! (Connect to modern diffusion-based RL: Diffusion-QL, Diffuser, Decision Diffusion, etc.)</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#7-alternative-learning-mechanisms-beyond-sarsa","title":"7. Alternative Learning Mechanisms Beyond SARSA","text":""},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#71-q-learning-in-rkhs","title":"7.1 Q-Learning in RKHS","text":"<p>Replace SARSA's on-policy update with Q-learning's off-policy update:</p> <p>Original SARSA: $\\(\\delta = r + \\gamma Q(s', a') - Q(s, a) \\quad \\text{(uses actual next action } a'\\text{)}\\)$</p> <p>Q-Learning: $\\(\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\quad \\text{(uses max over actions)}\\)$</p> <p>For continuous actions: Replace \\(\\max_{a'}\\) with optimization: $\\(\\theta^* = \\arg\\max_\\theta Q^+(s', \\theta)\\)$</p> <p>Use gradient ascent or Langevin sampling to find \\(\\theta^*\\).</p> <p>Advantage: Off-policy (can reuse experience, more sample-efficient).</p> <p>Limitation: Still requires optimization at each step (costly).</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#72-model-based-dyna-style-planning","title":"7.2 Model-Based: Dyna-Style Planning","text":"<p>Idea: Use particle memory as a forward model, perform planning.</p> <p>Dyna-Q analog for GRL:</p> <ol> <li>Direct learning: Update \\(\\Omega\\) from real experience (as in RF-SARSA)</li> <li>Model learning: Particles encode transitions \\((s, \\theta) \\to (r, s')\\)</li> <li>Planning: Simulate trajectories using particle memory</li> <li>Query \\(Q^+(s, \\theta)\\) to predict \\(r\\)</li> <li>Use GP to predict \\(s'\\) (if we model transition dynamics)</li> <li>Perform TD updates on simulated experience</li> </ol> <p>Advantage: Sample efficiency (real experience + simulated).</p> <p>Limitation: Requires modeling \\(p(s' | s, \\theta)\\), not just \\(Q^+\\).</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#73-successor-representations","title":"7.3 Successor Representations","text":"<p>Idea: Decouple environment dynamics from reward.</p> <p>Successor representation: $\\(\\psi(s, \\theta) = \\mathbb{E}[\\sum_{t=0}^\\infty \\gamma^t \\phi(s_t) \\mid s_0 = s, \\theta_0 = \\theta]\\)$</p> <p>where \\(\\phi(s)\\) are state features.</p> <p>Value function: $\\(Q^+(s, \\theta) = \\psi(s, \\theta)^\\top w\\)$</p> <p>where \\(w\\) are reward weights.</p> <p>Advantage: Transfer learning (reuse \\(\\psi\\) for new reward functions \\(w\\)).</p> <p>GRL connection: Can represent \\(\\psi\\) via particles in RKHS!</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#8-practical-recommendations","title":"8. Practical Recommendations","text":""},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#81-decision-tree-which-approach-to-use","title":"8.1 Decision Tree: Which Approach to Use?","text":"<pre><code>Is action space discrete with &lt; 100 actions?\n\u251c\u2500 YES \u2192 Use original RF-SARSA (Chapter 7)\n\u2514\u2500 NO \u2192 Is action dimensionality high (d_a &gt; 5)?\n    \u251c\u2500 YES \u2192 Use Actor-Critic in RKHS (Solution 2)\n    \u2502        Efficient sampling, scalable\n    \u2514\u2500 NO \u2192 Is gradient computation feasible?\n        \u251c\u2500 YES \u2192 Use Continuous RF-SARSA with Langevin (Solution 1)\n        \u2502        Principled, no extra networks\n        \u2514\u2500 NO \u2192 Use Learned Embeddings (Solution 3) or\n                 Hierarchical Discovery (Solution 4)\n</code></pre>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#82-hybrid-approach-recommended","title":"8.2 Hybrid Approach (Recommended)","text":"<p>Best of both worlds: Combine solutions!</p> <p>Stage 1: Exploration (Episodes 1-100) - Use Langevin sampling (Solution 1) for pure exploration - Build diverse particle memory</p> <p>Stage 2: Discovery (Episodes 100-200) - Cluster actions (Solution 4) to find structure - Use discovered discrete actions for efficiency</p> <p>Stage 3: Exploitation (Episodes 200+) - Use Actor-Critic (Solution 2) with structured embedding - Fine-tune via continuous Langevin when needed</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#9-implementation-continuous-rf-sarsa","title":"9. Implementation: Continuous RF-SARSA","text":"<p>Here's a complete implementation of Solution 1:</p> <pre><code>import numpy as np\n\nclass ContinuousRFSARSA:\n    \"\"\"RF-SARSA without discrete actions: pure Langevin sampling.\"\"\"\n\n    def __init__(self, kernel, epsilon=0.01, lambda_temp=1.0, K_sample=10, \n                 gamma=0.9, tau=0.1, T_ard=100):\n        self.kernel = kernel\n        self.epsilon = epsilon  # Langevin step size\n        self.lambda_temp = lambda_temp  # Temperature\n        self.K_sample = K_sample  # Langevin iterations\n        self.gamma = gamma\n        self.tau = tau\n        self.T_ard = T_ard\n\n        self.particles = []  # (z, y) pairs\n        self.t_ard = 0\n\n    def q_plus(self, s, theta):\n        \"\"\"Query Q+(s, \u03b8) via GPR.\"\"\"\n        if len(self.particles) == 0:\n            return 0.0\n\n        z = np.concatenate([s, theta])\n\n        # GP prediction (assuming precomputed alpha coefficients)\n        k_vec = np.array([self.kernel(z, p[0]) for p in self.particles])\n        return k_vec @ self.alpha_gpr\n\n    def grad_q_plus(self, s, theta):\n        \"\"\"Compute \u2207_\u03b8 Q+(s, \u03b8) via Riesz representer.\"\"\"\n        if len(self.particles) == 0:\n            return np.zeros_like(theta)\n\n        z = np.concatenate([s, theta])\n        d_s = len(s)\n\n        # Gradient via kernel derivatives\n        grad = np.zeros_like(theta)\n        for (z_i, q_i), alpha_i in zip(self.particles, self.alpha_gpr):\n            # \u2202k(z, z_i)/\u2202\u03b8 (assuming RBF kernel)\n            diff = z - z_i\n            k_val = self.kernel(z, z_i)\n            grad_k = -diff[d_s:] / (self.kernel.lengthscale**2) * k_val\n            grad += alpha_i * grad_k\n\n        return grad\n\n    def sample_action_langevin(self, s):\n        \"\"\"Sample action via Langevin dynamics on Q+ field.\"\"\"\n        # Initialize randomly\n        theta = np.random.randn(self.action_dim)\n\n        # Langevin iterations\n        for k in range(self.K_sample):\n            grad = self.grad_q_plus(s, theta)\n            theta = theta + self.epsilon * grad + \\\n                    np.sqrt(2 * self.epsilon * self.lambda_temp) * np.random.randn(self.action_dim)\n\n        return theta\n\n    def update(self, s, theta, r, s_next, theta_next):\n        \"\"\"Continuous RF-SARSA update.\"\"\"\n        # Query field for TD\n        Q_current = self.q_plus(s, theta)\n        Q_next = self.q_plus(s_next, theta_next)\n\n        # TD error\n        delta = r + self.gamma * Q_next - Q_current\n\n        # TD target (bootstrap from field, not from Q-table!)\n        y = r + self.gamma * Q_next\n\n        # Form particle\n        z = np.concatenate([s, theta])\n        particle = (z, y)\n\n        # MemoryUpdate\n        self.particles = memory_update(particle, delta, self.kernel, self.tau, self.particles)\n\n        # Periodic ARD\n        self.t_ard += 1\n        if self.t_ard % self.T_ard == 0:\n            self.update_kernel_hyperparameters()\n\n    def update_kernel_hyperparameters(self):\n        \"\"\"Run ARD to update kernel lengthscales.\"\"\"\n        if len(self.particles) &lt; 10:\n            return  # Need sufficient data\n\n        Z = np.array([p[0] for p in self.particles])\n        q = np.array([p[1] for p in self.particles])\n\n        # Fit GP with ARD\n        from sklearn.gaussian_process import GaussianProcessRegressor\n        from sklearn.gaussian_process.kernels import RBF\n\n        kernel = RBF(length_scale=np.ones(Z.shape[1]), length_scale_bounds=(1e-2, 1e2))\n        gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-6)\n        gp.fit(Z, q)\n\n        # Update kernel\n        self.kernel.set_lengthscales(gp.kernel_.length_scale)\n\n        # Recompute GPR coefficients\n        K = self.kernel.matrix(Z, Z)\n        self.alpha_gpr = np.linalg.solve(K + 1e-6 * np.eye(len(Z)), q)\n\n    def train(self, env, n_episodes=100):\n        \"\"\"Training loop.\"\"\"\n        for episode in range(n_episodes):\n            s = env.reset()\n            theta = self.sample_action_langevin(s)\n\n            done = False\n            while not done:\n                # Execute\n                s_next, r, done = env.step(theta)\n\n                # Next action\n                theta_next = self.sample_action_langevin(s_next)\n\n                # Update\n                self.update(s, theta, r, s_next, theta_next)\n\n                # Advance\n                s, theta = s_next, theta_next\n\n            print(f\"Episode {episode}: Total reward = {env.episode_return}\")\n</code></pre>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#10-summary","title":"10. Summary","text":""},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#101-the-discrete-action-bottleneck","title":"10.1 The Discrete Action Bottleneck","text":"<p>RF-SARSA (Chapter 7) requires discrete primitive actions, creating limitations:</p> <ul> <li>Manual action mapping needed</li> <li>Curse of dimensionality for high-d actions</li> <li>Suboptimal actions (discrete approximation of continuous space)</li> </ul>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#102-five-solutions","title":"10.2 Five Solutions","text":"Solution Key Idea Advantages Limitations 1. Continuous SARSA Langevin sampling on field Principled, no manual design Gradient computation, convergence 2. Actor-Critic in RKHS Parametric policy + particle critic Efficient sampling, scalable Parametric policy, instability risk 3. Learned Embeddings Learn action representation jointly Adaptive, discovers structure Non-stationary, requires experience 4. Hierarchical Discovery Cluster actions automatically Data-driven, enables hierarchy K selection, non-stationary 5. Direct Optimization Policy gradient on field No TD needed High variance, complex"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#103-practical-recommendations","title":"10.3 Practical Recommendations","text":"<p>For most problems: Start with Continuous SARSA (Solution 1) or Actor-Critic (Solution 2).</p> <p>Hybrid approach: Combine exploration (Langevin) \u2192 discovery (clustering) \u2192 exploitation (actor-critic).</p>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#104-beyond-sarsa","title":"10.4 Beyond SARSA","text":"<p>Alternative learning mechanisms:</p> <ul> <li>Q-Learning in RKHS: Off-policy, sample-efficient</li> <li>Dyna-style planning: Model-based, simulate with particles</li> <li>Successor representations: Transfer learning across reward functions</li> </ul>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#11-key-takeaways","title":"11. Key Takeaways","text":"<ol> <li> <p>Original RF-SARSA has a discrete action bottleneck requiring manual mapping \\(f_{A^+}\\)</p> </li> <li> <p>Continuous SARSA via Langevin eliminates discrete actions entirely\u2014sample directly from field</p> </li> <li> <p>Actor-Critic in RKHS combines efficiency (neural network policy) with non-parametric critic (particles)</p> </li> <li> <p>Learned embeddings remove manual design\u2014discover action structure from data</p> </li> <li> <p>Hierarchical discovery via clustering enables data-driven action spaces</p> </li> <li> <p>SARSA is not the only way\u2014Q-learning, model-based, policy gradients all viable</p> </li> <li> <p>The least action principle (Chapter 03a) provides theoretical foundation for Langevin-based continuous RL</p> </li> </ol>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#12-open-research-questions","title":"12. Open Research Questions","text":"<ol> <li> <p>Convergence theory for continuous RF-SARSA: Does Langevin + TD converge? Under what conditions?</p> </li> <li> <p>Optimal exploration in continuous actions: How to balance Langevin temperature vs. ARD adaptation?</p> </li> <li> <p>Sparse particle representations: Can we maintain performance with fewer particles in continuous action spaces?</p> </li> <li> <p>Hierarchical GRL: How to discover and learn action hierarchies automatically?</p> </li> <li> <p>Transfer learning: Can learned action embeddings transfer across tasks?</p> </li> </ol>"},{"location":"GRL0/tutorials/07a-continuous-policy-inference/#further-reading","title":"Further Reading","text":"<p>Continuous Action RL:</p> <ul> <li>Lillicrap, T. P., et al. (2016). \"Continuous control with deep reinforcement learning\" (DDPG). ICLR.</li> <li>Haarnoja, T., et al. (2018). \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor\" (SAC). ICML.</li> </ul> <p>Langevin Dynamics for RL:</p> <ul> <li>Levine, S. (2018). \"Reinforcement learning and control as probabilistic inference: Tutorial and review.\" arXiv:1805.00909.</li> <li>Ajay, A., et al. (2023). \"Is conditional generative modeling all you need for decision making?\" ICLR (Diffusion-QL).</li> </ul> <p>Action Embeddings:</p> <ul> <li>van den Oord, A., Li, Y., &amp; Vinyals, O. (2018). \"Representation learning with contrastive predictive coding.\" arXiv:1807.03748.</li> <li>Kipf, T., et al. (2020). \"Contrastive learning of structured world models.\" ICLR.</li> </ul> <p>Options Framework:</p> <ul> <li>Sutton, R. S., Precup, D., &amp; Singh, S. (1999). \"Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning.\" Artificial Intelligence.</li> </ul> <p>\u2190 Back to Chapter 07: RF-SARSA | Next: Chapter 08 \u2192</p> <p>Related: Chapter 03a - Least Action Principle | Related: Chapter 04a - Riesz Representer</p> <p>Last Updated: January 14, 2026</p>"},{"location":"notebooks/","title":"Interactive Notebooks","text":"<p>Welcome to the GRL interactive demonstrations! These Jupyter notebooks provide hands-on visualizations for understanding reinforcement fields.</p>"},{"location":"notebooks/#field-series-understanding-grl-through-visualization","title":"\ud83d\udcda Field Series: Understanding GRL Through Visualization","text":"<p>A progressive notebook series building intuition from classical vector fields to GRL's reinforcement fields.</p>"},{"location":"notebooks/#field-series-overview","title":"Field Series Overview","text":"<p>Complete Series | \u23f1\ufe0f ~60-90 minutes total</p> # Notebook Status Time 0 Introduction to Vector Fields \u2705 Complete ~10-15 min 1 Classical Vector Fields \u2705 Complete ~20-25 min 1a Vector Fields and ODEs \u2705 Complete ~25-30 min 2 Functional Fields \u2705 Complete ~20-25 min 3 Reinforcement Fields \u2705 Complete ~30 min"},{"location":"notebooks/#complete-documentation","title":"\ud83d\uddfa\ufe0f Complete Documentation","text":"<p>Learn More: - \ud83d\udcd6 Field Series Roadmap \u2014 Planned future notebooks (Policy Inference, Memory Update, RF-SARSA) - \ud83c\udfaf Learning Paths \u2014 How to use these notebooks - \ud83d\udd17 genai-lab Connection \u2014 Flow Matching &amp; Diffusion Models</p>"},{"location":"notebooks/#quick-start","title":"\ud83d\udca1 Quick Start","text":"<p>Best viewing experience: 1. Click notebook links above (rendered on this site) 2. Math and plots display correctly 3. No GitHub timeout issues!</p> <p>Want to run interactively? <pre><code>git clone https://github.com/pleiadian53/GRL.git\ncd GRL/notebooks\njupyter notebook\n</code></pre></p>"},{"location":"notebooks/#related-resources","title":"\ud83d\udcd6 Related Resources","text":"<ul> <li>Tutorial Series \u2014 Mathematical depth</li> <li>Implementation Guide \u2014 Technical specs</li> <li>Quantum-Inspired Extensions \u2014 Advanced topics</li> </ul>"},{"location":"notebooks/#note-for-contributors","title":"\ud83d\udccd Note for Contributors","text":"<p>These notebooks are rendered copies from <code>/notebooks/</code> in the repository.</p> <p>Development workflow: 1. Develop in <code>/notebooks/</code> (primary location) 2. Copy to <code>/docs/notebooks/</code> when ready to publish 3. GitHub Actions deploys automatically</p> <p>Why two locations? - <code>/notebooks/</code> \u2014 Source of truth, easy to find - <code>/docs/notebooks/</code> \u2014 Rendered for reliable display</p>"},{"location":"notebooks/field_series/","title":"Field Series: From Vector Fields to Reinforcement Fields","text":"<p>Understanding GRL's Core Concept Through Progressive Visualization</p> <p>This series of notebooks builds intuition for GRL's reinforcement field by starting with familiar concepts (classical vector fields) and progressively introducing the abstract notion of functional fields.</p>"},{"location":"notebooks/field_series/#series-overview","title":"Series Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Notebook 1: Classical Vector Fields    \u2502\n\u2502  (Concrete: arrows at points)           \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502 You understand: arrows at points\n               \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Notebook 2: Functional Fields          \u2502\n\u2502  (Abstract: functions as vectors)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502 You understand: functions at points\n               \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Notebook 3: Reinforcement Fields       \u2502\n\u2502  (GRL's Q\u207a field in RKHS)               \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502 You understand: GRL's learning mechanism!\n</code></pre>"},{"location":"notebooks/field_series/#notebooks","title":"Notebooks","text":"# Notebook Status Description 0 <code>00_intro_vector_fields.ipynb</code> \u2705 Complete Gentle intro with real-world examples (optional) 1 <code>01_classical_vector_fields.ipynb</code> \u2705 Complete Gradient fields, rotational fields, superposition, trajectories 1a <code>01a_vector_fields_and_odes.ipynb</code> \u2705 Complete ODEs, numerical solvers (Euler/RK4), flow matching connection \ud83d\udd17 2 <code>02_functional_fields.ipynb</code> \u2705 Complete Functions as vectors, kernels, RKHS intuition 3 <code>03_reinforcement_fields/</code> \u2705 Complete GRL's Q\u207a field, 2D navigation domain, policy inference \ud83d\udcc1 <p>Note: Notebook 3 is in a subdirectory with supplementary materials: - <code>03_reinforcement_fields/03_reinforcement_fields.ipynb</code> \u2014 Main notebook - <code>03_reinforcement_fields/03a_particle_coverage_effects.ipynb</code> \u2014 Visual proof of particle coverage effects - <code>03_reinforcement_fields/particle_vs_gradient_fields.md</code> \u2014 Theory note</p> <p>\ud83d\udccb See ROADMAP.md for planned future notebooks (Policy Inference, Memory Update, RF-SARSA)</p>"},{"location":"notebooks/field_series/#key-concepts","title":"Key Concepts","text":""},{"location":"notebooks/field_series/#notebook-1-classical-vector-fields","title":"Notebook 1: Classical Vector Fields","text":"<ul> <li>Vector field definition: \\(\\mathbf{F}(x, y) = [F_x(x,y), F_y(x,y)]^T\\)</li> <li>Gradient fields: \\(\\nabla V\\) points uphill on potential \\(V\\)</li> <li>Rotational fields: Circular flow, curl</li> <li>Superposition: \\(\\mathbf{F}_{\\text{total}} = \\mathbf{F}_1 + \\mathbf{F}_2\\)</li> <li>Trajectories: Following the field to find extrema</li> </ul>"},{"location":"notebooks/field_series/#notebook-2-functional-fields","title":"Notebook 2: Functional Fields","text":"<ul> <li>Functions as vectors: Addition, scaling, inner products</li> <li>Kernel functions: \\(k(x, x')\\) as similarity measure</li> <li>RKHS: Reproducing Kernel Hilbert Space</li> <li>Functional gradient: Gradient in function space</li> </ul>"},{"location":"notebooks/field_series/#notebook-3-reinforcement-fields","title":"Notebook 3: Reinforcement Fields","text":"<ul> <li>Augmented space: \\(z = (s, \\theta)\\) \u2014 state-action pairs</li> <li>Particle memory: \\(\\{(z_i, w_i)\\}\\) \u2014 weighted experience points</li> <li>Reinforcement field: \\(Q^+(z) = \\sum_i w_i \\, k(z, z_i)\\)</li> <li>Policy inference: Reading the field to choose actions</li> </ul>"},{"location":"notebooks/field_series/#related-projects","title":"Related Projects","text":""},{"location":"notebooks/field_series/#genai-lab-generative-ai-diffusion-models","title":"\ud83d\udd17 genai-lab \u2014 Generative AI &amp; Diffusion Models","text":"<p>Notebook 1a bridges to the genai-lab project, which covers flow matching and diffusion models \u2014 both built on the same vector field / ODE foundations:</p> Topic genai-lab Document Flow Matching <code>docs/flow_matching/01_flow_matching_foundations.md</code> Diffusion Models <code>docs/DDPM/01_ddpm_foundations.md</code> Diffusion Transformers <code>docs/diffusion/DiT/diffusion_transformer.md</code> <p>Shared concepts: Velocity fields, ODE solvers (Euler, RK4), probability transport, gradient-based sampling.</p>"},{"location":"notebooks/field_series/#learning-paths","title":"Learning Paths","text":""},{"location":"notebooks/field_series/#quick-visual-tour-45-min","title":"Quick Visual Tour (~45 min)","text":"<p>Run through all three notebooks, focusing on visualizations.</p>"},{"location":"notebooks/field_series/#deep-understanding-2-hours","title":"Deep Understanding (~2 hours)","text":"<p>Combine notebooks with tutorial chapters: - Notebook 2 \u2192 Tutorial Ch 2: RKHS Foundations - Notebook 3 \u2192 Tutorial Ch 4: Reinforcement Field</p>"},{"location":"notebooks/field_series/#running-the-notebooks","title":"Running the Notebooks","text":"<pre><code>cd GRL/notebooks/field_series\nconda activate grl  # or your environment\njupyter lab\n</code></pre>"},{"location":"notebooks/field_series/#dependencies","title":"Dependencies","text":"<pre><code>numpy\nmatplotlib\nseaborn\nipywidgets (optional, for interactivity)\n</code></pre>"},{"location":"notebooks/field_series/#related-documentation","title":"Related Documentation","text":"<ul> <li>Tutorial Series</li> <li>RKHS Foundations</li> <li>Reinforcement Field</li> <li>Original Paper</li> </ul> <p>Last Updated: January 15, 2026</p>"},{"location":"notebooks/field_series/00_intro_vector_fields/","title":"00: Introduction to Vector Fields","text":"In\u00a0[\u00a0]: Copied! <pre># Setup: Import libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\n# Set visualization style\nsns.set_theme(style='whitegrid', context='notebook')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 11\n\n# For inline plots\n%matplotlib inline\n\nprint(\"\u2713 Libraries loaded successfully!\")\n</pre> # Setup: Import libraries import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D import seaborn as sns  # Set visualization style sns.set_theme(style='whitegrid', context='notebook') plt.rcParams['figure.figsize'] = (12, 8) plt.rcParams['font.size'] = 11  # For inline plots %matplotlib inline  print(\"\u2713 Libraries loaded successfully!\") In\u00a0[\u00a0]: Copied! <pre># Example 1: Simple Linear Vector Field\n# F(x, y) = (x, y) \u2014 arrows point away from origin\n\n# Create a grid of points\nx = np.linspace(-2, 2, 15)\ny = np.linspace(-2, 2, 15)\nX, Y = np.meshgrid(x, y)\n\n# Define the vector field: F(x,y) = (x, y)\nU = X  # x-component\nV = Y  # y-component\n\n# Compute magnitude for coloring\nmagnitude = np.sqrt(U**2 + V**2)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 10))\nquiver = ax.quiver(X, Y, U, V, magnitude, cmap='viridis', alpha=0.8, scale=20)\nax.set_xlabel('x', fontsize=14)\nax.set_ylabel('y', fontsize=14)\nax.set_title('Linear Vector Field: $\\\\mathbf{F}(x,y) = (x, y)$', fontsize=16)\nax.grid(True, alpha=0.3)\nax.set_aspect('equal')\nplt.colorbar(quiver, ax=ax, label='Magnitude')\nplt.show()\n\nprint(\"Key Observation: Arrows point AWAY from the origin\")\nprint(\"                 Magnitude increases with distance from center\")\n</pre> # Example 1: Simple Linear Vector Field # F(x, y) = (x, y) \u2014 arrows point away from origin  # Create a grid of points x = np.linspace(-2, 2, 15) y = np.linspace(-2, 2, 15) X, Y = np.meshgrid(x, y)  # Define the vector field: F(x,y) = (x, y) U = X  # x-component V = Y  # y-component  # Compute magnitude for coloring magnitude = np.sqrt(U**2 + V**2)  # Visualize fig, ax = plt.subplots(figsize=(10, 10)) quiver = ax.quiver(X, Y, U, V, magnitude, cmap='viridis', alpha=0.8, scale=20) ax.set_xlabel('x', fontsize=14) ax.set_ylabel('y', fontsize=14) ax.set_title('Linear Vector Field: $\\\\mathbf{F}(x,y) = (x, y)$', fontsize=16) ax.grid(True, alpha=0.3) ax.set_aspect('equal') plt.colorbar(quiver, ax=ax, label='Magnitude') plt.show()  print(\"Key Observation: Arrows point AWAY from the origin\") print(\"                 Magnitude increases with distance from center\") In\u00a0[\u00a0]: Copied! <pre># Example: Gradient of a Parabolic Bowl\n# Potential: V(x,y) = x^2 + y^2\n# Gradient: \u2207V = (2x, 2y)\n\ndef potential(x, y):\n    \"\"\"Parabolic bowl potential\"\"\"\n    return x**2 + y**2\n\ndef gradient_field(x, y):\n    \"\"\"Gradient of the potential\"\"\"\n    return 2*x, 2*y\n\n# Create grid (finer for smoother surface)\nx = np.linspace(-2, 2, 20)\ny = np.linspace(-2, 2, 20)\nX, Y = np.meshgrid(x, y)\n\n# Compute potential and gradient\nZ = potential(X, Y)\nU, V = gradient_field(X, Y)\n\n# Create side-by-side visualization\nfig = plt.figure(figsize=(18, 7))\n\n# Left: Potential surface (3D)\nax1 = fig.add_subplot(121, projection='3d')\nsurf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none')\nax1.set_xlabel('x', fontsize=12)\nax1.set_ylabel('y', fontsize=12)\nax1.set_zlabel('$V(x,y)$', fontsize=12)\nax1.set_title('Potential Surface: $V(x,y) = x^2 + y^2$', fontsize=14)\nfig.colorbar(surf, ax=ax1, shrink=0.5, label='V(x,y)')\n\n# Right: Gradient field (2D)\nax2 = fig.add_subplot(122)\ncontour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.4)\nquiver = ax2.quiver(X, Y, U, V, color='red', alpha=0.8, scale=40)\nax2.set_xlabel('x', fontsize=14)\nax2.set_ylabel('y', fontsize=14)\nax2.set_title('Gradient Field: $\\\\nabla V = (2x, 2y)$', fontsize=14)\nax2.set_aspect('equal')\nax2.grid(True, alpha=0.3)\nfig.colorbar(contour, ax=ax2, label='V(x,y)')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\u2713 Key Insights:\")\nprint(\"  1. Gradient arrows point UPHILL on the potential surface\")\nprint(\"  2. At the minimum (0,0), gradient is ZERO (no arrows)\")\nprint(\"  3. Magnitude grows with distance from minimum\")\nprint(\"  4. To MINIMIZE: follow NEGATIVE gradient (gradient descent)\")\nprint(\"  5. To MAXIMIZE: follow POSITIVE gradient (gradient ascent)\")\n</pre> # Example: Gradient of a Parabolic Bowl # Potential: V(x,y) = x^2 + y^2 # Gradient: \u2207V = (2x, 2y)  def potential(x, y):     \"\"\"Parabolic bowl potential\"\"\"     return x**2 + y**2  def gradient_field(x, y):     \"\"\"Gradient of the potential\"\"\"     return 2*x, 2*y  # Create grid (finer for smoother surface) x = np.linspace(-2, 2, 20) y = np.linspace(-2, 2, 20) X, Y = np.meshgrid(x, y)  # Compute potential and gradient Z = potential(X, Y) U, V = gradient_field(X, Y)  # Create side-by-side visualization fig = plt.figure(figsize=(18, 7))  # Left: Potential surface (3D) ax1 = fig.add_subplot(121, projection='3d') surf = ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8, edgecolor='none') ax1.set_xlabel('x', fontsize=12) ax1.set_ylabel('y', fontsize=12) ax1.set_zlabel('$V(x,y)$', fontsize=12) ax1.set_title('Potential Surface: $V(x,y) = x^2 + y^2$', fontsize=14) fig.colorbar(surf, ax=ax1, shrink=0.5, label='V(x,y)')  # Right: Gradient field (2D) ax2 = fig.add_subplot(122) contour = ax2.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.4) quiver = ax2.quiver(X, Y, U, V, color='red', alpha=0.8, scale=40) ax2.set_xlabel('x', fontsize=14) ax2.set_ylabel('y', fontsize=14) ax2.set_title('Gradient Field: $\\\\nabla V = (2x, 2y)$', fontsize=14) ax2.set_aspect('equal') ax2.grid(True, alpha=0.3) fig.colorbar(contour, ax=ax2, label='V(x,y)')  plt.tight_layout() plt.show()  print(\"\u2713 Key Insights:\") print(\"  1. Gradient arrows point UPHILL on the potential surface\") print(\"  2. At the minimum (0,0), gradient is ZERO (no arrows)\") print(\"  3. Magnitude grows with distance from minimum\") print(\"  4. To MINIMIZE: follow NEGATIVE gradient (gradient descent)\") print(\"  5. To MAXIMIZE: follow POSITIVE gradient (gradient ascent)\")"},{"location":"notebooks/field_series/00_intro_vector_fields/#introduction-to-vector-fields","title":"Introduction to Vector Fields\u00b6","text":"<p>Part 0 of the GRL Field Series \u2014 Gentle Introduction</p> <p>Welcome! This notebook provides a gentle introduction to vector fields with real-world examples and intuition-building.</p>"},{"location":"notebooks/field_series/00_intro_vector_fields/#learning-objectives","title":"Learning Objectives\u00b6","text":"<p>By the end of this notebook, you'll understand:</p> <ol> <li>What is a vector field? (arrows at each point in space)</li> <li>Gradient fields \u2014 following \"uphill\" directions</li> </ol> <p>Prerequisites: Basic calculus (derivatives), linear algebra (vectors)</p> <p>Time: ~10-15 minutes</p>"},{"location":"notebooks/field_series/00_intro_vector_fields/#why-this-matters-for-grl","title":"Why This Matters for GRL\u00b6","text":"<p>Vector fields are the stepping stone to functional fields. Once you understand how arrows at each point create a field, you'll be ready to understand how functions at each point create GRL's reinforcement field!</p> <p>Progression:</p> <pre><code>Introduction (this notebook) \u2190 You are here\n    \u2193\nClassical Vector Fields (Notebook 1)\n    \u2193\nFunctional Fields (Notebook 2)\n    \u2193\nGRL Reinforcement Fields (Notebook 3)\n</code></pre> <p>Next: For a complete treatment including rotational fields, superposition, and trajectories, see <code>01_classical_vector_fields.ipynb</code>.</p>"},{"location":"notebooks/field_series/00_intro_vector_fields/#part-1-what-is-a-vector-field","title":"Part 1: What is a Vector Field?\u00b6","text":""},{"location":"notebooks/field_series/00_intro_vector_fields/#definition","title":"Definition\u00b6","text":"<p>A vector field assigns a vector (arrow) to every point in space.</p> <p>Mathematically, in 2D: $$ \\mathbf{F}(x, y) = \\begin{bmatrix} F_x(x, y) \\\\ F_y(x, y) \\end{bmatrix} $$</p> <p>At each point $(x, y)$, the field $\\mathbf{F}$ gives us a 2D vector with:</p> <ul> <li>Direction: Where the arrow points</li> <li>Magnitude: How long the arrow is</li> </ul>"},{"location":"notebooks/field_series/00_intro_vector_fields/#real-world-examples","title":"Real-World Examples\u00b6","text":"<ul> <li>Gravity field: Arrows point downward, magnitude depends on mass/distance</li> <li>Wind field: Arrows show wind direction and speed at each location</li> <li>Electric field: Arrows show force on a charged particle</li> <li>Fluid flow: Arrows show velocity of water/air at each point</li> </ul>"},{"location":"notebooks/field_series/00_intro_vector_fields/#visual-representation","title":"Visual Representation\u00b6","text":"<p>We typically use:</p> <ul> <li>Quiver plots: Arrows at sampled grid points</li> <li>Streamlines: Curves that follow the field direction</li> <li>Color maps: Color intensity shows magnitude</li> </ul> <p>Let's create our first vector field!</p>"},{"location":"notebooks/field_series/00_intro_vector_fields/#part-2-gradient-fields-the-path-to-optimization","title":"Part 2: Gradient Fields \u2014 The Path to Optimization\u00b6","text":""},{"location":"notebooks/field_series/00_intro_vector_fields/#what-is-a-gradient","title":"What is a Gradient?\u00b6","text":"<p>Given a scalar function $V(x, y)$ (called a potential), its gradient is: $$ \\nabla V = \\begin{bmatrix} \\frac{\\partial V}{\\partial x} \\\\ \\frac{\\partial V}{\\partial y} \\end{bmatrix} $$</p> <p>Key Property: The gradient points in the direction of steepest ascent!</p>"},{"location":"notebooks/field_series/00_intro_vector_fields/#connection-to-grl","title":"Connection to GRL\u00b6","text":"<p>In GRL, the reinforcement field $Q^+(s, a)$ is like a potential. Its gradient tells us which direction to move in state-action space to improve expected reward!</p> <p>Gradient descent (going downhill) \u2192 Gradient ascent (going uphill in GRL!)</p> <p>Let's visualize a gradient field.</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/","title":"01: Classical Vector Fields","text":"In\u00a0[1]: Copied! <pre># Setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\n# Optional interactivity\nINTERACTIVE = False\ntry:\n    import ipywidgets as widgets\n    from IPython.display import display\n    WIDGETS_AVAILABLE = True\nexcept ImportError:\n    WIDGETS_AVAILABLE = False\n\nsns.set_theme(style='whitegrid', context='notebook')\nplt.rcParams['figure.figsize'] = (12, 8)\n%matplotlib inline\nprint(f\"Libraries loaded. Interactive: {INTERACTIVE and WIDGETS_AVAILABLE}\")\n</pre> # Setup import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D import seaborn as sns  # Optional interactivity INTERACTIVE = False try:     import ipywidgets as widgets     from IPython.display import display     WIDGETS_AVAILABLE = True except ImportError:     WIDGETS_AVAILABLE = False  sns.set_theme(style='whitegrid', context='notebook') plt.rcParams['figure.figsize'] = (12, 8) %matplotlib inline print(f\"Libraries loaded. Interactive: {INTERACTIVE and WIDGETS_AVAILABLE}\") <pre>Libraries loaded. Interactive: False\n</pre> In\u00a0[2]: Copied! <pre># Example 1.1: Linear Vector Field F(x,y) = (x, y)\nx = np.linspace(-2, 2, 15)\ny = np.linspace(-2, 2, 15)\nX, Y = np.meshgrid(x, y)\nU, V = X, Y  # F(x,y) = (x, y)\nmagnitude = np.sqrt(U**2 + V**2)\n\nfig, ax = plt.subplots(figsize=(10, 10))\nquiver = ax.quiver(X, Y, U, V, magnitude, cmap='viridis', alpha=0.8, scale=25)\nax.plot(0, 0, 'ro', markersize=10, label='Origin')\nax.set_xlabel('$x$'); ax.set_ylabel('$y$')\nax.set_title(r'Linear Vector Field: $\\mathbf{F}(x,y) = (x, y)$')\nax.set_aspect('equal'); ax.legend()\nplt.colorbar(quiver, label='Magnitude')\nplt.show()\nprint(\"Arrows point AWAY from origin; magnitude increases with distance.\")\n</pre> # Example 1.1: Linear Vector Field F(x,y) = (x, y) x = np.linspace(-2, 2, 15) y = np.linspace(-2, 2, 15) X, Y = np.meshgrid(x, y) U, V = X, Y  # F(x,y) = (x, y) magnitude = np.sqrt(U**2 + V**2)  fig, ax = plt.subplots(figsize=(10, 10)) quiver = ax.quiver(X, Y, U, V, magnitude, cmap='viridis', alpha=0.8, scale=25) ax.plot(0, 0, 'ro', markersize=10, label='Origin') ax.set_xlabel('$x$'); ax.set_ylabel('$y$') ax.set_title(r'Linear Vector Field: $\\mathbf{F}(x,y) = (x, y)$') ax.set_aspect('equal'); ax.legend() plt.colorbar(quiver, label='Magnitude') plt.show() print(\"Arrows point AWAY from origin; magnitude increases with distance.\") <pre>Arrows point AWAY from origin; magnitude increases with distance.\n</pre> In\u00a0[3]: Copied! <pre># Example 2.1: Gradient of Parabolic Bowl V(x,y) = x\u00b2 + y\u00b2\nx = np.linspace(-2, 2, 20)\ny = np.linspace(-2, 2, 20)\nX, Y = np.meshgrid(x, y)\nZ = X**2 + Y**2  # Potential\nU, V = 2*X, 2*Y  # Gradient\n\nfig = plt.figure(figsize=(16, 6))\n\n# 3D Surface\nax1 = fig.add_subplot(131, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8)\nax1.set_xlabel('$x$'); ax1.set_ylabel('$y$'); ax1.set_zlabel('$V$')\nax1.set_title(r'Potential: $V = x^2 + y^2$')\n\n# Gradient field (ascent)\nax2 = fig.add_subplot(132)\nax2.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\nax2.quiver(X[::2,::2], Y[::2,::2], U[::2,::2], V[::2,::2], color='red', scale=50)\nax2.plot(0, 0, 'k*', markersize=15)\nax2.set_title(r'$\\nabla V$ (ascent)'); ax2.set_aspect('equal')\n\n# Negative gradient (descent)\nax3 = fig.add_subplot(133)\nax3.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.5)\nax3.quiver(X[::2,::2], Y[::2,::2], -U[::2,::2], -V[::2,::2], color='blue', scale=50)\nax3.plot(0, 0, 'k*', markersize=15)\nax3.set_title(r'$-\\nabla V$ (descent)'); ax3.set_aspect('equal')\n\nplt.tight_layout(); plt.show()\nprint(\"Red=ascent (uphill), Blue=descent (downhill). At minimum: gradient=0.\")\n</pre> # Example 2.1: Gradient of Parabolic Bowl V(x,y) = x\u00b2 + y\u00b2 x = np.linspace(-2, 2, 20) y = np.linspace(-2, 2, 20) X, Y = np.meshgrid(x, y) Z = X**2 + Y**2  # Potential U, V = 2*X, 2*Y  # Gradient  fig = plt.figure(figsize=(16, 6))  # 3D Surface ax1 = fig.add_subplot(131, projection='3d') ax1.plot_surface(X, Y, Z, cmap='viridis', alpha=0.8) ax1.set_xlabel('$x$'); ax1.set_ylabel('$y$'); ax1.set_zlabel('$V$') ax1.set_title(r'Potential: $V = x^2 + y^2$')  # Gradient field (ascent) ax2 = fig.add_subplot(132) ax2.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.5) ax2.quiver(X[::2,::2], Y[::2,::2], U[::2,::2], V[::2,::2], color='red', scale=50) ax2.plot(0, 0, 'k*', markersize=15) ax2.set_title(r'$\\nabla V$ (ascent)'); ax2.set_aspect('equal')  # Negative gradient (descent) ax3 = fig.add_subplot(133) ax3.contourf(X, Y, Z, levels=20, cmap='viridis', alpha=0.5) ax3.quiver(X[::2,::2], Y[::2,::2], -U[::2,::2], -V[::2,::2], color='blue', scale=50) ax3.plot(0, 0, 'k*', markersize=15) ax3.set_title(r'$-\\nabla V$ (descent)'); ax3.set_aspect('equal')  plt.tight_layout(); plt.show() print(\"Red=ascent (uphill), Blue=descent (downhill). At minimum: gradient=0.\") <pre>Red=ascent (uphill), Blue=descent (downhill). At minimum: gradient=0.\n</pre> In\u00a0[4]: Copied! <pre># Example 2.2: Multi-Modal Potential (Two Peaks)\ndef multimodal(X, Y):\n    return 2*np.exp(-((X-1)**2+(Y-1)**2)/0.5) + 1.5*np.exp(-((X+1)**2+(Y+0.5)**2)/0.8)\n\nx = np.linspace(-3, 3, 30)\ny = np.linspace(-2.5, 3, 30)\nX, Y = np.meshgrid(x, y)\nZ = multimodal(X, Y)\n\n# Numerical gradient\neps = 1e-5\nU = (multimodal(X+eps, Y) - multimodal(X-eps, Y)) / (2*eps)\nV = (multimodal(X, Y+eps) - multimodal(X, Y-eps)) / (2*eps)\n\nfig = plt.figure(figsize=(14, 5))\nax1 = fig.add_subplot(121, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='plasma', alpha=0.9)\nax1.set_title('Multi-Modal Potential')\n\nax2 = fig.add_subplot(122)\nax2.contourf(X, Y, Z, levels=25, cmap='plasma', alpha=0.6)\nax2.streamplot(X, Y, U, V, color='gray', density=1.5)\nax2.plot(1, 1, 'w*', markersize=15, markeredgecolor='k')\nax2.plot(-1, 0.5, 'w^', markersize=12, markeredgecolor='k')\nax2.set_title('Streamlines (Gradient Ascent Paths)'); ax2.set_aspect('equal')\nplt.tight_layout(); plt.show()\nprint(\"Multiple peaks = multiple 'good' regions. Gradient points to nearest peak.\")\n</pre> # Example 2.2: Multi-Modal Potential (Two Peaks) def multimodal(X, Y):     return 2*np.exp(-((X-1)**2+(Y-1)**2)/0.5) + 1.5*np.exp(-((X+1)**2+(Y+0.5)**2)/0.8)  x = np.linspace(-3, 3, 30) y = np.linspace(-2.5, 3, 30) X, Y = np.meshgrid(x, y) Z = multimodal(X, Y)  # Numerical gradient eps = 1e-5 U = (multimodal(X+eps, Y) - multimodal(X-eps, Y)) / (2*eps) V = (multimodal(X, Y+eps) - multimodal(X, Y-eps)) / (2*eps)  fig = plt.figure(figsize=(14, 5)) ax1 = fig.add_subplot(121, projection='3d') ax1.plot_surface(X, Y, Z, cmap='plasma', alpha=0.9) ax1.set_title('Multi-Modal Potential')  ax2 = fig.add_subplot(122) ax2.contourf(X, Y, Z, levels=25, cmap='plasma', alpha=0.6) ax2.streamplot(X, Y, U, V, color='gray', density=1.5) ax2.plot(1, 1, 'w*', markersize=15, markeredgecolor='k') ax2.plot(-1, 0.5, 'w^', markersize=12, markeredgecolor='k') ax2.set_title('Streamlines (Gradient Ascent Paths)'); ax2.set_aspect('equal') plt.tight_layout(); plt.show() print(\"Multiple peaks = multiple 'good' regions. Gradient points to nearest peak.\") <pre>Multiple peaks = multiple 'good' regions. Gradient points to nearest peak.\n</pre> In\u00a0[\u00a0]: Copied! <pre># Example 3.1: Rotational Fields\nx = np.linspace(-2, 2, 20)\ny = np.linspace(-2, 2, 20)\nX, Y = np.meshgrid(x, y)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Pure rotation\nU, V = -Y, X\naxes[0].quiver(X, Y, U, V, np.sqrt(U**2+V**2), cmap='coolwarm', scale=30)\naxes[0].streamplot(X, Y, U, V, color='gray', density=1.2, linewidth=0.5)\naxes[0].set_title(r'Pure Rotation: $\\mathbf{F} = (-y, x)$'); axes[0].set_aspect('equal')\n\n# Vortex\nr2 = X**2 + Y**2 + 0.1\nU, V = -Y/r2, X/r2\naxes[1].quiver(X, Y, U, V, np.sqrt(U**2+V**2), cmap='coolwarm', scale=15)\naxes[1].streamplot(X, Y, U, V, color='gray', density=1.2, linewidth=0.5)\naxes[1].set_title('Vortex (decaying rotation)'); axes[1].set_aspect('equal')\n\nplt.tight_layout(); plt.show()\nprint(\"Rotational fields have curl\u22600. GRL's Q\u207a is a gradient field (curl=0).\")\n</pre> # Example 3.1: Rotational Fields x = np.linspace(-2, 2, 20) y = np.linspace(-2, 2, 20) X, Y = np.meshgrid(x, y)  fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # Pure rotation U, V = -Y, X axes[0].quiver(X, Y, U, V, np.sqrt(U**2+V**2), cmap='coolwarm', scale=30) axes[0].streamplot(X, Y, U, V, color='gray', density=1.2, linewidth=0.5) axes[0].set_title(r'Pure Rotation: $\\mathbf{F} = (-y, x)$'); axes[0].set_aspect('equal')  # Vortex r2 = X**2 + Y**2 + 0.1 U, V = -Y/r2, X/r2 axes[1].quiver(X, Y, U, V, np.sqrt(U**2+V**2), cmap='coolwarm', scale=15) axes[1].streamplot(X, Y, U, V, color='gray', density=1.2, linewidth=0.5) axes[1].set_title('Vortex (decaying rotation)'); axes[1].set_aspect('equal')  plt.tight_layout(); plt.show() print(\"Rotational fields have curl\u22600. GRL's Q\u207a is a gradient field (curl=0).\") <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[5], line 11\n      9 U, V = -Y, X\n     10 axes[0].quiver(X, Y, U, V, np.sqrt(U**2+V**2), cmap='coolwarm', scale=30)\n---&gt; 11 axes[0].streamplot(X, Y, U, V, color='gray', density=1.2, alpha=0.5)\n     12 axes[0].set_title(r'Pure Rotation: $\\mathbf{F} = (-y, x)$'); axes[0].set_aspect('equal')\n     14 # Vortex\n\nFile ~/miniforge3-new/envs/grl/lib/python3.12/site-packages/matplotlib/__init__.py:1524, in _preprocess_data.&lt;locals&gt;.inner(ax, data, *args, **kwargs)\n   1521 @functools.wraps(func)\n   1522 def inner(ax, *args, data=None, **kwargs):\n   1523     if data is None:\n-&gt; 1524         return func(\n   1525             ax,\n   1526             *map(cbook.sanitize_sequence, args),\n   1527             **{k: cbook.sanitize_sequence(v) for k, v in kwargs.items()})\n   1529     bound = new_sig.bind(ax, *args, **kwargs)\n   1530     auto_label = (bound.arguments.get(label_namer)\n   1531                   or bound.kwargs.get(label_namer))\n\nTypeError: Axes.streamplot() got an unexpected keyword argument 'alpha'</pre> In\u00a0[\u00a0]: Copied! <pre># Example 4.1: Superposition of Gaussian Bumps (Preview of GRL!)\ndef gaussian_bump(X, Y, x0, y0, w, l=0.5):\n    return w * np.exp(-((X-x0)**2 + (Y-y0)**2) / (2*l**2))\n\ndef gaussian_grad(X, Y, x0, y0, w, l=0.5):\n    bump = gaussian_bump(X, Y, x0, y0, w, l)\n    return -w/(l**2) * (X-x0) * bump / w, -w/(l**2) * (Y-y0) * bump / w\n\nx = np.linspace(-3, 3, 30)\ny = np.linspace(-3, 3, 30)\nX, Y = np.meshgrid(x, y)\n\n# Particles: (x, y, weight, color)\nparticles = [(1.5, 1.0, 2.0, 'blue'), (-1.0, 1.5, 1.5, 'blue'),\n             (0.0, -1.0, -1.5, 'red'), (-1.5, -1.5, -1.0, 'red')]\n\nZ = sum(gaussian_bump(X, Y, p[0], p[1], p[2]) for p in particles)\n\nfig = plt.figure(figsize=(16, 5))\nax1 = fig.add_subplot(131, projection='3d')\nax1.plot_surface(X, Y, Z, cmap='RdBu_r', alpha=0.9)\nax1.set_title(\"Superposition of Gaussian Bumps\\n(Preview of GRL's Q\u207a)\")\n\nax2 = fig.add_subplot(132)\nc = ax2.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.7)\nax2.contour(X, Y, Z, levels=[0], colors='k', linewidths=2, linestyles='--')\nfor p in particles:\n    ax2.plot(p[0], p[1], 'o' if p[2]&gt;0 else 's', color=p[3], markersize=12, mec='k', mew=2)\nax2.set_title('Potential (dashed=zero)'); ax2.set_aspect('equal')\nplt.colorbar(c, ax=ax2)\n\n# Gradient\neps = 1e-5\nU = (sum(gaussian_bump(X+eps, Y, p[0], p[1], p[2]) for p in particles) -\n     sum(gaussian_bump(X-eps, Y, p[0], p[1], p[2]) for p in particles)) / (2*eps)\nV = (sum(gaussian_bump(X, Y+eps, p[0], p[1], p[2]) for p in particles) -\n     sum(gaussian_bump(X, Y-eps, p[0], p[1], p[2]) for p in particles)) / (2*eps)\n\nax3 = fig.add_subplot(133)\nax3.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.5)\nax3.quiver(X[::2,::2], Y[::2,::2], U[::2,::2], V[::2,::2], color='k', alpha=0.7, scale=30)\nfor p in particles:\n    ax3.plot(p[0], p[1], 'o' if p[2]&gt;0 else 's', color=p[3], markersize=12, mec='k', mew=2)\nax3.set_title(r'Gradient $\\nabla V$'); ax3.set_aspect('equal')\n\nplt.tight_layout(); plt.show()\nprint(\"This IS how GRL works! Q\u207a(z) = \u03a3\u1d62 w\u1d62 k(z, z\u1d62)\")\nprint(\"Blue=positive (good), Red=negative (bad). Gradient \u2192 high-value regions.\")\n</pre> # Example 4.1: Superposition of Gaussian Bumps (Preview of GRL!) def gaussian_bump(X, Y, x0, y0, w, l=0.5):     return w * np.exp(-((X-x0)**2 + (Y-y0)**2) / (2*l**2))  def gaussian_grad(X, Y, x0, y0, w, l=0.5):     bump = gaussian_bump(X, Y, x0, y0, w, l)     return -w/(l**2) * (X-x0) * bump / w, -w/(l**2) * (Y-y0) * bump / w  x = np.linspace(-3, 3, 30) y = np.linspace(-3, 3, 30) X, Y = np.meshgrid(x, y)  # Particles: (x, y, weight, color) particles = [(1.5, 1.0, 2.0, 'blue'), (-1.0, 1.5, 1.5, 'blue'),              (0.0, -1.0, -1.5, 'red'), (-1.5, -1.5, -1.0, 'red')]  Z = sum(gaussian_bump(X, Y, p[0], p[1], p[2]) for p in particles)  fig = plt.figure(figsize=(16, 5)) ax1 = fig.add_subplot(131, projection='3d') ax1.plot_surface(X, Y, Z, cmap='RdBu_r', alpha=0.9) ax1.set_title(\"Superposition of Gaussian Bumps\\n(Preview of GRL's Q\u207a)\")  ax2 = fig.add_subplot(132) c = ax2.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.7) ax2.contour(X, Y, Z, levels=[0], colors='k', linewidths=2, linestyles='--') for p in particles:     ax2.plot(p[0], p[1], 'o' if p[2]&gt;0 else 's', color=p[3], markersize=12, mec='k', mew=2) ax2.set_title('Potential (dashed=zero)'); ax2.set_aspect('equal') plt.colorbar(c, ax=ax2)  # Gradient eps = 1e-5 U = (sum(gaussian_bump(X+eps, Y, p[0], p[1], p[2]) for p in particles) -      sum(gaussian_bump(X-eps, Y, p[0], p[1], p[2]) for p in particles)) / (2*eps) V = (sum(gaussian_bump(X, Y+eps, p[0], p[1], p[2]) for p in particles) -      sum(gaussian_bump(X, Y-eps, p[0], p[1], p[2]) for p in particles)) / (2*eps)  ax3 = fig.add_subplot(133) ax3.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.5) ax3.quiver(X[::2,::2], Y[::2,::2], U[::2,::2], V[::2,::2], color='k', alpha=0.7, scale=30) for p in particles:     ax3.plot(p[0], p[1], 'o' if p[2]&gt;0 else 's', color=p[3], markersize=12, mec='k', mew=2) ax3.set_title(r'Gradient $\\nabla V$'); ax3.set_aspect('equal')  plt.tight_layout(); plt.show() print(\"This IS how GRL works! Q\u207a(z) = \u03a3\u1d62 w\u1d62 k(z, z\u1d62)\") print(\"Blue=positive (good), Red=negative (bad). Gradient \u2192 high-value regions.\") In\u00a0[\u00a0]: Copied! <pre># Example 5.1: Gradient Ascent Trajectories\ndef grad_ascent(start, grad_func, lr=0.15, steps=50):\n    traj = [start.copy()]\n    x = start.copy()\n    for _ in range(steps):\n        g = np.array(grad_func(x[0], x[1]))\n        if np.linalg.norm(g) &lt; 1e-4: break\n        x = x + lr * g\n        traj.append(x.copy())\n    return np.array(traj)\n\ndef mm_grad(x, y):\n    eps = 1e-5\n    return ((multimodal(x+eps,y)-multimodal(x-eps,y))/(2*eps),\n            (multimodal(x,y+eps)-multimodal(x,y-eps))/(2*eps))\n\nx = np.linspace(-3, 3, 50)\ny = np.linspace(-2.5, 3, 50)\nX, Y = np.meshgrid(x, y)\nZ = multimodal(X, Y)\n\nstarts = [np.array([-2.5,-2]), np.array([-2.5,2]), np.array([2.5,-1]),\n          np.array([0,-2]), np.array([-0.5,0.5])]\ntrajs = [grad_ascent(s, mm_grad) for s in starts]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\ncolors = plt.cm.tab10(np.linspace(0, 1, len(trajs)))\n\nax1 = axes[0]\nax1.contourf(X, Y, Z, levels=30, cmap='plasma', alpha=0.7)\nfor i, t in enumerate(trajs):\n    ax1.plot(t[:,0], t[:,1], '-', color=colors[i], lw=2)\n    ax1.plot(t[0,0], t[0,1], 'o', color=colors[i], ms=10, mec='w', mew=2)\n    ax1.plot(t[-1,0], t[-1,1], '*', color=colors[i], ms=15, mec='w')\nax1.plot(1, 1, 'w*', ms=20, mec='k', mew=2)\nax1.plot(-1, 0.5, 'w^', ms=15, mec='k', mew=2)\nax1.set_title('Gradient Ascent Trajectories'); ax1.set_aspect('equal')\n\nax2 = axes[1]\nfor i, t in enumerate(trajs):\n    vals = [multimodal(p[0], p[1]) for p in t]\n    ax2.plot(vals, '-o', color=colors[i], ms=4, label=f'Traj {i+1}')\nax2.set_xlabel('Step'); ax2.set_ylabel('V(x,y)')\nax2.set_title('Value Along Trajectories (monotonic!)'); ax2.legend(); ax2.grid(True, alpha=0.3)\n\nplt.tight_layout(); plt.show()\nprint(\"Different starts \u2192 different peaks. Value always increases along gradient ascent.\")\n</pre> # Example 5.1: Gradient Ascent Trajectories def grad_ascent(start, grad_func, lr=0.15, steps=50):     traj = [start.copy()]     x = start.copy()     for _ in range(steps):         g = np.array(grad_func(x[0], x[1]))         if np.linalg.norm(g) &lt; 1e-4: break         x = x + lr * g         traj.append(x.copy())     return np.array(traj)  def mm_grad(x, y):     eps = 1e-5     return ((multimodal(x+eps,y)-multimodal(x-eps,y))/(2*eps),             (multimodal(x,y+eps)-multimodal(x,y-eps))/(2*eps))  x = np.linspace(-3, 3, 50) y = np.linspace(-2.5, 3, 50) X, Y = np.meshgrid(x, y) Z = multimodal(X, Y)  starts = [np.array([-2.5,-2]), np.array([-2.5,2]), np.array([2.5,-1]),           np.array([0,-2]), np.array([-0.5,0.5])] trajs = [grad_ascent(s, mm_grad) for s in starts]  fig, axes = plt.subplots(1, 2, figsize=(14, 6)) colors = plt.cm.tab10(np.linspace(0, 1, len(trajs)))  ax1 = axes[0] ax1.contourf(X, Y, Z, levels=30, cmap='plasma', alpha=0.7) for i, t in enumerate(trajs):     ax1.plot(t[:,0], t[:,1], '-', color=colors[i], lw=2)     ax1.plot(t[0,0], t[0,1], 'o', color=colors[i], ms=10, mec='w', mew=2)     ax1.plot(t[-1,0], t[-1,1], '*', color=colors[i], ms=15, mec='w') ax1.plot(1, 1, 'w*', ms=20, mec='k', mew=2) ax1.plot(-1, 0.5, 'w^', ms=15, mec='k', mew=2) ax1.set_title('Gradient Ascent Trajectories'); ax1.set_aspect('equal')  ax2 = axes[1] for i, t in enumerate(trajs):     vals = [multimodal(p[0], p[1]) for p in t]     ax2.plot(vals, '-o', color=colors[i], ms=4, label=f'Traj {i+1}') ax2.set_xlabel('Step'); ax2.set_ylabel('V(x,y)') ax2.set_title('Value Along Trajectories (monotonic!)'); ax2.legend(); ax2.grid(True, alpha=0.3)  plt.tight_layout(); plt.show() print(\"Different starts \u2192 different peaks. Value always increases along gradient ascent.\")"},{"location":"notebooks/field_series/01_classical_vector_fields/#notebook-1-classical-vector-fields","title":"Notebook 1: Classical Vector Fields\u00b6","text":"<p>Part 1 of the GRL Field Series</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/#overview","title":"Overview\u00b6","text":"<p>This notebook introduces vector fields \u2014 a foundational concept for understanding GRL's reinforcement fields.</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/#learning-objectives","title":"Learning Objectives\u00b6","text":"<ol> <li>What is a vector field? \u2014 Arrows at each point in space</li> <li>Gradient fields \u2014 Following \"uphill\" directions for optimization</li> <li>Rotational fields \u2014 Circular flows and curl</li> <li>Superposition \u2014 Combining multiple fields</li> <li>Trajectories \u2014 Following a field to find extrema</li> </ol>"},{"location":"notebooks/field_series/01_classical_vector_fields/#why-this-matters-for-grl","title":"Why This Matters for GRL\u00b6","text":"<p>In GRL, the agent learns a reinforcement field $Q^+(z)$ over augmented state-action space. Understanding vector fields provides the intuition for how gradients guide policy improvement.</p> <p>Time: ~20-25 minutes</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/#part-1-what-is-a-vector-field","title":"Part 1: What is a Vector Field?\u00b6","text":"<p>A vector field assigns a vector (arrow) to every point in space:</p> <p>$$\\mathbf{F}: \\mathbb{R}^2 \\to \\mathbb{R}^2, \\quad \\mathbf{F}(x, y) = \\begin{bmatrix} F_x(x, y) \\\\ F_y(x, y) \\end{bmatrix}$$</p> <p>At each point $(x, y)$:</p> <ul> <li>Direction: Where the arrow points</li> <li>Magnitude: How long the arrow is</li> </ul>"},{"location":"notebooks/field_series/01_classical_vector_fields/#part-2-gradient-fields","title":"Part 2: Gradient Fields\u00b6","text":"<p>Given a scalar potential $V(x, y)$, its gradient is:</p> <p>$$\\nabla V = \\begin{bmatrix} \\partial V / \\partial x \\\\ \\partial V / \\partial y \\end{bmatrix}$$</p> <p>Key property: $\\nabla V$ points in the direction of steepest ascent.</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/#grl-connection","title":"GRL Connection\u00b6","text":"<p>In GRL, $Q^+(s, a)$ is like a potential. Its gradient $\\nabla_a Q^+$ tells us how to improve actions!</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/#part-3-rotational-fields","title":"Part 3: Rotational Fields\u00b6","text":"<p>Not all fields come from gradients. Rotational fields have circular flow:</p> <p>$$\\mathbf{F}(x, y) = (-y, x) \\quad \\text{(counterclockwise)}$$</p> <p>The curl measures rotation: $\\text{curl}(\\mathbf{F}) = \\partial F_y/\\partial x - \\partial F_x/\\partial y$</p> <p>GRL insight: GRL's field is a gradient field (curl=0), so it always points toward/away from extrema.</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/#part-4-superposition","title":"Part 4: Superposition\u00b6","text":"<p>Vector fields can be added:</p> <p>$$\\mathbf{F}_{\\text{total}} = \\mathbf{F}_1 + \\mathbf{F}_2$$</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/#grls-superposition","title":"GRL's Superposition\u00b6","text":"<p>The reinforcement field is a superposition of particle influences:</p> <p>$$Q^+(z) = \\sum_{i=1}^{N} w_i \\, k(z, z_i)$$</p> <p>Each particle contributes a \"bump\" (kernel), and the total field is their sum.</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/#part-5-trajectories","title":"Part 5: Trajectories\u00b6","text":"<p>Gradient ascent follows the field to find maxima:</p> <p>$$x_{t+1} = x_t + \\eta \\nabla V(x_t)$$</p> <p>In GRL, the agent \"climbs\" the $Q^+$ landscape to find optimal actions.</p>"},{"location":"notebooks/field_series/01_classical_vector_fields/#summary","title":"Summary\u00b6","text":"Concept Definition GRL Connection Vector Field Arrow at each point Gradient of $Q^+$ gives improvement direction Gradient $\\nabla V$ = steepest ascent $\\nabla_a Q^+$ improves actions Superposition $\\mathbf{F} = \\sum_i \\mathbf{F}_i$ $Q^+ = \\sum_i w_i k(z, z_i)$ Trajectories $x_{t+1} = x_t + \\eta \\nabla V$ Policy improvement via gradient"},{"location":"notebooks/field_series/01_classical_vector_fields/#key-equations","title":"Key Equations\u00b6","text":"<p>$$\\nabla V = \\begin{bmatrix} \\partial V/\\partial x \\\\ \\partial V/\\partial y \\end{bmatrix}, \\quad V_{\\text{total}} = \\sum_i w_i \\phi_i(x), \\quad Q^+(z) = \\sum_i w_i k(z, z_i)$$</p> <p>Next: Notebook 2 \u2014 Functional Fields (functions as vectors in RKHS)</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/","title":"01a: Vector Fields and ODEs","text":"In\u00a0[1]: Copied! <pre># Setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\nsns.set_theme(style='whitegrid', context='notebook')\nplt.rcParams['figure.figsize'] = (12, 8)\n%matplotlib inline\nprint(\"Libraries loaded.\")\n</pre> # Setup import numpy as np import matplotlib.pyplot as plt from matplotlib import cm import seaborn as sns  sns.set_theme(style='whitegrid', context='notebook') plt.rcParams['figure.figsize'] = (12, 8) %matplotlib inline print(\"Libraries loaded.\") <pre>Libraries loaded.\n</pre> In\u00a0[2]: Copied! <pre># Example 1.1: Simple Linear ODE\n# dx/dt = -x  (exponential decay)\n\ndef F_decay(x):\n    \"\"\"Vector field for dx/dt = -x\"\"\"\n    return -x\n\n# Analytical solution: x(t) = x_0 * exp(-t)\ndef analytical_decay(x0, t):\n    return x0 * np.exp(-t)\n\n# Visualize in 1D\nx = np.linspace(-3, 3, 20)\nt_vals = np.linspace(0, 3, 100)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Vector field (1D arrows)\nax1 = axes[0]\nax1.quiver(x, np.zeros_like(x), F_decay(x), np.zeros_like(x), \n           np.abs(F_decay(x)), cmap='coolwarm', scale=20)\nax1.axhline(0, color='k', lw=0.5)\nax1.axvline(0, color='k', lw=2, label='Fixed point (x=0)')\nax1.set_xlabel('$x$'); ax1.set_ylabel('')\nax1.set_title(r'Vector Field: $F(x) = -x$')\nax1.set_ylim(-0.5, 0.5); ax1.legend()\n\n# Right: Solutions (trajectories)\nax2 = axes[1]\nfor x0 in [-2, -1, 1, 2]:\n    ax2.plot(t_vals, analytical_decay(x0, t_vals), lw=2, label=f'$x_0 = {x0}$')\nax2.axhline(0, color='k', linestyle='--', alpha=0.5)\nax2.set_xlabel('$t$'); ax2.set_ylabel('$x(t)$')\nax2.set_title(r'Solutions: $x(t) = x_0 e^{-t}$')\nax2.legend(); ax2.grid(True, alpha=0.3)\n\nplt.tight_layout(); plt.show()\nprint(\"All trajectories converge to x=0 (the fixed point/attractor).\")\n</pre> # Example 1.1: Simple Linear ODE # dx/dt = -x  (exponential decay)  def F_decay(x):     \"\"\"Vector field for dx/dt = -x\"\"\"     return -x  # Analytical solution: x(t) = x_0 * exp(-t) def analytical_decay(x0, t):     return x0 * np.exp(-t)  # Visualize in 1D x = np.linspace(-3, 3, 20) t_vals = np.linspace(0, 3, 100)  fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # Left: Vector field (1D arrows) ax1 = axes[0] ax1.quiver(x, np.zeros_like(x), F_decay(x), np.zeros_like(x),             np.abs(F_decay(x)), cmap='coolwarm', scale=20) ax1.axhline(0, color='k', lw=0.5) ax1.axvline(0, color='k', lw=2, label='Fixed point (x=0)') ax1.set_xlabel('$x$'); ax1.set_ylabel('') ax1.set_title(r'Vector Field: $F(x) = -x$') ax1.set_ylim(-0.5, 0.5); ax1.legend()  # Right: Solutions (trajectories) ax2 = axes[1] for x0 in [-2, -1, 1, 2]:     ax2.plot(t_vals, analytical_decay(x0, t_vals), lw=2, label=f'$x_0 = {x0}$') ax2.axhline(0, color='k', linestyle='--', alpha=0.5) ax2.set_xlabel('$t$'); ax2.set_ylabel('$x(t)$') ax2.set_title(r'Solutions: $x(t) = x_0 e^{-t}$') ax2.legend(); ax2.grid(True, alpha=0.3)  plt.tight_layout(); plt.show() print(\"All trajectories converge to x=0 (the fixed point/attractor).\") <pre>All trajectories converge to x=0 (the fixed point/attractor).\n</pre> In\u00a0[3]: Copied! <pre># Example 1.2: 2D ODE System\n# dx/dt = -y\n# dy/dt = x\n# This is circular motion!\n\ndef F_rotation(state):\n    \"\"\"Vector field for circular motion\"\"\"\n    x, y = state\n    return np.array([-y, x])\n\n# Create grid\nx = np.linspace(-2, 2, 15)\ny = np.linspace(-2, 2, 15)\nX, Y = np.meshgrid(x, y)\nU, V = -Y, X\n\n# Analytical solution: x(t) = r*cos(t + \u03c6), y(t) = r*sin(t + \u03c6)\ndef solve_rotation(x0, y0, t_max=2*np.pi, n_steps=100):\n    t = np.linspace(0, t_max, n_steps)\n    r = np.sqrt(x0**2 + y0**2)\n    phi = np.arctan2(y0, x0)\n    return r * np.cos(t + phi), r * np.sin(t + phi)\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# Vector field\nax.quiver(X, Y, U, V, np.sqrt(U**2 + V**2), cmap='viridis', alpha=0.6, scale=25)\n\n# Trajectories\ncolors = plt.cm.tab10(np.linspace(0, 1, 4))\nfor i, (x0, y0) in enumerate([(1.5, 0), (0, 1), (0.5, 0.5), (1, 1)]):\n    traj_x, traj_y = solve_rotation(x0, y0)\n    ax.plot(traj_x, traj_y, '-', color=colors[i], lw=2)\n    ax.plot(x0, y0, 'o', color=colors[i], markersize=10, mec='k')\n\nax.plot(0, 0, 'k*', markersize=15, label='Fixed point')\nax.set_xlabel('$x$'); ax.set_ylabel('$y$')\nax.set_title(r'2D ODE: $\\dot{x} = -y, \\, \\dot{y} = x$ (Circular Motion)')\nax.set_aspect('equal'); ax.legend(); ax.grid(True, alpha=0.3)\nplt.show()\n\nprint(\"Trajectories are circles \u2014 the ODE describes rotation!\")\nprint(\"The fixed point at origin is a 'center' (neutral stability).\")\n</pre> # Example 1.2: 2D ODE System # dx/dt = -y # dy/dt = x # This is circular motion!  def F_rotation(state):     \"\"\"Vector field for circular motion\"\"\"     x, y = state     return np.array([-y, x])  # Create grid x = np.linspace(-2, 2, 15) y = np.linspace(-2, 2, 15) X, Y = np.meshgrid(x, y) U, V = -Y, X  # Analytical solution: x(t) = r*cos(t + \u03c6), y(t) = r*sin(t + \u03c6) def solve_rotation(x0, y0, t_max=2*np.pi, n_steps=100):     t = np.linspace(0, t_max, n_steps)     r = np.sqrt(x0**2 + y0**2)     phi = np.arctan2(y0, x0)     return r * np.cos(t + phi), r * np.sin(t + phi)  fig, ax = plt.subplots(figsize=(10, 10))  # Vector field ax.quiver(X, Y, U, V, np.sqrt(U**2 + V**2), cmap='viridis', alpha=0.6, scale=25)  # Trajectories colors = plt.cm.tab10(np.linspace(0, 1, 4)) for i, (x0, y0) in enumerate([(1.5, 0), (0, 1), (0.5, 0.5), (1, 1)]):     traj_x, traj_y = solve_rotation(x0, y0)     ax.plot(traj_x, traj_y, '-', color=colors[i], lw=2)     ax.plot(x0, y0, 'o', color=colors[i], markersize=10, mec='k')  ax.plot(0, 0, 'k*', markersize=15, label='Fixed point') ax.set_xlabel('$x$'); ax.set_ylabel('$y$') ax.set_title(r'2D ODE: $\\dot{x} = -y, \\, \\dot{y} = x$ (Circular Motion)') ax.set_aspect('equal'); ax.legend(); ax.grid(True, alpha=0.3) plt.show()  print(\"Trajectories are circles \u2014 the ODE describes rotation!\") print(\"The fixed point at origin is a 'center' (neutral stability).\") <pre>Trajectories are circles \u2014 the ODE describes rotation!\nThe fixed point at origin is a 'center' (neutral stability).\n</pre> In\u00a0[4]: Copied! <pre># Implement Euler and RK4 solvers\n\ndef euler_step(F, x, dt):\n    \"\"\"Single Euler step: x_new = x + dt * F(x)\"\"\"\n    return x + dt * F(x)\n\ndef rk4_step(F, x, dt):\n    \"\"\"Single RK4 step\"\"\"\n    k1 = F(x)\n    k2 = F(x + 0.5 * dt * k1)\n    k3 = F(x + 0.5 * dt * k2)\n    k4 = F(x + dt * k3)\n    return x + (dt / 6) * (k1 + 2*k2 + 2*k3 + k4)\n\ndef solve_ode(F, x0, t_span, dt, method='euler'):\n    \"\"\"Solve ODE from t_span[0] to t_span[1]\"\"\"\n    t = np.arange(t_span[0], t_span[1] + dt, dt)\n    x = np.zeros((len(t), len(x0)))\n    x[0] = x0\n    \n    step_fn = euler_step if method == 'euler' else rk4_step\n    \n    for i in range(1, len(t)):\n        x[i] = step_fn(F, x[i-1], dt)\n    \n    return t, x\n\nprint(\"Solvers defined: euler_step, rk4_step, solve_ode\")\n</pre> # Implement Euler and RK4 solvers  def euler_step(F, x, dt):     \"\"\"Single Euler step: x_new = x + dt * F(x)\"\"\"     return x + dt * F(x)  def rk4_step(F, x, dt):     \"\"\"Single RK4 step\"\"\"     k1 = F(x)     k2 = F(x + 0.5 * dt * k1)     k3 = F(x + 0.5 * dt * k2)     k4 = F(x + dt * k3)     return x + (dt / 6) * (k1 + 2*k2 + 2*k3 + k4)  def solve_ode(F, x0, t_span, dt, method='euler'):     \"\"\"Solve ODE from t_span[0] to t_span[1]\"\"\"     t = np.arange(t_span[0], t_span[1] + dt, dt)     x = np.zeros((len(t), len(x0)))     x[0] = x0          step_fn = euler_step if method == 'euler' else rk4_step          for i in range(1, len(t)):         x[i] = step_fn(F, x[i-1], dt)          return t, x  print(\"Solvers defined: euler_step, rk4_step, solve_ode\") <pre>Solvers defined: euler_step, rk4_step, solve_ode\n</pre> In\u00a0[5]: Copied! <pre># Example 2.1: Compare Euler vs RK4 on circular motion\n# True solution should be a perfect circle!\n\nx0 = np.array([1.0, 0.0])\nt_span = (0, 4 * np.pi)  # Two full rotations\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Different step sizes\nstep_sizes = [0.5, 0.2, 0.05]\n\nfor ax, dt in zip(axes, step_sizes):\n    # Euler\n    t_e, x_e = solve_ode(F_rotation, x0, t_span, dt, method='euler')\n    # RK4\n    t_r, x_r = solve_ode(F_rotation, x0, t_span, dt, method='rk4')\n    # True circle\n    theta = np.linspace(0, 2*np.pi, 100)\n    \n    ax.plot(np.cos(theta), np.sin(theta), 'k--', lw=1, label='True circle')\n    ax.plot(x_e[:, 0], x_e[:, 1], 'r-', lw=1.5, alpha=0.7, label='Euler')\n    ax.plot(x_r[:, 0], x_r[:, 1], 'b-', lw=1.5, alpha=0.7, label='RK4')\n    ax.plot(x0[0], x0[1], 'go', markersize=10, label='Start')\n    \n    ax.set_title(f'$\\Delta t = {dt}$\\n({int(t_span[1]/dt)} steps)')\n    ax.set_xlabel('$x$'); ax.set_ylabel('$y$')\n    ax.set_xlim(-2, 2); ax.set_ylim(-2, 2)\n    ax.set_aspect('equal'); ax.legend(loc='upper right', fontsize=8)\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Euler vs RK4: Solving Circular Motion ODE', y=1.02)\nplt.tight_layout(); plt.show()\n\nprint(\"Euler spirals outward (energy drift). RK4 stays on the circle!\")\nprint(\"This is why flow matching uses RK4 or adaptive solvers.\")\n</pre> # Example 2.1: Compare Euler vs RK4 on circular motion # True solution should be a perfect circle!  x0 = np.array([1.0, 0.0]) t_span = (0, 4 * np.pi)  # Two full rotations  fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # Different step sizes step_sizes = [0.5, 0.2, 0.05]  for ax, dt in zip(axes, step_sizes):     # Euler     t_e, x_e = solve_ode(F_rotation, x0, t_span, dt, method='euler')     # RK4     t_r, x_r = solve_ode(F_rotation, x0, t_span, dt, method='rk4')     # True circle     theta = np.linspace(0, 2*np.pi, 100)          ax.plot(np.cos(theta), np.sin(theta), 'k--', lw=1, label='True circle')     ax.plot(x_e[:, 0], x_e[:, 1], 'r-', lw=1.5, alpha=0.7, label='Euler')     ax.plot(x_r[:, 0], x_r[:, 1], 'b-', lw=1.5, alpha=0.7, label='RK4')     ax.plot(x0[0], x0[1], 'go', markersize=10, label='Start')          ax.set_title(f'$\\Delta t = {dt}$\\n({int(t_span[1]/dt)} steps)')     ax.set_xlabel('$x$'); ax.set_ylabel('$y$')     ax.set_xlim(-2, 2); ax.set_ylim(-2, 2)     ax.set_aspect('equal'); ax.legend(loc='upper right', fontsize=8)     ax.grid(True, alpha=0.3)  plt.suptitle('Euler vs RK4: Solving Circular Motion ODE', y=1.02) plt.tight_layout(); plt.show()  print(\"Euler spirals outward (energy drift). RK4 stays on the circle!\") print(\"This is why flow matching uses RK4 or adaptive solvers.\") <pre>&lt;&gt;:25: SyntaxWarning: invalid escape sequence '\\D'\n&lt;&gt;:25: SyntaxWarning: invalid escape sequence '\\D'\n/var/folders/jt/h4k6wdyx36nbnjk_rmkwdk280000gn/T/ipykernel_55695/524608160.py:25: SyntaxWarning: invalid escape sequence '\\D'\n  ax.set_title(f'$\\Delta t = {dt}$\\n({int(t_span[1]/dt)} steps)')\n</pre> <pre>Euler spirals outward (energy drift). RK4 stays on the circle!\nThis is why flow matching uses RK4 or adaptive solvers.\n</pre> In\u00a0[6]: Copied! <pre># Example 2.2: Error Analysis\n\ndef compute_error(method, dt, t_final=2*np.pi):\n    \"\"\"Compute final position error for circular motion\"\"\"\n    x0 = np.array([1.0, 0.0])\n    t, x = solve_ode(F_rotation, x0, (0, t_final), dt, method=method)\n    # True final position (one full rotation = back to start)\n    true_final = x0\n    return np.linalg.norm(x[-1] - true_final)\n\ndt_values = np.logspace(-2, 0, 20)  # 0.01 to 1.0\neuler_errors = [compute_error('euler', dt) for dt in dt_values]\nrk4_errors = [compute_error('rk4', dt) for dt in dt_values]\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.loglog(dt_values, euler_errors, 'ro-', lw=2, label='Euler (1st order)')\nax.loglog(dt_values, rk4_errors, 'bs-', lw=2, label='RK4 (4th order)')\n\n# Reference lines\nax.loglog(dt_values, 0.5 * dt_values, 'r--', alpha=0.5, label='$O(\\Delta t)$')\nax.loglog(dt_values, 0.1 * dt_values**4, 'b--', alpha=0.5, label='$O(\\Delta t^4)$')\n\nax.set_xlabel('Step size $\\Delta t$')\nax.set_ylabel('Error (distance from true solution)')\nax.set_title('Error vs Step Size: Euler vs RK4')\nax.legend(); ax.grid(True, alpha=0.3, which='both')\nplt.show()\n\nprint(\"Euler: Error ~ \u0394t (1st order). RK4: Error ~ \u0394t\u2074 (4th order).\")\nprint(\"RK4 with \u0394t=0.1 beats Euler with \u0394t=0.01!\")\n</pre> # Example 2.2: Error Analysis  def compute_error(method, dt, t_final=2*np.pi):     \"\"\"Compute final position error for circular motion\"\"\"     x0 = np.array([1.0, 0.0])     t, x = solve_ode(F_rotation, x0, (0, t_final), dt, method=method)     # True final position (one full rotation = back to start)     true_final = x0     return np.linalg.norm(x[-1] - true_final)  dt_values = np.logspace(-2, 0, 20)  # 0.01 to 1.0 euler_errors = [compute_error('euler', dt) for dt in dt_values] rk4_errors = [compute_error('rk4', dt) for dt in dt_values]  fig, ax = plt.subplots(figsize=(10, 6)) ax.loglog(dt_values, euler_errors, 'ro-', lw=2, label='Euler (1st order)') ax.loglog(dt_values, rk4_errors, 'bs-', lw=2, label='RK4 (4th order)')  # Reference lines ax.loglog(dt_values, 0.5 * dt_values, 'r--', alpha=0.5, label='$O(\\Delta t)$') ax.loglog(dt_values, 0.1 * dt_values**4, 'b--', alpha=0.5, label='$O(\\Delta t^4)$')  ax.set_xlabel('Step size $\\Delta t$') ax.set_ylabel('Error (distance from true solution)') ax.set_title('Error vs Step Size: Euler vs RK4') ax.legend(); ax.grid(True, alpha=0.3, which='both') plt.show()  print(\"Euler: Error ~ \u0394t (1st order). RK4: Error ~ \u0394t\u2074 (4th order).\") print(\"RK4 with \u0394t=0.1 beats Euler with \u0394t=0.01!\") <pre>&lt;&gt;:20: SyntaxWarning: invalid escape sequence '\\D'\n&lt;&gt;:21: SyntaxWarning: invalid escape sequence '\\D'\n&lt;&gt;:23: SyntaxWarning: invalid escape sequence '\\D'\n&lt;&gt;:20: SyntaxWarning: invalid escape sequence '\\D'\n&lt;&gt;:21: SyntaxWarning: invalid escape sequence '\\D'\n&lt;&gt;:23: SyntaxWarning: invalid escape sequence '\\D'\n/var/folders/jt/h4k6wdyx36nbnjk_rmkwdk280000gn/T/ipykernel_55695/2141023215.py:20: SyntaxWarning: invalid escape sequence '\\D'\n  ax.loglog(dt_values, 0.5 * dt_values, 'r--', alpha=0.5, label='$O(\\Delta t)$')\n/var/folders/jt/h4k6wdyx36nbnjk_rmkwdk280000gn/T/ipykernel_55695/2141023215.py:21: SyntaxWarning: invalid escape sequence '\\D'\n  ax.loglog(dt_values, 0.1 * dt_values**4, 'b--', alpha=0.5, label='$O(\\Delta t^4)$')\n/var/folders/jt/h4k6wdyx36nbnjk_rmkwdk280000gn/T/ipykernel_55695/2141023215.py:23: SyntaxWarning: invalid escape sequence '\\D'\n  ax.set_xlabel('Step size $\\Delta t$')\n</pre> <pre>Euler: Error ~ \u0394t (1st order). RK4: Error ~ \u0394t\u2074 (4th order).\nRK4 with \u0394t=0.1 beats Euler with \u0394t=0.01!\n</pre> In\u00a0[7]: Copied! <pre># Example 3.1: Different Types of Fixed Points\n\ndef stable_node(state):\n    \"\"\"Stable node: eigenvalues -1, -2\"\"\"\n    x, y = state\n    return np.array([-x, -2*y])\n\ndef unstable_node(state):\n    \"\"\"Unstable node: eigenvalues +1, +2\"\"\"\n    x, y = state\n    return np.array([x, 2*y])\n\ndef saddle(state):\n    \"\"\"Saddle point: eigenvalues +1, -1\"\"\"\n    x, y = state\n    return np.array([x, -y])\n\ndef stable_spiral(state):\n    \"\"\"Stable spiral: eigenvalues -0.2 \u00b1 i\"\"\"\n    x, y = state\n    return np.array([-0.2*x - y, x - 0.2*y])\n\nsystems = [\n    (stable_node, 'Stable Node', 'Attractor'),\n    (unstable_node, 'Unstable Node', 'Repeller'),\n    (saddle, 'Saddle Point', 'Mixed'),\n    (stable_spiral, 'Stable Spiral', 'Attractor'),\n]\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\n\nx = np.linspace(-2, 2, 15)\ny = np.linspace(-2, 2, 15)\nX, Y = np.meshgrid(x, y)\n\nfor ax, (F, title, ftype) in zip(axes.flat, systems):\n    # Vector field\n    U = np.zeros_like(X)\n    V = np.zeros_like(Y)\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            vec = F(np.array([X[i,j], Y[i,j]]))\n            U[i,j], V[i,j] = vec\n    \n    ax.streamplot(X, Y, U, V, color='gray', density=1.5, linewidth=0.8)\n    \n    # Sample trajectories\n    starts = [np.array([1.5, 1.5]), np.array([-1.5, 1.0]), \n              np.array([1.0, -1.5]), np.array([-1.0, -1.0])]\n    colors = plt.cm.tab10(np.linspace(0, 0.4, len(starts)))\n    \n    for x0, c in zip(starts, colors):\n        t, traj = solve_ode(F, x0, (0, 5), 0.05, method='rk4')\n        ax.plot(traj[:, 0], traj[:, 1], '-', color=c, lw=2)\n        ax.plot(x0[0], x0[1], 'o', color=c, markersize=8)\n    \n    ax.plot(0, 0, 'k*', markersize=15)\n    ax.set_title(f'{title}\\n({ftype})')\n    ax.set_xlim(-2, 2); ax.set_ylim(-2, 2)\n    ax.set_aspect('equal'); ax.grid(True, alpha=0.3)\n\nplt.tight_layout(); plt.show()\nprint(\"Fixed point type determines long-term behavior of ALL trajectories.\")\n</pre> # Example 3.1: Different Types of Fixed Points  def stable_node(state):     \"\"\"Stable node: eigenvalues -1, -2\"\"\"     x, y = state     return np.array([-x, -2*y])  def unstable_node(state):     \"\"\"Unstable node: eigenvalues +1, +2\"\"\"     x, y = state     return np.array([x, 2*y])  def saddle(state):     \"\"\"Saddle point: eigenvalues +1, -1\"\"\"     x, y = state     return np.array([x, -y])  def stable_spiral(state):     \"\"\"Stable spiral: eigenvalues -0.2 \u00b1 i\"\"\"     x, y = state     return np.array([-0.2*x - y, x - 0.2*y])  systems = [     (stable_node, 'Stable Node', 'Attractor'),     (unstable_node, 'Unstable Node', 'Repeller'),     (saddle, 'Saddle Point', 'Mixed'),     (stable_spiral, 'Stable Spiral', 'Attractor'), ]  fig, axes = plt.subplots(2, 2, figsize=(12, 12))  x = np.linspace(-2, 2, 15) y = np.linspace(-2, 2, 15) X, Y = np.meshgrid(x, y)  for ax, (F, title, ftype) in zip(axes.flat, systems):     # Vector field     U = np.zeros_like(X)     V = np.zeros_like(Y)     for i in range(X.shape[0]):         for j in range(X.shape[1]):             vec = F(np.array([X[i,j], Y[i,j]]))             U[i,j], V[i,j] = vec          ax.streamplot(X, Y, U, V, color='gray', density=1.5, linewidth=0.8)          # Sample trajectories     starts = [np.array([1.5, 1.5]), np.array([-1.5, 1.0]),                np.array([1.0, -1.5]), np.array([-1.0, -1.0])]     colors = plt.cm.tab10(np.linspace(0, 0.4, len(starts)))          for x0, c in zip(starts, colors):         t, traj = solve_ode(F, x0, (0, 5), 0.05, method='rk4')         ax.plot(traj[:, 0], traj[:, 1], '-', color=c, lw=2)         ax.plot(x0[0], x0[1], 'o', color=c, markersize=8)          ax.plot(0, 0, 'k*', markersize=15)     ax.set_title(f'{title}\\n({ftype})')     ax.set_xlim(-2, 2); ax.set_ylim(-2, 2)     ax.set_aspect('equal'); ax.grid(True, alpha=0.3)  plt.tight_layout(); plt.show() print(\"Fixed point type determines long-term behavior of ALL trajectories.\") <pre>Fixed point type determines long-term behavior of ALL trajectories.\n</pre> In\u00a0[8]: Copied! <pre># Example 4.1: Gradient Descent as ODE\n\ndef rosenbrock(state):\n    \"\"\"Rosenbrock function: f(x,y) = (1-x)\u00b2 + 100(y-x\u00b2)\u00b2\"\"\"\n    x, y = state\n    return (1 - x)**2 + 100 * (y - x**2)**2\n\ndef rosenbrock_grad(state):\n    \"\"\"Gradient of Rosenbrock\"\"\"\n    x, y = state\n    dfdx = -2*(1-x) - 400*x*(y - x**2)\n    dfdy = 200*(y - x**2)\n    return np.array([dfdx, dfdy])\n\ndef neg_grad_rosenbrock(state):\n    \"\"\"Negative gradient (for descent)\"\"\"\n    return -rosenbrock_grad(state)\n\n# Create contour plot\nx = np.linspace(-2, 2, 100)\ny = np.linspace(-1, 3, 100)\nX, Y = np.meshgrid(x, y)\nZ = np.zeros_like(X)\nfor i in range(X.shape[0]):\n    for j in range(X.shape[1]):\n        Z[i,j] = rosenbrock(np.array([X[i,j], Y[i,j]]))\n\nfig, ax = plt.subplots(figsize=(12, 8))\n\n# Contours (log scale for better visualization)\nlevels = np.logspace(-1, 3, 20)\nc = ax.contour(X, Y, Z, levels=levels, cmap='viridis', alpha=0.7)\nax.contourf(X, Y, Z, levels=levels, cmap='viridis', alpha=0.3)\n\n# Gradient descent trajectories\nstarts = [np.array([-1.5, 2.5]), np.array([1.5, 2.5]), \n          np.array([-0.5, -0.5]), np.array([0.0, 2.0])]\ncolors = ['red', 'blue', 'orange', 'purple']\n\nfor x0, c in zip(starts, colors):\n    # Use small step size for stability\n    t, traj = solve_ode(neg_grad_rosenbrock, x0, (0, 50), 0.001, method='rk4')\n    ax.plot(traj[:, 0], traj[:, 1], '-', color=c, lw=2, alpha=0.8)\n    ax.plot(x0[0], x0[1], 'o', color=c, markersize=10, mec='k')\n\nax.plot(1, 1, 'w*', markersize=20, mec='k', mew=2, label='Global minimum (1,1)')\nax.set_xlabel('$x$'); ax.set_ylabel('$y$')\nax.set_title('Gradient Descent on Rosenbrock Function\\n(Optimization as ODE)')\nax.legend(); ax.grid(True, alpha=0.3)\nplt.colorbar(ax.contourf(X, Y, Z, levels=levels, cmap='viridis', alpha=0), \n             ax=ax, label='$f(x,y)$')\nplt.show()\n\nprint(\"Gradient descent = following the negative gradient field.\")\nprint(\"All trajectories flow toward the minimum at (1, 1).\")\n</pre> # Example 4.1: Gradient Descent as ODE  def rosenbrock(state):     \"\"\"Rosenbrock function: f(x,y) = (1-x)\u00b2 + 100(y-x\u00b2)\u00b2\"\"\"     x, y = state     return (1 - x)**2 + 100 * (y - x**2)**2  def rosenbrock_grad(state):     \"\"\"Gradient of Rosenbrock\"\"\"     x, y = state     dfdx = -2*(1-x) - 400*x*(y - x**2)     dfdy = 200*(y - x**2)     return np.array([dfdx, dfdy])  def neg_grad_rosenbrock(state):     \"\"\"Negative gradient (for descent)\"\"\"     return -rosenbrock_grad(state)  # Create contour plot x = np.linspace(-2, 2, 100) y = np.linspace(-1, 3, 100) X, Y = np.meshgrid(x, y) Z = np.zeros_like(X) for i in range(X.shape[0]):     for j in range(X.shape[1]):         Z[i,j] = rosenbrock(np.array([X[i,j], Y[i,j]]))  fig, ax = plt.subplots(figsize=(12, 8))  # Contours (log scale for better visualization) levels = np.logspace(-1, 3, 20) c = ax.contour(X, Y, Z, levels=levels, cmap='viridis', alpha=0.7) ax.contourf(X, Y, Z, levels=levels, cmap='viridis', alpha=0.3)  # Gradient descent trajectories starts = [np.array([-1.5, 2.5]), np.array([1.5, 2.5]),            np.array([-0.5, -0.5]), np.array([0.0, 2.0])] colors = ['red', 'blue', 'orange', 'purple']  for x0, c in zip(starts, colors):     # Use small step size for stability     t, traj = solve_ode(neg_grad_rosenbrock, x0, (0, 50), 0.001, method='rk4')     ax.plot(traj[:, 0], traj[:, 1], '-', color=c, lw=2, alpha=0.8)     ax.plot(x0[0], x0[1], 'o', color=c, markersize=10, mec='k')  ax.plot(1, 1, 'w*', markersize=20, mec='k', mew=2, label='Global minimum (1,1)') ax.set_xlabel('$x$'); ax.set_ylabel('$y$') ax.set_title('Gradient Descent on Rosenbrock Function\\n(Optimization as ODE)') ax.legend(); ax.grid(True, alpha=0.3) plt.colorbar(ax.contourf(X, Y, Z, levels=levels, cmap='viridis', alpha=0),               ax=ax, label='$f(x,y)$') plt.show()  print(\"Gradient descent = following the negative gradient field.\") print(\"All trajectories flow toward the minimum at (1, 1).\") <pre>Gradient descent = following the negative gradient field.\nAll trajectories flow toward the minimum at (1, 1).\n</pre> In\u00a0[9]: Copied! <pre># Example 5.1: Simulating Flow Matching (Toy Example)\n# Transport a Gaussian to a mixture of Gaussians\n\nnp.random.seed(42)\n\n# \"Data\" distribution: mixture of 4 Gaussians\ndef sample_data(n):\n    centers = np.array([[2, 2], [-2, 2], [-2, -2], [2, -2]])\n    idx = np.random.randint(0, 4, n)\n    return centers[idx] + 0.3 * np.random.randn(n, 2)\n\n# \"Noise\" distribution: standard Gaussian\ndef sample_noise(n):\n    return np.random.randn(n, 2)\n\n# Linear interpolation path (rectified flow)\ndef interpolate(x0, x1, t):\n    \"\"\"x_t = (1-t)*x0 + t*x1\"\"\"\n    return (1 - t) * x0 + t * x1\n\n# True velocity for linear interpolation\ndef true_velocity(x0, x1):\n    \"\"\"v = x1 - x0 (constant velocity)\"\"\"\n    return x1 - x0\n\n# Visualize the transport\nn_samples = 200\nx0 = sample_data(n_samples)  # Data\nx1 = sample_noise(n_samples)  # Noise\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\ntimes = [0, 0.25, 0.5, 0.75, 1.0]\n\nfor ax, t in zip(axes, times):\n    x_t = interpolate(x0, x1, t)\n    ax.scatter(x_t[:, 0], x_t[:, 1], alpha=0.5, s=20)\n    ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)\n    ax.set_title(f'$t = {t}$')\n    ax.set_aspect('equal'); ax.grid(True, alpha=0.3)\n\naxes[0].set_ylabel('$y$')\nplt.suptitle('Flow Matching: Transporting Data \u2192 Noise', y=1.02)\nplt.tight_layout(); plt.show()\n\nprint(\"t=0: Data (4 clusters). t=1: Noise (single Gaussian).\")\nprint(\"Flow matching learns the velocity field that creates this transport.\")\n</pre> # Example 5.1: Simulating Flow Matching (Toy Example) # Transport a Gaussian to a mixture of Gaussians  np.random.seed(42)  # \"Data\" distribution: mixture of 4 Gaussians def sample_data(n):     centers = np.array([[2, 2], [-2, 2], [-2, -2], [2, -2]])     idx = np.random.randint(0, 4, n)     return centers[idx] + 0.3 * np.random.randn(n, 2)  # \"Noise\" distribution: standard Gaussian def sample_noise(n):     return np.random.randn(n, 2)  # Linear interpolation path (rectified flow) def interpolate(x0, x1, t):     \"\"\"x_t = (1-t)*x0 + t*x1\"\"\"     return (1 - t) * x0 + t * x1  # True velocity for linear interpolation def true_velocity(x0, x1):     \"\"\"v = x1 - x0 (constant velocity)\"\"\"     return x1 - x0  # Visualize the transport n_samples = 200 x0 = sample_data(n_samples)  # Data x1 = sample_noise(n_samples)  # Noise  fig, axes = plt.subplots(1, 5, figsize=(20, 4)) times = [0, 0.25, 0.5, 0.75, 1.0]  for ax, t in zip(axes, times):     x_t = interpolate(x0, x1, t)     ax.scatter(x_t[:, 0], x_t[:, 1], alpha=0.5, s=20)     ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)     ax.set_title(f'$t = {t}$')     ax.set_aspect('equal'); ax.grid(True, alpha=0.3)  axes[0].set_ylabel('$y$') plt.suptitle('Flow Matching: Transporting Data \u2192 Noise', y=1.02) plt.tight_layout(); plt.show()  print(\"t=0: Data (4 clusters). t=1: Noise (single Gaussian).\") print(\"Flow matching learns the velocity field that creates this transport.\") <pre>t=0: Data (4 clusters). t=1: Noise (single Gaussian).\nFlow matching learns the velocity field that creates this transport.\n</pre> In\u00a0[10]: Copied! <pre># Example 5.2: Visualize Velocity Field at Different Times\n\n# For visualization, we'll show the average velocity at each point\n# (In practice, this is what the neural network learns)\n\ndef estimate_velocity_field(x0_samples, x1_samples, t, grid_x, grid_y):\n    \"\"\"Estimate velocity field by averaging over nearby samples\"\"\"\n    X, Y = np.meshgrid(grid_x, grid_y)\n    U = np.zeros_like(X)\n    V = np.zeros_like(Y)\n    \n    # Interpolated positions at time t\n    x_t = interpolate(x0_samples, x1_samples, t)\n    velocities = true_velocity(x0_samples, x1_samples)\n    \n    # For each grid point, average velocities of nearby samples\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            point = np.array([X[i,j], Y[i,j]])\n            dists = np.linalg.norm(x_t - point, axis=1)\n            weights = np.exp(-dists**2 / 0.5)  # Gaussian weighting\n            if weights.sum() &gt; 1e-6:\n                U[i,j] = np.average(velocities[:, 0], weights=weights)\n                V[i,j] = np.average(velocities[:, 1], weights=weights)\n    \n    return X, Y, U, V\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\ngrid = np.linspace(-4, 4, 15)\n\nfor ax, t in zip(axes, [0.1, 0.5, 0.9]):\n    X, Y, U, V = estimate_velocity_field(x0, x1, t, grid, grid)\n    x_t = interpolate(x0, x1, t)\n    \n    ax.scatter(x_t[:, 0], x_t[:, 1], alpha=0.3, s=10, c='blue')\n    ax.quiver(X, Y, U, V, np.sqrt(U**2+V**2), cmap='Reds', scale=50, alpha=0.8)\n    ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)\n    ax.set_title(f'Velocity Field at $t = {t}$')\n    ax.set_aspect('equal'); ax.grid(True, alpha=0.3)\n\nplt.suptitle('Flow Matching: Learned Velocity Field', y=1.02)\nplt.tight_layout(); plt.show()\n\nprint(\"The velocity field points from data toward noise.\")\nprint(\"To SAMPLE: start from noise, follow NEGATIVE velocity (reverse ODE).\")\n</pre> # Example 5.2: Visualize Velocity Field at Different Times  # For visualization, we'll show the average velocity at each point # (In practice, this is what the neural network learns)  def estimate_velocity_field(x0_samples, x1_samples, t, grid_x, grid_y):     \"\"\"Estimate velocity field by averaging over nearby samples\"\"\"     X, Y = np.meshgrid(grid_x, grid_y)     U = np.zeros_like(X)     V = np.zeros_like(Y)          # Interpolated positions at time t     x_t = interpolate(x0_samples, x1_samples, t)     velocities = true_velocity(x0_samples, x1_samples)          # For each grid point, average velocities of nearby samples     for i in range(X.shape[0]):         for j in range(X.shape[1]):             point = np.array([X[i,j], Y[i,j]])             dists = np.linalg.norm(x_t - point, axis=1)             weights = np.exp(-dists**2 / 0.5)  # Gaussian weighting             if weights.sum() &gt; 1e-6:                 U[i,j] = np.average(velocities[:, 0], weights=weights)                 V[i,j] = np.average(velocities[:, 1], weights=weights)          return X, Y, U, V  fig, axes = plt.subplots(1, 3, figsize=(15, 5)) grid = np.linspace(-4, 4, 15)  for ax, t in zip(axes, [0.1, 0.5, 0.9]):     X, Y, U, V = estimate_velocity_field(x0, x1, t, grid, grid)     x_t = interpolate(x0, x1, t)          ax.scatter(x_t[:, 0], x_t[:, 1], alpha=0.3, s=10, c='blue')     ax.quiver(X, Y, U, V, np.sqrt(U**2+V**2), cmap='Reds', scale=50, alpha=0.8)     ax.set_xlim(-4, 4); ax.set_ylim(-4, 4)     ax.set_title(f'Velocity Field at $t = {t}$')     ax.set_aspect('equal'); ax.grid(True, alpha=0.3)  plt.suptitle('Flow Matching: Learned Velocity Field', y=1.02) plt.tight_layout(); plt.show()  print(\"The velocity field points from data toward noise.\") print(\"To SAMPLE: start from noise, follow NEGATIVE velocity (reverse ODE).\") <pre>The velocity field points from data toward noise.\nTo SAMPLE: start from noise, follow NEGATIVE velocity (reverse ODE).\n</pre>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#notebook-1a-vector-fields-and-odes","title":"Notebook 1a: Vector Fields and ODEs\u00b6","text":"<p>Spin-off from Notebook 1 \u2014 Classical Vector Fields</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#overview","title":"Overview\u00b6","text":"<p>Vector fields aren't just pretty pictures \u2014 they define ordinary differential equations (ODEs). Following a vector field means solving an ODE!</p> <p>This notebook bridges:</p> <ul> <li>GRL: Gradient ascent on $Q^+$ is an ODE</li> <li>Flow Matching: Sampling = solving the flow ODE</li> <li>Diffusion Models: Probability flow ODEs</li> </ul> <p>\ud83d\udcda Cross-reference: For a deep dive into flow matching and diffusion models, see the genai-lab project:</p> <ul> <li><code>docs/flow_matching/01_flow_matching_foundations.md</code> \u2014 Flow matching theory</li> <li><code>docs/DDPM/01_ddpm_foundations.md</code> \u2014 Diffusion model foundations</li> </ul> <p>Both projects share the same mathematical foundation: vector fields define dynamics.</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#learning-objectives","title":"Learning Objectives\u00b6","text":"<ol> <li>ODEs as vector fields \u2014 $\\dot{x} = F(x)$ means \"follow the arrows\"</li> <li>Numerical solvers \u2014 Euler, RK4, and why accuracy matters</li> <li>Phase portraits \u2014 Visualizing solution families</li> <li>Fixed points \u2014 Attractors, repellers, and stability</li> <li>Applications \u2014 Gradient flow, flow matching, GRL</li> </ol>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Notebook 1 (Classical Vector Fields)</li> <li>Basic calculus (derivatives)</li> </ul>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#time","title":"Time\u00b6","text":"<p>~25-30 minutes</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#part-1-odes-as-vector-fields","title":"Part 1: ODEs as Vector Fields\u00b6","text":""},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#the-key-insight","title":"The Key Insight\u00b6","text":"<p>An ordinary differential equation (ODE):</p> <p>$$\\frac{dx}{dt} = F(x)$$</p> <p>says: \"The velocity at position $x$ is given by $F(x)$\"</p> <p>This IS a vector field! The field $F$ tells you which direction to move at each point.</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#solving-an-ode-following-the-field","title":"Solving an ODE = Following the Field\u00b6","text":"<p>Given initial condition $x(0) = x_0$, the solution $x(t)$ is the trajectory that:</p> <ol> <li>Starts at $x_0$</li> <li>Always moves in the direction given by $F(x)$</li> </ol>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#why-this-matters","title":"Why This Matters\u00b6","text":"Application ODE Vector Field GRL $\\theta_{t+1} = \\theta_t + \\eta \\nabla_\\theta Q^+$ Gradient of $Q^+$ Flow Matching $\\frac{dx}{dt} = v_\\theta(x, t)$ Learned velocity field Gradient Descent $\\frac{dx}{dt} = -\\nabla f(x)$ Negative gradient"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#part-2-numerical-ode-solvers","title":"Part 2: Numerical ODE Solvers\u00b6","text":"<p>Most ODEs can't be solved analytically. We need numerical methods.</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#euler-method-simplest","title":"Euler Method (Simplest)\u00b6","text":"<p>$$x_{n+1} = x_n + \\Delta t \\cdot F(x_n)$$</p> <p>\"Take a step in the direction of the field.\"</p> <p>Problem: Error accumulates! Step size $\\Delta t$ must be small.</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#runge-kutta-4-rk4-the-workhorse","title":"Runge-Kutta 4 (RK4) \u2014 The Workhorse\u00b6","text":"<p>$$\\begin{align} k_1 &amp;= F(x_n) \\\\ k_2 &amp;= F(x_n + \\frac{\\Delta t}{2} k_1) \\\\ k_3 &amp;= F(x_n + \\frac{\\Delta t}{2} k_2) \\\\ k_4 &amp;= F(x_n + \\Delta t \\cdot k_3) \\\\ x_{n+1} &amp;= x_n + \\frac{\\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4) \\end{align}$$</p> <p>\"Sample the field at multiple points, then average.\"</p> <p>Much more accurate for the same step size!</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#part-3-phase-portraits-and-fixed-points","title":"Part 3: Phase Portraits and Fixed Points\u00b6","text":""},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#phase-portrait","title":"Phase Portrait\u00b6","text":"<p>A phase portrait shows the vector field plus representative trajectories. It reveals the qualitative behavior of all solutions.</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#fixed-points","title":"Fixed Points\u00b6","text":"<p>A fixed point (equilibrium) is where $F(x^*) = 0$. The system stays there forever.</p> <p>Types of fixed points in 2D:</p> Type Behavior Eigenvalues Stable node Trajectories converge Both negative real Unstable node Trajectories diverge Both positive real Saddle Some converge, some diverge Opposite signs Center Closed orbits Pure imaginary Spiral Spiraling in/out Complex with real part"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#part-4-gradient-flow-optimization-as-ode","title":"Part 4: Gradient Flow \u2014 Optimization as ODE\u00b6","text":""},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#the-connection","title":"The Connection\u00b6","text":"<p>Gradient descent is just following the ODE:</p> <p>$$\\frac{dx}{dt} = -\\nabla f(x)$$</p> <p>The vector field is the negative gradient of the loss function!</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#properties","title":"Properties\u00b6","text":"<ul> <li>Fixed points = critical points of $f$ (minima, maxima, saddles)</li> <li>Stable fixed points = local minima</li> <li>Trajectories always decrease $f$: $\\frac{df}{dt} = \\nabla f \\cdot \\dot{x} = -\\|\\nabla f\\|^2 \\leq 0$</li> </ul>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#grl-connection","title":"GRL Connection\u00b6","text":"<p>In GRL, we do gradient ascent on $Q^+$:</p> <p>$$\\frac{d\\theta}{dt} = +\\nabla_\\theta Q^+(s, \\theta)$$</p> <p>This is the same math \u2014 just maximizing instead of minimizing!</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#part-5-flow-matching-transporting-distributions","title":"Part 5: Flow Matching \u2014 Transporting Distributions\u00b6","text":""},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#the-big-picture","title":"The Big Picture\u00b6","text":"<p>In flow matching (generative modeling), we learn a velocity field that transports noise to data:</p> <p>$$\\frac{dx}{dt} = v_\\theta(x, t), \\quad t \\in [0, 1]$$</p> <ul> <li>$t=0$: Data distribution</li> <li>$t=1$: Noise distribution</li> <li>Sampling: Start from noise, solve ODE backward to get data</li> </ul>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#why-vector-fields","title":"Why Vector Fields?\u00b6","text":"<p>The velocity field $v_\\theta(x, t)$ IS a (time-dependent) vector field!</p> <ul> <li>Training: Learn $v_\\theta$ to match the true transport</li> <li>Sampling: Solve the ODE using Euler/RK4</li> <li>Quality: Better ODE solver = better samples</li> </ul>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#summary","title":"Summary\u00b6","text":""},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#key-concepts","title":"Key Concepts\u00b6","text":"Concept Meaning Application ODE $\\dot{x} = F(x)$ Follow the vector field Euler method $x_{n+1} = x_n + \\Delta t \\cdot F(x_n)$ Simple, 1st order RK4 Weighted average of 4 samples Accurate, 4th order Fixed point $F(x^*) = 0$ Equilibrium Gradient flow $\\dot{x} = -\\nabla f$ Optimization Flow matching $\\dot{x} = v_\\theta(x, t)$ Generative modeling"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#key-equations","title":"Key Equations\u00b6","text":"<p>Euler step: $$x_{n+1} = x_n + \\Delta t \\cdot F(x_n)$$</p> <p>RK4 step: $$x_{n+1} = x_n + \\frac{\\Delta t}{6}(k_1 + 2k_2 + 2k_3 + k_4)$$</p> <p>Gradient descent: $$\\frac{dx}{dt} = -\\nabla f(x)$$</p> <p>Flow matching: $$\\frac{dx}{dt} = v_\\theta(x, t)$$</p>"},{"location":"notebooks/field_series/01a_vector_fields_and_odes/#connections","title":"Connections\u00b6","text":"<ul> <li>GRL: Policy improvement via gradient ascent on $Q^+$</li> <li>Flow Matching: Sample generation via ODE integration</li> <li>Diffusion Models: Probability flow ODE</li> </ul> <p>Next: Notebook 2 \u2014 Functional Fields (functions as vectors in RKHS)</p>"},{"location":"notebooks/field_series/02_functional_fields/","title":"02: Functional Fields","text":"In\u00a0[1]: Copied! <pre># Setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\nINTERACTIVE = False\ntry:\n    import ipywidgets as widgets\n    from IPython.display import display\n    WIDGETS_AVAILABLE = True\nexcept ImportError:\n    WIDGETS_AVAILABLE = False\n\nsns.set_theme(style='whitegrid', context='notebook')\nplt.rcParams['figure.figsize'] = (12, 8)\n%matplotlib inline\nprint(f\"Libraries loaded. Interactive: {INTERACTIVE and WIDGETS_AVAILABLE}\")\n</pre> # Setup import numpy as np import matplotlib.pyplot as plt from matplotlib import cm from mpl_toolkits.mplot3d import Axes3D import seaborn as sns  INTERACTIVE = False try:     import ipywidgets as widgets     from IPython.display import display     WIDGETS_AVAILABLE = True except ImportError:     WIDGETS_AVAILABLE = False  sns.set_theme(style='whitegrid', context='notebook') plt.rcParams['figure.figsize'] = (12, 8) %matplotlib inline print(f\"Libraries loaded. Interactive: {INTERACTIVE and WIDGETS_AVAILABLE}\") <pre>Libraries loaded. Interactive: False\n</pre> In\u00a0[2]: Copied! <pre># Example 1.1: Functions as Vectors \u2014 Addition and Scaling\nx = np.linspace(-3, 3, 200)\n\n# Define two \"basis\" functions\nf1 = np.exp(-x**2)           # Gaussian centered at 0\nf2 = np.exp(-(x-1.5)**2)     # Gaussian centered at 1.5\n\n# Linear combinations (just like vectors!)\nf_sum = f1 + f2              # Addition\nf_scaled = 2 * f1            # Scaling\nf_combo = 0.5 * f1 + 1.5 * f2  # General linear combination\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Addition\naxes[0].plot(x, f1, 'b-', lw=2, label=r'$f_1(x) = e^{-x^2}$')\naxes[0].plot(x, f2, 'r-', lw=2, label=r'$f_2(x) = e^{-(x-1.5)^2}$')\naxes[0].plot(x, f_sum, 'g--', lw=2, label=r'$f_1 + f_2$')\naxes[0].set_title('Function Addition'); axes[0].legend(); axes[0].grid(True, alpha=0.3)\n\n# Scaling\naxes[1].plot(x, f1, 'b-', lw=2, label=r'$f_1$')\naxes[1].plot(x, f_scaled, 'b--', lw=2, label=r'$2 \\cdot f_1$')\naxes[1].plot(x, 0.5*f1, 'b:', lw=2, label=r'$0.5 \\cdot f_1$')\naxes[1].set_title('Function Scaling'); axes[1].legend(); axes[1].grid(True, alpha=0.3)\n\n# Linear combination\naxes[2].plot(x, f1, 'b-', lw=1, alpha=0.5, label=r'$f_1$')\naxes[2].plot(x, f2, 'r-', lw=1, alpha=0.5, label=r'$f_2$')\naxes[2].plot(x, f_combo, 'purple', lw=2, label=r'$0.5 f_1 + 1.5 f_2$')\naxes[2].set_title('Linear Combination'); axes[2].legend(); axes[2].grid(True, alpha=0.3)\n\nplt.tight_layout(); plt.show()\nprint(\"Functions behave like vectors: we can add them and scale them!\")\n</pre> # Example 1.1: Functions as Vectors \u2014 Addition and Scaling x = np.linspace(-3, 3, 200)  # Define two \"basis\" functions f1 = np.exp(-x**2)           # Gaussian centered at 0 f2 = np.exp(-(x-1.5)**2)     # Gaussian centered at 1.5  # Linear combinations (just like vectors!) f_sum = f1 + f2              # Addition f_scaled = 2 * f1            # Scaling f_combo = 0.5 * f1 + 1.5 * f2  # General linear combination  fig, axes = plt.subplots(1, 3, figsize=(15, 4))  # Addition axes[0].plot(x, f1, 'b-', lw=2, label=r'$f_1(x) = e^{-x^2}$') axes[0].plot(x, f2, 'r-', lw=2, label=r'$f_2(x) = e^{-(x-1.5)^2}$') axes[0].plot(x, f_sum, 'g--', lw=2, label=r'$f_1 + f_2$') axes[0].set_title('Function Addition'); axes[0].legend(); axes[0].grid(True, alpha=0.3)  # Scaling axes[1].plot(x, f1, 'b-', lw=2, label=r'$f_1$') axes[1].plot(x, f_scaled, 'b--', lw=2, label=r'$2 \\cdot f_1$') axes[1].plot(x, 0.5*f1, 'b:', lw=2, label=r'$0.5 \\cdot f_1$') axes[1].set_title('Function Scaling'); axes[1].legend(); axes[1].grid(True, alpha=0.3)  # Linear combination axes[2].plot(x, f1, 'b-', lw=1, alpha=0.5, label=r'$f_1$') axes[2].plot(x, f2, 'r-', lw=1, alpha=0.5, label=r'$f_2$') axes[2].plot(x, f_combo, 'purple', lw=2, label=r'$0.5 f_1 + 1.5 f_2$') axes[2].set_title('Linear Combination'); axes[2].legend(); axes[2].grid(True, alpha=0.3)  plt.tight_layout(); plt.show() print(\"Functions behave like vectors: we can add them and scale them!\") <pre>Functions behave like vectors: we can add them and scale them!\n</pre> In\u00a0[3]: Copied! <pre># Example 1.2: Inner Products on Functions\n# Inner product: &lt;f, g&gt; = \u222b f(x) g(x) dx\n\ndef inner_product(f, g, x):\n    \"\"\"Approximate inner product via numerical integration\"\"\"\n    dx = x[1] - x[0]\n    return np.sum(f * g) * dx\n\nx = np.linspace(-5, 5, 500)\n\n# Three functions\ng1 = np.exp(-x**2)           # Gaussian at 0\ng2 = np.exp(-(x-2)**2)       # Gaussian at 2 (some overlap)\ng3 = np.exp(-(x-5)**2)       # Gaussian at 5 (little overlap)\n\n# Compute inner products\nip_11 = inner_product(g1, g1, x)  # &lt;g1, g1&gt; = ||g1||\u00b2\nip_12 = inner_product(g1, g2, x)  # &lt;g1, g2&gt; \u2014 some overlap\nip_13 = inner_product(g1, g3, x)  # &lt;g1, g3&gt; \u2014 little overlap\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Plot functions\nax1 = axes[0]\nax1.plot(x, g1, 'b-', lw=2, label=r'$g_1$ (at $x=0$)')\nax1.plot(x, g2, 'r-', lw=2, label=r'$g_2$ (at $x=2$)')\nax1.plot(x, g3, 'g-', lw=2, label=r'$g_3$ (at $x=5$)')\nax1.fill_between(x, 0, g1*g2, alpha=0.3, color='purple', label=r'$g_1 \\cdot g_2$ (overlap)')\nax1.set_title('Functions and Their Overlap')\nax1.legend(); ax1.grid(True, alpha=0.3); ax1.set_xlim(-5, 7)\n\n# Inner product matrix\nax2 = axes[1]\nip_matrix = np.array([[ip_11, ip_12, ip_13],\n                      [ip_12, inner_product(g2,g2,x), inner_product(g2,g3,x)],\n                      [ip_13, inner_product(g2,g3,x), inner_product(g3,g3,x)]])\nim = ax2.imshow(ip_matrix, cmap='Blues')\nax2.set_xticks([0,1,2]); ax2.set_yticks([0,1,2])\nax2.set_xticklabels([r'$g_1$', r'$g_2$', r'$g_3$'])\nax2.set_yticklabels([r'$g_1$', r'$g_2$', r'$g_3$'])\nfor i in range(3):\n    for j in range(3):\n        ax2.text(j, i, f'{ip_matrix[i,j]:.2f}', ha='center', va='center', fontsize=12)\nax2.set_title(r'Inner Product Matrix $\\langle g_i, g_j \\rangle$')\nplt.colorbar(im, ax=ax2)\n\nplt.tight_layout(); plt.show()\nprint(f\"Inner products: &lt;g1,g1&gt;={ip_11:.2f}, &lt;g1,g2&gt;={ip_12:.2f}, &lt;g1,g3&gt;={ip_13:.4f}\")\nprint(\"Large overlap \u2192 large inner product. Little overlap \u2192 small inner product.\")\n</pre> # Example 1.2: Inner Products on Functions # Inner product:  = \u222b f(x) g(x) dx  def inner_product(f, g, x):     \"\"\"Approximate inner product via numerical integration\"\"\"     dx = x[1] - x[0]     return np.sum(f * g) * dx  x = np.linspace(-5, 5, 500)  # Three functions g1 = np.exp(-x**2)           # Gaussian at 0 g2 = np.exp(-(x-2)**2)       # Gaussian at 2 (some overlap) g3 = np.exp(-(x-5)**2)       # Gaussian at 5 (little overlap)  # Compute inner products ip_11 = inner_product(g1, g1, x)  #  = ||g1||\u00b2 ip_12 = inner_product(g1, g2, x)  #  \u2014 some overlap ip_13 = inner_product(g1, g3, x)  #  \u2014 little overlap  fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # Plot functions ax1 = axes[0] ax1.plot(x, g1, 'b-', lw=2, label=r'$g_1$ (at $x=0$)') ax1.plot(x, g2, 'r-', lw=2, label=r'$g_2$ (at $x=2$)') ax1.plot(x, g3, 'g-', lw=2, label=r'$g_3$ (at $x=5$)') ax1.fill_between(x, 0, g1*g2, alpha=0.3, color='purple', label=r'$g_1 \\cdot g_2$ (overlap)') ax1.set_title('Functions and Their Overlap') ax1.legend(); ax1.grid(True, alpha=0.3); ax1.set_xlim(-5, 7)  # Inner product matrix ax2 = axes[1] ip_matrix = np.array([[ip_11, ip_12, ip_13],                       [ip_12, inner_product(g2,g2,x), inner_product(g2,g3,x)],                       [ip_13, inner_product(g2,g3,x), inner_product(g3,g3,x)]]) im = ax2.imshow(ip_matrix, cmap='Blues') ax2.set_xticks([0,1,2]); ax2.set_yticks([0,1,2]) ax2.set_xticklabels([r'$g_1$', r'$g_2$', r'$g_3$']) ax2.set_yticklabels([r'$g_1$', r'$g_2$', r'$g_3$']) for i in range(3):     for j in range(3):         ax2.text(j, i, f'{ip_matrix[i,j]:.2f}', ha='center', va='center', fontsize=12) ax2.set_title(r'Inner Product Matrix $\\langle g_i, g_j \\rangle$') plt.colorbar(im, ax=ax2)  plt.tight_layout(); plt.show() print(f\"Inner products: ={ip_11:.2f}, ={ip_12:.2f}, ={ip_13:.4f}\") print(\"Large overlap \u2192 large inner product. Little overlap \u2192 small inner product.\") <pre>Inner products: &lt;g1,g1&gt;=1.25, &lt;g1,g2&gt;=0.17, &lt;g1,g3&gt;=0.0000\nLarge overlap \u2192 large inner product. Little overlap \u2192 small inner product.\n</pre> In\u00a0[4]: Copied! <pre># Example 2.1: RBF Kernel Visualization\ndef rbf_kernel(x, x_prime, lengthscale=1.0):\n    \"\"\"RBF (Gaussian) kernel: k(x, x') = exp(-||x-x'||\u00b2 / 2\u2113\u00b2)\"\"\"\n    return np.exp(-np.sum((x - x_prime)**2) / (2 * lengthscale**2))\n\ndef rbf_kernel_matrix(X, lengthscale=1.0):\n    \"\"\"Compute kernel matrix K_ij = k(x_i, x_j)\"\"\"\n    n = len(X)\n    K = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            K[i, j] = rbf_kernel(X[i], X[j], lengthscale)\n    return K\n\n# Visualize kernel as a function of distance\ndistances = np.linspace(0, 5, 100)\nlengthscales = [0.5, 1.0, 2.0]\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Kernel vs distance for different lengthscales\nax1 = axes[0]\nfor l in lengthscales:\n    k_vals = np.exp(-distances**2 / (2 * l**2))\n    ax1.plot(distances, k_vals, lw=2, label=rf'$\\ell = {l}$')\nax1.set_xlabel(r'Distance $\\|x - x\\'\\|$')\nax1.set_ylabel(r'$k(x, x\\')$')\nax1.set_title(r'RBF Kernel: $k(x, x\\') = \\exp(-\\|x-x\\'\\|^2 / 2\\ell^2)$')\nax1.legend(); ax1.grid(True, alpha=0.3)\nax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)\n\n# Right: 2D kernel heatmap\nax2 = axes[1]\nx = np.linspace(-3, 3, 50)\ny = np.linspace(-3, 3, 50)\nX, Y = np.meshgrid(x, y)\n# Kernel centered at (0, 0)\nK_2d = np.exp(-(X**2 + Y**2) / (2 * 1.0**2))\nim = ax2.contourf(X, Y, K_2d, levels=20, cmap='viridis')\nax2.plot(0, 0, 'r*', markersize=15, label='Center point')\nax2.set_xlabel('$x$'); ax2.set_ylabel('$y$')\nax2.set_title('2D RBF Kernel (centered at origin)')\nax2.set_aspect('equal'); ax2.legend()\nplt.colorbar(im, ax=ax2, label='$k(x, 0)$')\n\nplt.tight_layout(); plt.show()\nprint(\"Kernel = similarity measure. Large \u2113 \u2192 wider influence. Small \u2113 \u2192 local influence.\")\n</pre> # Example 2.1: RBF Kernel Visualization def rbf_kernel(x, x_prime, lengthscale=1.0):     \"\"\"RBF (Gaussian) kernel: k(x, x') = exp(-||x-x'||\u00b2 / 2\u2113\u00b2)\"\"\"     return np.exp(-np.sum((x - x_prime)**2) / (2 * lengthscale**2))  def rbf_kernel_matrix(X, lengthscale=1.0):     \"\"\"Compute kernel matrix K_ij = k(x_i, x_j)\"\"\"     n = len(X)     K = np.zeros((n, n))     for i in range(n):         for j in range(n):             K[i, j] = rbf_kernel(X[i], X[j], lengthscale)     return K  # Visualize kernel as a function of distance distances = np.linspace(0, 5, 100) lengthscales = [0.5, 1.0, 2.0]  fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # Left: Kernel vs distance for different lengthscales ax1 = axes[0] for l in lengthscales:     k_vals = np.exp(-distances**2 / (2 * l**2))     ax1.plot(distances, k_vals, lw=2, label=rf'$\\ell = {l}$') ax1.set_xlabel(r'Distance $\\|x - x\\'\\|$') ax1.set_ylabel(r'$k(x, x\\')$') ax1.set_title(r'RBF Kernel: $k(x, x\\') = \\exp(-\\|x-x\\'\\|^2 / 2\\ell^2)$') ax1.legend(); ax1.grid(True, alpha=0.3) ax1.axhline(y=0.5, color='gray', linestyle='--', alpha=0.5)  # Right: 2D kernel heatmap ax2 = axes[1] x = np.linspace(-3, 3, 50) y = np.linspace(-3, 3, 50) X, Y = np.meshgrid(x, y) # Kernel centered at (0, 0) K_2d = np.exp(-(X**2 + Y**2) / (2 * 1.0**2)) im = ax2.contourf(X, Y, K_2d, levels=20, cmap='viridis') ax2.plot(0, 0, 'r*', markersize=15, label='Center point') ax2.set_xlabel('$x$'); ax2.set_ylabel('$y$') ax2.set_title('2D RBF Kernel (centered at origin)') ax2.set_aspect('equal'); ax2.legend() plt.colorbar(im, ax=ax2, label='$k(x, 0)$')  plt.tight_layout(); plt.show() print(\"Kernel = similarity measure. Large \u2113 \u2192 wider influence. Small \u2113 \u2192 local influence.\") <pre>&lt;&gt;:25: SyntaxWarning: invalid escape sequence '\\e'\n&lt;&gt;:26: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:28: SyntaxWarning: invalid escape sequence '\\e'\n&lt;&gt;:25: SyntaxWarning: invalid escape sequence '\\e'\n&lt;&gt;:26: SyntaxWarning: invalid escape sequence '\\|'\n&lt;&gt;:28: SyntaxWarning: invalid escape sequence '\\e'\n/var/folders/jt/h4k6wdyx36nbnjk_rmkwdk280000gn/T/ipykernel_62045/4146359833.py:25: SyntaxWarning: invalid escape sequence '\\e'\n  ax1.plot(distances, k_vals, lw=2, label=f'$\\ell = {l}$')\n/var/folders/jt/h4k6wdyx36nbnjk_rmkwdk280000gn/T/ipykernel_62045/4146359833.py:26: SyntaxWarning: invalid escape sequence '\\|'\n  ax1.set_xlabel('Distance $\\|x - x\\'\\|$')\n/var/folders/jt/h4k6wdyx36nbnjk_rmkwdk280000gn/T/ipykernel_62045/4146359833.py:28: SyntaxWarning: invalid escape sequence '\\e'\n  ax1.set_title('RBF Kernel: $k(x, x\\') = \\exp(-\\|x-x\\'\\|^2 / 2\\ell^2)$')\n</pre> <pre>Kernel = similarity measure. Large \u2113 \u2192 wider influence. Small \u2113 \u2192 local influence.\n</pre> In\u00a0[5]: Copied! <pre># Example 3.1: Building a Field from Particles\ndef build_field(X, Y, particles, lengthscale=0.8):\n    \"\"\"Build Q\u207a field from particles: Q\u207a(z) = \u03a3\u1d62 w\u1d62 k(z, z\u1d62)\"\"\"\n    Z = np.zeros_like(X)\n    for p in particles:\n        r2 = (X - p['x'])**2 + (Y - p['y'])**2\n        Z += p['w'] * np.exp(-r2 / (2 * lengthscale**2))\n    return Z\n\n# Create grid\nx = np.linspace(-4, 4, 60)\ny = np.linspace(-4, 4, 60)\nX, Y = np.meshgrid(x, y)\n\n# Single particle\nsingle = [{'x': 0, 'y': 0, 'w': 1.0}]\nZ_single = build_field(X, Y, single)\n\n# Multiple particles\nmulti = [\n    {'x': 2, 'y': 2, 'w': 2.0},     # Strong positive\n    {'x': -2, 'y': 1, 'w': 1.5},    # Positive\n    {'x': 0, 'y': -2, 'w': -1.5},   # Negative\n    {'x': -2, 'y': -2, 'w': -1.0},  # Negative\n]\nZ_multi = build_field(X, Y, multi)\n\nfig = plt.figure(figsize=(16, 5))\n\n# Single particle\nax1 = fig.add_subplot(131, projection='3d')\nax1.plot_surface(X, Y, Z_single, cmap='viridis', alpha=0.9)\nax1.set_title('Single Particle\\n$Q^+(z) = w_1 k(z, z_1)$')\nax1.set_xlabel('$x$'); ax1.set_ylabel('$y$')\n\n# Multiple particles - 3D\nax2 = fig.add_subplot(132, projection='3d')\nax2.plot_surface(X, Y, Z_multi, cmap='RdBu_r', alpha=0.9)\nax2.set_title(r'Multiple Particles' + '\\n' + r'$Q^+(z) = \\sum_i w_i k(z, z_i)$')\nax2.set_xlabel('$x$'); ax2.set_ylabel('$y$')\n\n# Multiple particles - 2D with particles marked\nax3 = fig.add_subplot(133)\nc = ax3.contourf(X, Y, Z_multi, levels=25, cmap='RdBu_r', alpha=0.8)\nax3.contour(X, Y, Z_multi, levels=[0], colors='k', linewidths=2, linestyles='--')\nfor p in multi:\n    color = 'blue' if p['w'] &gt; 0 else 'red'\n    marker = 'o' if p['w'] &gt; 0 else 's'\n    size = 100 + 50 * abs(p['w'])\n    ax3.scatter(p['x'], p['y'], c=color, s=size, marker=marker, edgecolors='k', linewidths=2)\nax3.set_title('Field with Particles\\n(Blue=positive, Red=negative)')\nax3.set_xlabel('$x$'); ax3.set_ylabel('$y$'); ax3.set_aspect('equal')\nplt.colorbar(c, ax=ax3, label='$Q^+(z)$')\n\nplt.tight_layout(); plt.show()\nprint(\"Each particle creates a 'bump'. The field is their superposition.\")\nprint(\"This is EXACTLY how GRL represents the value function!\")\n</pre> # Example 3.1: Building a Field from Particles def build_field(X, Y, particles, lengthscale=0.8):     \"\"\"Build Q\u207a field from particles: Q\u207a(z) = \u03a3\u1d62 w\u1d62 k(z, z\u1d62)\"\"\"     Z = np.zeros_like(X)     for p in particles:         r2 = (X - p['x'])**2 + (Y - p['y'])**2         Z += p['w'] * np.exp(-r2 / (2 * lengthscale**2))     return Z  # Create grid x = np.linspace(-4, 4, 60) y = np.linspace(-4, 4, 60) X, Y = np.meshgrid(x, y)  # Single particle single = [{'x': 0, 'y': 0, 'w': 1.0}] Z_single = build_field(X, Y, single)  # Multiple particles multi = [     {'x': 2, 'y': 2, 'w': 2.0},     # Strong positive     {'x': -2, 'y': 1, 'w': 1.5},    # Positive     {'x': 0, 'y': -2, 'w': -1.5},   # Negative     {'x': -2, 'y': -2, 'w': -1.0},  # Negative ] Z_multi = build_field(X, Y, multi)  fig = plt.figure(figsize=(16, 5))  # Single particle ax1 = fig.add_subplot(131, projection='3d') ax1.plot_surface(X, Y, Z_single, cmap='viridis', alpha=0.9) ax1.set_title('Single Particle\\n$Q^+(z) = w_1 k(z, z_1)$') ax1.set_xlabel('$x$'); ax1.set_ylabel('$y$')  # Multiple particles - 3D ax2 = fig.add_subplot(132, projection='3d') ax2.plot_surface(X, Y, Z_multi, cmap='RdBu_r', alpha=0.9) ax2.set_title(r'Multiple Particles' + '\\n' + r'$Q^+(z) = \\sum_i w_i k(z, z_i)$') ax2.set_xlabel('$x$'); ax2.set_ylabel('$y$')  # Multiple particles - 2D with particles marked ax3 = fig.add_subplot(133) c = ax3.contourf(X, Y, Z_multi, levels=25, cmap='RdBu_r', alpha=0.8) ax3.contour(X, Y, Z_multi, levels=[0], colors='k', linewidths=2, linestyles='--') for p in multi:     color = 'blue' if p['w'] &gt; 0 else 'red'     marker = 'o' if p['w'] &gt; 0 else 's'     size = 100 + 50 * abs(p['w'])     ax3.scatter(p['x'], p['y'], c=color, s=size, marker=marker, edgecolors='k', linewidths=2) ax3.set_title('Field with Particles\\n(Blue=positive, Red=negative)') ax3.set_xlabel('$x$'); ax3.set_ylabel('$y$'); ax3.set_aspect('equal') plt.colorbar(c, ax=ax3, label='$Q^+(z)$')  plt.tight_layout(); plt.show() print(\"Each particle creates a 'bump'. The field is their superposition.\") print(\"This is EXACTLY how GRL represents the value function!\") <pre>Each particle creates a 'bump'. The field is their superposition.\nThis is EXACTLY how GRL represents the value function!\n</pre> In\u00a0[6]: Copied! <pre># Example 3.2: Effect of Lengthscale\nparticles = [\n    {'x': 1.5, 'y': 1.5, 'w': 2.0},\n    {'x': -1.5, 'y': 0, 'w': 1.5},\n    {'x': 0, 'y': -1.5, 'w': -1.5},\n]\n\nlengthscales = [0.4, 0.8, 1.5]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4.5))\n\nfor ax, l in zip(axes, lengthscales):\n    Z = build_field(X, Y, particles, lengthscale=l)\n    c = ax.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.8)\n    ax.contour(X, Y, Z, levels=[0], colors='k', linewidths=1.5, linestyles='--')\n    for p in particles:\n        color = 'blue' if p['w'] &gt; 0 else 'red'\n        ax.plot(p['x'], p['y'], 'o' if p['w']&gt;0 else 's', color=color, ms=12, mec='k', mew=2)\n    ax.set_title(rf'Lengthscale $\\ell = {l}$')\n    ax.set_xlabel('$x$'); ax.set_ylabel('$y$'); ax.set_aspect('equal')\n    plt.colorbar(c, ax=ax, shrink=0.8)\n\nplt.tight_layout(); plt.show()\nprint(\"Small \u2113: Sharp, localized bumps. Large \u2113: Smooth, spread-out influence.\")\nprint(\"In GRL, \u2113 controls how far each experience 'spreads' its influence.\")\n</pre> # Example 3.2: Effect of Lengthscale particles = [     {'x': 1.5, 'y': 1.5, 'w': 2.0},     {'x': -1.5, 'y': 0, 'w': 1.5},     {'x': 0, 'y': -1.5, 'w': -1.5}, ]  lengthscales = [0.4, 0.8, 1.5]  fig, axes = plt.subplots(1, 3, figsize=(15, 4.5))  for ax, l in zip(axes, lengthscales):     Z = build_field(X, Y, particles, lengthscale=l)     c = ax.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.8)     ax.contour(X, Y, Z, levels=[0], colors='k', linewidths=1.5, linestyles='--')     for p in particles:         color = 'blue' if p['w'] &gt; 0 else 'red'         ax.plot(p['x'], p['y'], 'o' if p['w']&gt;0 else 's', color=color, ms=12, mec='k', mew=2)     ax.set_title(rf'Lengthscale $\\ell = {l}$')     ax.set_xlabel('$x$'); ax.set_ylabel('$y$'); ax.set_aspect('equal')     plt.colorbar(c, ax=ax, shrink=0.8)  plt.tight_layout(); plt.show() print(\"Small \u2113: Sharp, localized bumps. Large \u2113: Smooth, spread-out influence.\") print(\"In GRL, \u2113 controls how far each experience 'spreads' its influence.\") <pre>Small \u2113: Sharp, localized bumps. Large \u2113: Smooth, spread-out influence.\nIn GRL, \u2113 controls how far each experience 'spreads' its influence.\n</pre> In\u00a0[7]: Copied! <pre># Example 4.1: Gradient Field Visualization\ndef build_gradient(X, Y, particles, lengthscale=0.8):\n    \"\"\"Compute gradient of Q\u207a: \u2207Q\u207a = \u03a3\u1d62 w\u1d62 \u2207k(z, z\u1d62)\"\"\"\n    U = np.zeros_like(X)\n    V = np.zeros_like(Y)\n    for p in particles:\n        dx, dy = X - p['x'], Y - p['y']\n        r2 = dx**2 + dy**2\n        k = np.exp(-r2 / (2 * lengthscale**2))\n        factor = -p['w'] / (lengthscale**2)\n        U += factor * dx * k\n        V += factor * dy * k\n    return U, V\n\n# Particles\nparticles = [\n    {'x': 2, 'y': 2, 'w': 2.0},\n    {'x': -2, 'y': 1, 'w': 1.5},\n    {'x': 0, 'y': -2, 'w': -1.5},\n]\n\nZ = build_field(X, Y, particles)\nU, V = build_gradient(X, Y, particles)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Field with gradient arrows\nax1 = axes[0]\nc = ax1.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.6)\nskip = 4\nax1.quiver(X[::skip,::skip], Y[::skip,::skip], U[::skip,::skip], V[::skip,::skip],\n           color='black', alpha=0.7, scale=20)\nfor p in particles:\n    color = 'blue' if p['w'] &gt; 0 else 'red'\n    ax1.plot(p['x'], p['y'], 'o' if p['w']&gt;0 else 's', color=color, ms=15, mec='k', mew=2)\nax1.set_title(r'$Q^+$ Field with Gradient $\\nabla Q^+$')\nax1.set_xlabel('$x$'); ax1.set_ylabel('$y$'); ax1.set_aspect('equal')\nplt.colorbar(c, ax=ax1, label='$Q^+(z)$')\n\n# Streamlines\nax2 = axes[1]\nc2 = ax2.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.6)\nax2.streamplot(X, Y, U, V, color='black', density=1.5, linewidth=1)\nfor p in particles:\n    color = 'blue' if p['w'] &gt; 0 else 'red'\n    ax2.plot(p['x'], p['y'], 'o' if p['w']&gt;0 else 's', color=color, ms=15, mec='k', mew=2)\nax2.set_title('Streamlines (Gradient Ascent Paths)')\nax2.set_xlabel('$x$'); ax2.set_ylabel('$y$'); ax2.set_aspect('equal')\nplt.colorbar(c2, ax=ax2, label='$Q^+(z)$')\n\nplt.tight_layout(); plt.show()\nprint(\"Gradient points toward positive particles (good) and away from negative (bad).\")\nprint(\"Following the gradient = improving the action = GRL's policy!\")\n</pre> # Example 4.1: Gradient Field Visualization def build_gradient(X, Y, particles, lengthscale=0.8):     \"\"\"Compute gradient of Q\u207a: \u2207Q\u207a = \u03a3\u1d62 w\u1d62 \u2207k(z, z\u1d62)\"\"\"     U = np.zeros_like(X)     V = np.zeros_like(Y)     for p in particles:         dx, dy = X - p['x'], Y - p['y']         r2 = dx**2 + dy**2         k = np.exp(-r2 / (2 * lengthscale**2))         factor = -p['w'] / (lengthscale**2)         U += factor * dx * k         V += factor * dy * k     return U, V  # Particles particles = [     {'x': 2, 'y': 2, 'w': 2.0},     {'x': -2, 'y': 1, 'w': 1.5},     {'x': 0, 'y': -2, 'w': -1.5}, ]  Z = build_field(X, Y, particles) U, V = build_gradient(X, Y, particles)  fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # Field with gradient arrows ax1 = axes[0] c = ax1.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.6) skip = 4 ax1.quiver(X[::skip,::skip], Y[::skip,::skip], U[::skip,::skip], V[::skip,::skip],            color='black', alpha=0.7, scale=20) for p in particles:     color = 'blue' if p['w'] &gt; 0 else 'red'     ax1.plot(p['x'], p['y'], 'o' if p['w']&gt;0 else 's', color=color, ms=15, mec='k', mew=2) ax1.set_title(r'$Q^+$ Field with Gradient $\\nabla Q^+$') ax1.set_xlabel('$x$'); ax1.set_ylabel('$y$'); ax1.set_aspect('equal') plt.colorbar(c, ax=ax1, label='$Q^+(z)$')  # Streamlines ax2 = axes[1] c2 = ax2.contourf(X, Y, Z, levels=25, cmap='RdBu_r', alpha=0.6) ax2.streamplot(X, Y, U, V, color='black', density=1.5, linewidth=1) for p in particles:     color = 'blue' if p['w'] &gt; 0 else 'red'     ax2.plot(p['x'], p['y'], 'o' if p['w']&gt;0 else 's', color=color, ms=15, mec='k', mew=2) ax2.set_title('Streamlines (Gradient Ascent Paths)') ax2.set_xlabel('$x$'); ax2.set_ylabel('$y$'); ax2.set_aspect('equal') plt.colorbar(c2, ax=ax2, label='$Q^+(z)$')  plt.tight_layout(); plt.show() print(\"Gradient points toward positive particles (good) and away from negative (bad).\") print(\"Following the gradient = improving the action = GRL's policy!\") <pre>Gradient points toward positive particles (good) and away from negative (bad).\nFollowing the gradient = improving the action = GRL's policy!\n</pre> In\u00a0[8]: Copied! <pre># Example 5.1: Kernel as Inner Product\n# k(x, x') = &lt;\u03c6(x), \u03c6(x')&gt; where \u03c6 is the feature map\n\n# For RBF kernel, the feature map is infinite-dimensional!\n# But we can visualize the kernel matrix as a \"similarity matrix\"\n\n# Sample points\nnp.random.seed(42)\npoints = np.array([[-2, 0], [-1, 1], [0, 0], [1, -1], [2, 1]])\nn = len(points)\n\n# Compute kernel matrix\nK = np.zeros((n, n))\nfor i in range(n):\n    for j in range(n):\n        K[i, j] = np.exp(-np.sum((points[i] - points[j])**2) / (2 * 1.0**2))\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Points in space\nax1 = axes[0]\ncolors = plt.cm.tab10(np.linspace(0, 1, n))\nfor i, (p, c) in enumerate(zip(points, colors)):\n    ax1.scatter(p[0], p[1], c=[c], s=200, edgecolors='k', linewidths=2, zorder=5)\n    ax1.annotate(f'$z_{i+1}$', (p[0]+0.15, p[1]+0.15), fontsize=12)\nax1.set_xlabel('$x$'); ax1.set_ylabel('$y$')\nax1.set_title('Points in Space')\nax1.set_xlim(-3, 3); ax1.set_ylim(-2, 2); ax1.set_aspect('equal')\nax1.grid(True, alpha=0.3)\n\n# Kernel matrix\nax2 = axes[1]\nim = ax2.imshow(K, cmap='Blues', vmin=0, vmax=1)\nax2.set_xticks(range(n)); ax2.set_yticks(range(n))\nax2.set_xticklabels([f'$z_{i+1}$' for i in range(n)])\nax2.set_yticklabels([f'$z_{i+1}$' for i in range(n)])\nfor i in range(n):\n    for j in range(n):\n        ax2.text(j, i, f'{K[i,j]:.2f}', ha='center', va='center', fontsize=10)\nax2.set_title(r'Kernel Matrix $K_{ij} = k(z_i, z_j)$')\nplt.colorbar(im, ax=ax2, label='Similarity')\n\nplt.tight_layout(); plt.show()\nprint(\"Kernel matrix = similarity matrix = inner products in feature space.\")\nprint(\"Diagonal = 1 (self-similarity). Off-diagonal = similarity between points.\")\n</pre> # Example 5.1: Kernel as Inner Product # k(x, x') = &lt;\u03c6(x), \u03c6(x')&gt; where \u03c6 is the feature map  # For RBF kernel, the feature map is infinite-dimensional! # But we can visualize the kernel matrix as a \"similarity matrix\"  # Sample points np.random.seed(42) points = np.array([[-2, 0], [-1, 1], [0, 0], [1, -1], [2, 1]]) n = len(points)  # Compute kernel matrix K = np.zeros((n, n)) for i in range(n):     for j in range(n):         K[i, j] = np.exp(-np.sum((points[i] - points[j])**2) / (2 * 1.0**2))  fig, axes = plt.subplots(1, 2, figsize=(12, 5))  # Points in space ax1 = axes[0] colors = plt.cm.tab10(np.linspace(0, 1, n)) for i, (p, c) in enumerate(zip(points, colors)):     ax1.scatter(p[0], p[1], c=[c], s=200, edgecolors='k', linewidths=2, zorder=5)     ax1.annotate(f'$z_{i+1}$', (p[0]+0.15, p[1]+0.15), fontsize=12) ax1.set_xlabel('$x$'); ax1.set_ylabel('$y$') ax1.set_title('Points in Space') ax1.set_xlim(-3, 3); ax1.set_ylim(-2, 2); ax1.set_aspect('equal') ax1.grid(True, alpha=0.3)  # Kernel matrix ax2 = axes[1] im = ax2.imshow(K, cmap='Blues', vmin=0, vmax=1) ax2.set_xticks(range(n)); ax2.set_yticks(range(n)) ax2.set_xticklabels([f'$z_{i+1}$' for i in range(n)]) ax2.set_yticklabels([f'$z_{i+1}$' for i in range(n)]) for i in range(n):     for j in range(n):         ax2.text(j, i, f'{K[i,j]:.2f}', ha='center', va='center', fontsize=10) ax2.set_title(r'Kernel Matrix $K_{ij} = k(z_i, z_j)$') plt.colorbar(im, ax=ax2, label='Similarity')  plt.tight_layout(); plt.show() print(\"Kernel matrix = similarity matrix = inner products in feature space.\") print(\"Diagonal = 1 (self-similarity). Off-diagonal = similarity between points.\") <pre>Kernel matrix = similarity matrix = inner products in feature space.\nDiagonal = 1 (self-similarity). Off-diagonal = similarity between points.\n</pre>"},{"location":"notebooks/field_series/02_functional_fields/#notebook-2-functional-fields","title":"Notebook 2: Functional Fields\u00b6","text":"<p>Part 2 of the GRL Field Series</p>"},{"location":"notebooks/field_series/02_functional_fields/#overview","title":"Overview\u00b6","text":"<p>In Notebook 1, we saw vector fields \u2014 arrows at each point. Now we take the conceptual leap to functional fields, where each point is associated with a function (an infinite-dimensional vector).</p> <p>This is the mathematical foundation of GRL's reinforcement field.</p>"},{"location":"notebooks/field_series/02_functional_fields/#learning-objectives","title":"Learning Objectives\u00b6","text":"<ol> <li>Functions as vectors \u2014 Addition, scaling, inner products</li> <li>Kernel functions \u2014 Measuring similarity between points</li> <li>RKHS intuition \u2014 Reproducing Kernel Hilbert Space</li> <li>Functional gradients \u2014 Optimization in function space</li> <li>Bridge to GRL \u2014 How particles create the reinforcement field</li> </ol>"},{"location":"notebooks/field_series/02_functional_fields/#prerequisites","title":"Prerequisites\u00b6","text":"<ul> <li>Notebook 1 (Classical Vector Fields)</li> <li>Basic linear algebra (vectors, inner products)</li> </ul>"},{"location":"notebooks/field_series/02_functional_fields/#time","title":"Time\u00b6","text":"<p>~25-30 minutes</p>"},{"location":"notebooks/field_series/02_functional_fields/#part-1-functions-as-vectors","title":"Part 1: Functions as Vectors\u00b6","text":""},{"location":"notebooks/field_series/02_functional_fields/#the-key-insight","title":"The Key Insight\u00b6","text":"<p>Functions can be treated as vectors in an infinite-dimensional space!</p> Operation Finite Vectors Functions Addition $\\mathbf{u} + \\mathbf{v}$ $(f + g)(x) = f(x) + g(x)$ Scaling $c \\cdot \\mathbf{u}$ $(cf)(x) = c \\cdot f(x)$ Inner Product $\\mathbf{u} \\cdot \\mathbf{v} = \\sum_i u_i v_i$ $\\langle f, g \\rangle = \\int f(x) g(x) dx$ Norm $\\|\\mathbf{u}\\| = \\sqrt{\\mathbf{u} \\cdot \\mathbf{u}}$ $\\|f\\| = \\sqrt{\\langle f, f \\rangle}$"},{"location":"notebooks/field_series/02_functional_fields/#why-this-matters-for-grl","title":"Why This Matters for GRL\u00b6","text":"<p>In GRL, the reinforcement field $Q^+$ is a function (a vector in function space). Operations like \"adding experience\" become vector addition in this space.</p>"},{"location":"notebooks/field_series/02_functional_fields/#part-2-kernel-functions-similarity-in-function-space","title":"Part 2: Kernel Functions \u2014 Similarity in Function Space\u00b6","text":""},{"location":"notebooks/field_series/02_functional_fields/#what-is-a-kernel","title":"What is a Kernel?\u00b6","text":"<p>A kernel function $k(x, x')$ measures similarity between two points:</p> <p>$$k: \\mathcal{X} \\times \\mathcal{X} \\to \\mathbb{R}$$</p>"},{"location":"notebooks/field_series/02_functional_fields/#the-rbf-gaussian-kernel","title":"The RBF (Gaussian) Kernel\u00b6","text":"<p>The most common kernel is the Radial Basis Function (RBF) kernel:</p> <p>$$k(x, x') = \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\ell^2}\\right)$$</p> <p>where $\\ell$ is the lengthscale parameter.</p> <p>Properties:</p> <ul> <li>$k(x, x) = 1$ (self-similarity is maximal)</li> <li>$k(x, x') \\to 0$ as $\\|x - x'\\| \\to \\infty$ (distant points are dissimilar)</li> <li>$\\ell$ controls the \"range of influence\"</li> </ul>"},{"location":"notebooks/field_series/02_functional_fields/#connection-to-rkhs","title":"Connection to RKHS\u00b6","text":"<p>The kernel defines an implicit feature map $\\phi(x)$ such that:</p> <p>$$k(x, x') = \\langle \\phi(x), \\phi(x') \\rangle$$</p> <p>The kernel IS the inner product in feature space!</p>"},{"location":"notebooks/field_series/02_functional_fields/#part-3-from-kernels-to-fields-the-grl-connection","title":"Part 3: From Kernels to Fields \u2014 The GRL Connection\u00b6","text":""},{"location":"notebooks/field_series/02_functional_fields/#the-key-equation","title":"The Key Equation\u00b6","text":"<p>In GRL, the reinforcement field is built from particles using kernels:</p> <p>$$Q^+(z) = \\sum_{i=1}^{N} w_i \\, k(z, z_i)$$</p> <p>where:</p> <ul> <li>$z = (s, \\theta)$ is a point in augmented state-action space</li> <li>$(z_i, w_i)$ are experience particles with positions and weights</li> <li>$k(z, z_i)$ is the kernel (e.g., RBF)</li> </ul>"},{"location":"notebooks/field_series/02_functional_fields/#interpretation","title":"Interpretation\u00b6","text":"<p>Each particle creates a \"bump\" in the field:</p> <ul> <li>Positive weight ($w_i &gt; 0$): Creates a peak (good region)</li> <li>Negative weight ($w_i &lt; 0$): Creates a valley (bad region)</li> </ul> <p>The total field is the superposition of all bumps \u2014 exactly like we saw in Notebook 1!</p>"},{"location":"notebooks/field_series/02_functional_fields/#part-4-functional-gradients","title":"Part 4: Functional Gradients\u00b6","text":""},{"location":"notebooks/field_series/02_functional_fields/#gradient-of-the-field","title":"Gradient of the Field\u00b6","text":"<p>The gradient of $Q^+$ tells us the direction of improvement:</p> <p>$$\\nabla Q^+(z) = \\sum_{i=1}^{N} w_i \\nabla_z k(z, z_i)$$</p> <p>For the RBF kernel:</p> <p>$$\\nabla_z k(z, z_i) = -\\frac{z - z_i}{\\ell^2} k(z, z_i)$$</p>"},{"location":"notebooks/field_series/02_functional_fields/#policy-from-gradient","title":"Policy from Gradient\u00b6","text":"<p>In GRL, the agent can improve its action by following the gradient:</p> <p>$$a_{\\text{better}} = a + \\eta \\nabla_a Q^+(s, a)$$</p> <p>This is how policy emerges from the field!</p>"},{"location":"notebooks/field_series/02_functional_fields/#part-5-the-rkhs-perspective","title":"Part 5: The RKHS Perspective\u00b6","text":""},{"location":"notebooks/field_series/02_functional_fields/#what-is-rkhs","title":"What is RKHS?\u00b6","text":"<p>A Reproducing Kernel Hilbert Space (RKHS) is a function space where:</p> <ol> <li>Functions can be added and scaled (vector space)</li> <li>There's an inner product (Hilbert space)</li> <li>Evaluation is continuous: $f(x) = \\langle f, k(x, \\cdot) \\rangle$</li> </ol>"},{"location":"notebooks/field_series/02_functional_fields/#the-reproducing-property","title":"The Reproducing Property\u00b6","text":"<p>The kernel \"reproduces\" function values:</p> <p>$$f(x) = \\langle f, k(x, \\cdot) \\rangle_{\\mathcal{H}_k}$$</p> <p>And importantly:</p> <p>$$k(x, x') = \\langle k(x, \\cdot), k(x', \\cdot) \\rangle_{\\mathcal{H}_k}$$</p> <p>The kernel IS the inner product between feature representations!</p>"},{"location":"notebooks/field_series/02_functional_fields/#why-this-matters","title":"Why This Matters\u00b6","text":"<p>In GRL:</p> <ul> <li>$Q^+ = \\sum_i w_i k(z_i, \\cdot)$ is a vector in RKHS</li> <li>Each particle contributes a basis function $k(z_i, \\cdot)$</li> <li>The field is a linear combination of these basis functions</li> <li>All operations (evaluation, gradient, update) are well-defined</li> </ul>"},{"location":"notebooks/field_series/02_functional_fields/#summary-from-vectors-to-functions-to-grl","title":"Summary: From Vectors to Functions to GRL\u00b6","text":""},{"location":"notebooks/field_series/02_functional_fields/#the-progression","title":"The Progression\u00b6","text":"Concept Classical Vectors Functions in RKHS GRL Element Arrow $\\mathbf{v}$ Function $f(\\cdot)$ Field $Q^+(\\cdot)$ Basis $\\mathbf{e}_1, \\mathbf{e}_2, ...$ $k(z_1, \\cdot), k(z_2, \\cdot), ...$ Particle kernels Representation $\\mathbf{v} = \\sum_i v_i \\mathbf{e}_i$ $f = \\sum_i w_i k(z_i, \\cdot)$ $Q^+ = \\sum_i w_i k(z_i, \\cdot)$ Inner Product $\\mathbf{u} \\cdot \\mathbf{v}$ $\\langle f, g \\rangle_{\\mathcal{H}}$ $k(z, z')$ Gradient $\\nabla V$ $\\nabla_f J[f]$ $\\nabla Q^+$"},{"location":"notebooks/field_series/02_functional_fields/#key-equations","title":"Key Equations\u00b6","text":"<p>RBF Kernel: $$k(z, z') = \\exp\\left(-\\frac{\\|z - z'\\|^2}{2\\ell^2}\\right)$$</p> <p>Reinforcement Field: $$Q^+(z) = \\sum_{i=1}^{N} w_i \\, k(z, z_i)$$</p> <p>Gradient: $$\\nabla Q^+(z) = \\sum_{i=1}^{N} w_i \\nabla_z k(z, z_i) = -\\sum_{i=1}^{N} \\frac{w_i}{\\ell^2} (z - z_i) k(z, z_i)$$</p>"},{"location":"notebooks/field_series/02_functional_fields/#whats-next","title":"What's Next?\u00b6","text":"<p>In Notebook 3, we'll apply these concepts to a 2D navigation domain and see how GRL's reinforcement field guides an agent to find optimal paths!</p>"},{"location":"notebooks/field_series/ROADMAP/","title":"Field Series Roadmap","text":"<p>Building GRL Understanding Systematically</p> <p>This document tracks the progression from foundational concepts to complete GRL algorithms.</p>"},{"location":"notebooks/field_series/ROADMAP/#completed-foundation-notebooks-0-3","title":"\u2705 Completed: Foundation (Notebooks 0-3)","text":""},{"location":"notebooks/field_series/ROADMAP/#notebook-0-introduction-to-vector-fields","title":"Notebook 0: Introduction to Vector Fields","text":"<p>Status: Complete Topics: Real-world examples, basic intuition Time: ~10-15 minutes</p>"},{"location":"notebooks/field_series/ROADMAP/#notebook-1-classical-vector-fields","title":"Notebook 1: Classical Vector Fields","text":"<p>Status: Complete Topics:  - Vector field definition and visualization - Gradient fields (connection to optimization) - Rotational fields and curl - Superposition of fields - Trajectories following gradients</p> <p>Time: ~20-25 minutes</p>"},{"location":"notebooks/field_series/ROADMAP/#notebook-1a-vector-fields-and-odes","title":"Notebook 1a: Vector Fields and ODEs","text":"<p>Status: Complete Topics: - ODEs as following vector fields (\\(\\dot{x} = F(x)\\)) - Numerical solvers (Euler, RK4) - Phase portraits and fixed points - Gradient flow (optimization as ODE) - Connection to flow matching (genai-lab)</p> <p>Time: ~25-30 minutes</p>"},{"location":"notebooks/field_series/ROADMAP/#notebook-2-functional-fields","title":"Notebook 2: Functional Fields","text":"<p>Status: Complete Topics: - Functions as infinite-dimensional vectors - Kernel functions and similarity - RKHS intuition - Functional gradients - Superposition in function space</p> <p>Time: ~20-25 minutes</p>"},{"location":"notebooks/field_series/ROADMAP/#notebook-3-reinforcement-fields","title":"Notebook 3: Reinforcement Fields","text":"<p>Status: Complete Topics: - Augmented state-action space: \\(z = (s, \\theta)\\) - Particle memory: \\(\\{(z_i, w_i)\\}\\) - Field emergence: \\(Q^+(z) = \\sum_i w_i k(z, z_i)\\) - Basic policy inference: \\(\\theta^* = \\arg\\max_\\theta Q^+(s, \\theta)\\) (discrete search) - Obstacles via negative particles</p> <p>Time: ~30 minutes</p> <p>Supplementary: - <code>03a_particle_coverage_effects.ipynb</code> \u2014 Visual proof of particle coverage effects - <code>particle_vs_gradient_fields.md</code> \u2014 Theory comparison</p>"},{"location":"notebooks/field_series/ROADMAP/#in-progress-planned-learning-algorithms","title":"\ud83d\udea7 In Progress / Planned: Learning Algorithms","text":""},{"location":"notebooks/field_series/ROADMAP/#notebook-4-policy-inference-planned","title":"Notebook 4: Policy Inference (Planned)","text":"<p>Goal: Deep dive into how agents extract policies from the Q\u207a field</p> <p>Topics to Cover:</p> <ol> <li>Greedy Policy (already introduced in Notebook 3)</li> <li>Discrete action search: \\(\\theta^* = \\arg\\max_\\theta Q^+(s, \\theta)\\)</li> <li>Computational considerations (number of angles)</li> <li> <p>Limitations of discrete search</p> </li> <li> <p>Gradient-Based Policy (new)</p> </li> <li>Continuous optimization: \\(\\nabla_\\theta Q^+(s, \\theta) = 0\\)</li> <li>Gradient ascent on action space</li> <li> <p>Connection to policy gradient methods</p> </li> <li> <p>Boltzmann (Soft) Policy (new)</p> </li> <li>Exploration via softmax: \\(\\pi(\\theta|s) \\propto \\exp(\\beta Q^+(s, \\theta))\\)</li> <li>Temperature parameter \\(\\beta\\)</li> <li> <p>Entropy regularization</p> </li> <li> <p>Action Landscapes (expand from Notebook 3)</p> </li> <li>Visualizing \\(Q^+(s, \\cdot)\\) for fixed states</li> <li>Multi-modal action distributions</li> <li>Local vs. global optima</li> </ol> <p>Visualizations: - Polar plots of action landscapes at different temperatures - Comparison: greedy vs. Boltzmann sampling - Interactive sliders for temperature \\(\\beta\\)</p> <p>Time Estimate: ~25-30 minutes</p> <p>Prerequisites: Notebook 3</p>"},{"location":"notebooks/field_series/ROADMAP/#notebook-5-memory-update-learning-from-experience-planned","title":"Notebook 5: Memory Update \u2014 Learning from Experience (Planned)","text":"<p>Goal: Understand how the field evolves as the agent learns</p> <p>Topics to Cover:</p> <ol> <li>Single Particle Addition</li> <li>New experience: \\((s, a, r)\\)</li> <li>Creating particle: \\((z_{new}, w_{new})\\) where \\(z_{new} = (s, a)\\)</li> <li> <p>Weight assignment: \\(w_{new} = f(r, \\gamma, ...)\\)</p> </li> <li> <p>Field Evolution</p> </li> <li>Before/after comparison</li> <li>Difference map: \\(\\Delta Q^+ = Q^+_{after} - Q^+_{before}\\)</li> <li>\"Ripple\" effect from new particle</li> <li> <p>Kernel lengthscale controls influence radius</p> </li> <li> <p>MemoryUpdate Algorithm</p> </li> <li>Pseudocode walkthrough</li> <li>When to add positive vs. negative particles</li> <li> <p>Memory management (capacity limits)</p> </li> <li> <p>Interactive Demonstration</p> </li> <li>Click to add particles</li> <li>See field update in real-time</li> <li>Observe policy changes</li> </ol> <p>Visualizations: - Side-by-side: Q\u207a before/after adding particle - Heatmap of \\(\\Delta Q^+\\) - Animated field evolution over multiple updates - Policy vector field changes</p> <p>Code Examples: <pre><code>def add_particle(particles, z_new, w_new):\n    \"\"\"Add a new particle to memory.\"\"\"\n    particles.append({'z': z_new, 'w': w_new})\n    return particles\n\ndef compute_field_difference(X, Y, particles_before, particles_after):\n    \"\"\"Visualize how field changed.\"\"\"\n    Q_before = compute_Q_field(X, Y, particles_before)\n    Q_after = compute_Q_field(X, Y, particles_after)\n    return Q_after - Q_before\n</code></pre></p> <p>Time Estimate: ~30-35 minutes</p> <p>Prerequisites: Notebooks 3, 4</p>"},{"location":"notebooks/field_series/ROADMAP/#notebook-6-rf-sarsa-complete-learning-algorithm-planned","title":"Notebook 6: RF-SARSA \u2014 Complete Learning Algorithm (Planned)","text":"<p>Goal: Implement and understand the full GRL learning algorithm</p> <p>Topics to Cover:</p> <ol> <li>SARSA Recap</li> <li>Classical SARSA: \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma Q(s',a') - Q(s,a)]\\)</li> <li> <p>TD error and bootstrapping</p> </li> <li> <p>RF-SARSA Adaptation</p> </li> <li>No explicit Q-table \u2014 field represents Q-function</li> <li>TD error in RKHS: \\(\\delta = r + \\gamma Q^+(s', a') - Q^+(s, a)\\)</li> <li> <p>Particle weight from TD error: \\(w_{new} = \\alpha \\delta\\)</p> </li> <li> <p>Algorithm Walkthrough</p> </li> <li>Initialize: empty particle memory</li> <li>Episode loop:<ul> <li>Select action via policy (Boltzmann or greedy)</li> <li>Execute, observe \\((s', r)\\)</li> <li>Compute TD error</li> <li>Add particle if \\(|\\delta| &gt; \\epsilon\\)</li> </ul> </li> <li> <p>Field emerges from accumulated particles</p> </li> <li> <p>Convergence and Stability</p> </li> <li>When does the field stabilize?</li> <li>Memory growth over time</li> <li>Particle pruning strategies</li> </ol> <p>Visualizations: - Episode-by-episode field evolution (animated) - TD error over time - Number of particles vs. episodes - Final learned policy vs. optimal policy</p> <p>Code Examples: <pre><code>def rf_sarsa_episode(env, particles, alpha, gamma, beta):\n    \"\"\"Run one episode of RF-SARSA.\"\"\"\n    s = env.reset()\n    a = sample_boltzmann_policy(s, particles, beta)\n\n    for t in range(max_steps):\n        s_next, r, done = env.step(a)\n        a_next = sample_boltzmann_policy(s_next, particles, beta)\n\n        # TD error\n        Q_sa = compute_Q_plus(s, a, particles)\n        Q_next = compute_Q_plus(s_next, a_next, particles)\n        delta = r + gamma * Q_next - Q_sa\n\n        # Add particle if significant\n        if abs(delta) &gt; epsilon:\n            z_new = (s, a)\n            w_new = alpha * delta\n            particles.append({'z': z_new, 'w': w_new})\n\n        s, a = s_next, a_next\n        if done: break\n\n    return particles\n</code></pre></p> <p>Experiments: - 2D navigation (from Notebook 3) - Gridworld - Mountain car (continuous actions)</p> <p>Time Estimate: ~40-45 minutes</p> <p>Prerequisites: Notebooks 3, 4, 5</p>"},{"location":"notebooks/field_series/ROADMAP/#future-topics-beyond-core-series","title":"\ud83d\udd2e Future Topics (Beyond Core Series)","text":""},{"location":"notebooks/field_series/ROADMAP/#advanced-topics-potential-notebooks-7","title":"Advanced Topics (Potential Notebooks 7+)","text":"<ol> <li>Kernel Design and Selection</li> <li>RBF vs. other kernels</li> <li>Adaptive lengthscales</li> <li> <p>State-action factorization</p> </li> <li> <p>Scalability and Approximations</p> </li> <li>Particle pruning</li> <li>Sparse approximations</li> <li> <p>Nystr\u00f6m methods</p> </li> <li> <p>Multi-Task and Transfer Learning</p> </li> <li>Shared particle memories</li> <li>Task-specific fields</li> <li> <p>Meta-learning</p> </li> <li> <p>Theoretical Foundations</p> </li> <li>Convergence proofs</li> <li>Sample complexity</li> <li> <p>Relationship to kernel-based RL</p> </li> <li> <p>Comparison with Other Methods</p> </li> <li>GRL vs. DQN</li> <li>GRL vs. SAC</li> <li>GRL vs. PPO</li> <li>When to use GRL?</li> </ol>"},{"location":"notebooks/field_series/ROADMAP/#development-principles","title":"Development Principles","text":"<p>Systematic Progression: 1. \u2705 Build intuition (Notebooks 0-3) 2. \ud83d\udea7 Understand components (Notebooks 4-5) 3. \ud83d\udd2e Implement algorithms (Notebook 6) 4. \ud83d\udd2e Explore advanced topics (Notebooks 7+)</p> <p>Each Notebook Should: - Build on previous concepts - Include professional visualizations - Provide working code examples - Connect theory to practice - Take 20-45 minutes to complete</p> <p>Pedagogical Goals: - Visual &gt; Mathematical (when possible) - Interactive &gt; Static (when useful) - Synthetic &gt; Real (for clarity, then real for validation) - Incremental &gt; Comprehensive (build up systematically)</p>"},{"location":"notebooks/field_series/ROADMAP/#timeline-and-priorities","title":"Timeline and Priorities","text":""},{"location":"notebooks/field_series/ROADMAP/#high-priority-core-understanding","title":"High Priority (Core Understanding)","text":"<ul> <li> Notebook 4: Policy Inference</li> <li> Notebook 5: Memory Update</li> <li> Notebook 6: RF-SARSA</li> </ul>"},{"location":"notebooks/field_series/ROADMAP/#medium-priority-practical-application","title":"Medium Priority (Practical Application)","text":"<ul> <li> Integration with real RL environments</li> <li> Performance benchmarks</li> <li> Hyperparameter tuning guide</li> </ul>"},{"location":"notebooks/field_series/ROADMAP/#low-priority-advanced-topics","title":"Low Priority (Advanced Topics)","text":"<ul> <li> Theoretical deep dives</li> <li> Comparison studies</li> <li> Extensions and variants</li> </ul>"},{"location":"notebooks/field_series/ROADMAP/#related-resources","title":"Related Resources","text":"<p>Within GRL Project: - Tutorial series: <code>docs/GRL0/tutorials/</code> - Theory documents: <code>docs/theory/</code> - Implementation: <code>src/</code> (when available)</p> <p>External Projects: - genai-lab \u2014 Flow matching, diffusion models - Original GRL paper: arXiv:2208.04822</p> <p>Last Updated: January 15, 2026</p> <p>Status: Foundation complete (Notebooks 0-3), planning next phase (Notebooks 4-6)</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/","title":"Notebook 3: Reinforcement Fields in GRL","text":"<p>This directory contains the main notebook and supplementary materials for understanding GRL's reinforcement fields.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/#contents","title":"Contents","text":"File Description <code>03_reinforcement_fields.ipynb</code> Main notebook: 2D navigation domain, particles, Q\u207a field, policy inference <code>03a_particle_coverage_effects.ipynb</code> Supplementary: Visual proof of how particle coverage affects policy field behavior <code>particle_vs_gradient_fields.md</code> Theory note: Detailed comparison of particle-based Q\u207a vs true gradient fields"},{"location":"notebooks/field_series/03_reinforcement_fields/#key-question-addressed","title":"Key Question Addressed","text":"<p>Why do policy arrows appear parallel instead of converging on the goal?</p> <p>The supplementary notebook (<code>03a</code>) provides visual proof that:</p> <ul> <li>Sparse particles \u2192 parallel arrows (limited directional info)</li> <li>Rich particles \u2192 converging arrows (diverse directional info)</li> <li>True gradient \u2192 perfect convergence (geometric, not learned)</li> </ul>"},{"location":"notebooks/field_series/03_reinforcement_fields/#related","title":"Related","text":"<ul> <li>Field series overview: <code>../README.md</code></li> <li>Theory docs: <code>../../../docs/theory/particle_vs_gradient_fields.md</code> (canonical location)</li> </ul>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/","title":"03: Reinforcement Fields","text":"In\u00a0[1]: Copied! <pre># Setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle, Circle, FancyArrowPatch\nfrom matplotlib.collections import PatchCollection\nimport seaborn as sns\n\nINTERACTIVE = False\ntry:\n    import ipywidgets as widgets\n    from IPython.display import display\n    WIDGETS_AVAILABLE = True\nexcept ImportError:\n    WIDGETS_AVAILABLE = False\n\nsns.set_theme(style='whitegrid', context='notebook')\nplt.rcParams['figure.figsize'] = (12, 8)\n%matplotlib inline\nprint(f\"Libraries loaded. Interactive: {INTERACTIVE and WIDGETS_AVAILABLE}\")\n</pre> # Setup import numpy as np import matplotlib.pyplot as plt from matplotlib.patches import Rectangle, Circle, FancyArrowPatch from matplotlib.collections import PatchCollection import seaborn as sns  INTERACTIVE = False try:     import ipywidgets as widgets     from IPython.display import display     WIDGETS_AVAILABLE = True except ImportError:     WIDGETS_AVAILABLE = False  sns.set_theme(style='whitegrid', context='notebook') plt.rcParams['figure.figsize'] = (12, 8) %matplotlib inline print(f\"Libraries loaded. Interactive: {INTERACTIVE and WIDGETS_AVAILABLE}\") <pre>Libraries loaded. Interactive: False\n</pre> In\u00a0[2]: Copied! <pre># Helper functions for the navigation domain\n\ndef create_domain(xlim=(-1, 5), ylim=(-1, 5), goal=(4, 4), start=(0, 0), obstacles=None):\n    \"\"\"Create a navigation domain configuration.\"\"\"\n    return {\n        'xlim': xlim, 'ylim': ylim,\n        'goal': np.array(goal),\n        'start': np.array(start),\n        'obstacles': obstacles or []\n    }\n\ndef plot_domain(ax, domain, title='Navigation Domain'):\n    \"\"\"Plot the navigation domain with goal and obstacles.\"\"\"\n    ax.set_xlim(domain['xlim'])\n    ax.set_ylim(domain['ylim'])\n    \n    # Goal (green triangle)\n    ax.plot(*domain['goal'], 'g^', markersize=20, label='Goal', zorder=10)\n    \n    # Start\n    ax.plot(*domain['start'], 'ko', markersize=12, label='Start', zorder=10)\n    \n    # Obstacles\n    for obs in domain['obstacles']:\n        rect = Rectangle((obs['x'], obs['y']), obs['w'], obs['h'], \n                         facecolor='gray', edgecolor='black', alpha=0.7)\n        ax.add_patch(rect)\n    \n    ax.set_xlabel('$x$'); ax.set_ylabel('$y$')\n    ax.set_title(title)\n    ax.set_aspect('equal')\n    ax.legend(loc='upper left')\n    ax.grid(True, alpha=0.3)\n\n# Create two domains: without and with obstacles\ndomain_simple = create_domain()\ndomain_obstacles = create_domain(obstacles=[\n    {'x': 1.5, 'y': 1.5, 'w': 1.5, 'h': 1.5},  # Central obstacle\n])\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nplot_domain(axes[0], domain_simple, 'Simple Domain (No Obstacles)')\nplot_domain(axes[1], domain_obstacles, 'Domain with Obstacle')\nplt.tight_layout(); plt.show()\n</pre> # Helper functions for the navigation domain  def create_domain(xlim=(-1, 5), ylim=(-1, 5), goal=(4, 4), start=(0, 0), obstacles=None):     \"\"\"Create a navigation domain configuration.\"\"\"     return {         'xlim': xlim, 'ylim': ylim,         'goal': np.array(goal),         'start': np.array(start),         'obstacles': obstacles or []     }  def plot_domain(ax, domain, title='Navigation Domain'):     \"\"\"Plot the navigation domain with goal and obstacles.\"\"\"     ax.set_xlim(domain['xlim'])     ax.set_ylim(domain['ylim'])          # Goal (green triangle)     ax.plot(*domain['goal'], 'g^', markersize=20, label='Goal', zorder=10)          # Start     ax.plot(*domain['start'], 'ko', markersize=12, label='Start', zorder=10)          # Obstacles     for obs in domain['obstacles']:         rect = Rectangle((obs['x'], obs['y']), obs['w'], obs['h'],                           facecolor='gray', edgecolor='black', alpha=0.7)         ax.add_patch(rect)          ax.set_xlabel('$x$'); ax.set_ylabel('$y$')     ax.set_title(title)     ax.set_aspect('equal')     ax.legend(loc='upper left')     ax.grid(True, alpha=0.3)  # Create two domains: without and with obstacles domain_simple = create_domain() domain_obstacles = create_domain(obstacles=[     {'x': 1.5, 'y': 1.5, 'w': 1.5, 'h': 1.5},  # Central obstacle ])  fig, axes = plt.subplots(1, 2, figsize=(12, 5)) plot_domain(axes[0], domain_simple, 'Simple Domain (No Obstacles)') plot_domain(axes[1], domain_obstacles, 'Domain with Obstacle') plt.tight_layout(); plt.show() In\u00a0[3]: Copied! <pre># Example 2.1: Synthetic Particles for Simple Domain\n# These represent \"what a trained agent might have learned\"\n\ndef create_guiding_particles(goal, n_positive=8, n_negative=4):\n    \"\"\"Create particles that guide toward the goal.\"\"\"\n    particles = []\n    \n    # Positive particles: along path to goal\n    # Each particle encodes: \"at this position, move toward goal\"\n    path_points = [\n        (0.5, 0.5), (1.0, 1.0), (1.5, 1.5), (2.0, 2.0),\n        (2.5, 2.5), (3.0, 3.0), (3.5, 3.5), (4.0, 4.0)\n    ]\n    for i, (px, py) in enumerate(path_points):\n        # Action direction: toward goal\n        dx, dy = goal[0] - px, goal[1] - py\n        norm = np.sqrt(dx**2 + dy**2) + 1e-6\n        particles.append({\n            'x': px, 'y': py,           # State\n            'vx': dx/norm, 'vy': dy/norm,  # Action (normalized direction)\n            'w': 1.5 + 0.5 * (i / len(path_points)),  # Weight (stronger near goal)\n            'type': 'positive'\n        })\n    \n    # Negative particles: bad directions (e.g., going backward)\n    bad_actions = [\n        (1.0, 1.0, -0.7, -0.7),  # At (1,1), going backward is bad\n        (2.0, 2.0, -0.7, -0.7),  # At (2,2), going backward is bad\n        (0.5, 2.0, 0.0, -1.0),   # At (0.5,2), going down is bad\n        (2.0, 0.5, -1.0, 0.0),   # At (2,0.5), going left is bad\n    ]\n    for px, py, vx, vy in bad_actions:\n        particles.append({\n            'x': px, 'y': py, 'vx': vx, 'vy': vy,\n            'w': -1.0, 'type': 'negative'\n        })\n    \n    return particles\n\nparticles_simple = create_guiding_particles(domain_simple['goal'])\n\n# Visualize particles\nfig, ax = plt.subplots(figsize=(10, 10))\nplot_domain(ax, domain_simple, 'Particles in Simple Domain')\n\nfor p in particles_simple:\n    color = 'blue' if p['type'] == 'positive' else 'red'\n    # Plot particle position\n    ax.plot(p['x'], p['y'], 'o', color=color, markersize=10, mec='k', mew=1.5)\n    # Plot action direction as arrow\n    ax.arrow(p['x'], p['y'], p['vx']*0.3, p['vy']*0.3, \n             head_width=0.1, head_length=0.05, fc=color, ec=color, alpha=0.7)\n\nax.plot([], [], 'bo', markersize=10, label='Positive (good action)')\nax.plot([], [], 'ro', markersize=10, label='Negative (bad action)')\nax.legend(loc='upper left')\nplt.show()\n\nprint(\"Blue particles: 'At this state, this action direction is good'\")\nprint(\"Red particles: 'At this state, this action direction is bad'\")\n</pre> # Example 2.1: Synthetic Particles for Simple Domain # These represent \"what a trained agent might have learned\"  def create_guiding_particles(goal, n_positive=8, n_negative=4):     \"\"\"Create particles that guide toward the goal.\"\"\"     particles = []          # Positive particles: along path to goal     # Each particle encodes: \"at this position, move toward goal\"     path_points = [         (0.5, 0.5), (1.0, 1.0), (1.5, 1.5), (2.0, 2.0),         (2.5, 2.5), (3.0, 3.0), (3.5, 3.5), (4.0, 4.0)     ]     for i, (px, py) in enumerate(path_points):         # Action direction: toward goal         dx, dy = goal[0] - px, goal[1] - py         norm = np.sqrt(dx**2 + dy**2) + 1e-6         particles.append({             'x': px, 'y': py,           # State             'vx': dx/norm, 'vy': dy/norm,  # Action (normalized direction)             'w': 1.5 + 0.5 * (i / len(path_points)),  # Weight (stronger near goal)             'type': 'positive'         })          # Negative particles: bad directions (e.g., going backward)     bad_actions = [         (1.0, 1.0, -0.7, -0.7),  # At (1,1), going backward is bad         (2.0, 2.0, -0.7, -0.7),  # At (2,2), going backward is bad         (0.5, 2.0, 0.0, -1.0),   # At (0.5,2), going down is bad         (2.0, 0.5, -1.0, 0.0),   # At (2,0.5), going left is bad     ]     for px, py, vx, vy in bad_actions:         particles.append({             'x': px, 'y': py, 'vx': vx, 'vy': vy,             'w': -1.0, 'type': 'negative'         })          return particles  particles_simple = create_guiding_particles(domain_simple['goal'])  # Visualize particles fig, ax = plt.subplots(figsize=(10, 10)) plot_domain(ax, domain_simple, 'Particles in Simple Domain')  for p in particles_simple:     color = 'blue' if p['type'] == 'positive' else 'red'     # Plot particle position     ax.plot(p['x'], p['y'], 'o', color=color, markersize=10, mec='k', mew=1.5)     # Plot action direction as arrow     ax.arrow(p['x'], p['y'], p['vx']*0.3, p['vy']*0.3,               head_width=0.1, head_length=0.05, fc=color, ec=color, alpha=0.7)  ax.plot([], [], 'bo', markersize=10, label='Positive (good action)') ax.plot([], [], 'ro', markersize=10, label='Negative (bad action)') ax.legend(loc='upper left') plt.show()  print(\"Blue particles: 'At this state, this action direction is good'\") print(\"Red particles: 'At this state, this action direction is bad'\") <pre>Blue particles: 'At this state, this action direction is good'\nRed particles: 'At this state, this action direction is bad'\n</pre> In\u00a0[4]: Copied! <pre># Build the reinforcement field from particles\n\ndef rbf_kernel_4d(z, z_prime, lengthscale=0.8):\n    \"\"\"RBF kernel in 4D augmented space.\"\"\"\n    return np.exp(-np.sum((z - z_prime)**2) / (2 * lengthscale**2))\n\ndef compute_Q_plus(x, y, vx, vy, particles, lengthscale=0.8):\n    \"\"\"Compute Q\u207a(x, y, vx, vy) from particles.\"\"\"\n    z = np.array([x, y, vx, vy])\n    Q = 0.0\n    for p in particles:\n        z_i = np.array([p['x'], p['y'], p['vx'], p['vy']])\n        Q += p['w'] * rbf_kernel_4d(z, z_i, lengthscale)\n    return Q\n\ndef compute_Q_field_fixed_action(X, Y, vx_fixed, vy_fixed, particles, lengthscale=0.8):\n    \"\"\"Compute Q\u207a field for fixed action direction.\"\"\"\n    Q = np.zeros_like(X)\n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            Q[i, j] = compute_Q_plus(X[i,j], Y[i,j], vx_fixed, vy_fixed, particles, lengthscale)\n    return Q\n\n# Create grid for state space\nx = np.linspace(-0.5, 4.5, 40)\ny = np.linspace(-0.5, 4.5, 40)\nX, Y = np.meshgrid(x, y)\n\n# Compute Q\u207a for action \"toward goal\" (diagonal direction)\nvx_goal, vy_goal = 0.707, 0.707  # Normalized (1,1) direction\nQ_toward_goal = compute_Q_field_fixed_action(X, Y, vx_goal, vy_goal, particles_simple)\n\n# Compute Q\u207a for action \"away from goal\" (opposite direction)\nQ_away_goal = compute_Q_field_fixed_action(X, Y, -vx_goal, -vy_goal, particles_simple)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Q\u207a for \"toward goal\" action\nax1 = axes[0]\nc1 = ax1.contourf(X, Y, Q_toward_goal, levels=25, cmap='RdBu_r', alpha=0.8)\nax1.plot(*domain_simple['goal'], 'g^', markersize=15, zorder=10)\nax1.plot(*domain_simple['start'], 'ko', markersize=10, zorder=10)\nfor p in particles_simple:\n    if p['type'] == 'positive':\n        ax1.plot(p['x'], p['y'], 'bo', markersize=6, alpha=0.5)\nax1.set_title(r\"$Q^+(x, y, v_{\\rightarrow goal})$\" + \"\\n(Value of moving toward goal)\")\nax1.set_xlabel('$x$'); ax1.set_ylabel('$y$'); ax1.set_aspect('equal')\nplt.colorbar(c1, ax=ax1, label='$Q^+$')\n\n# Q\u207a for \"away from goal\" action\nax2 = axes[1]\nc2 = ax2.contourf(X, Y, Q_away_goal, levels=25, cmap='RdBu_r', alpha=0.8)\nax2.plot(*domain_simple['goal'], 'g^', markersize=15, zorder=10)\nax2.plot(*domain_simple['start'], 'ko', markersize=10, zorder=10)\nfor p in particles_simple:\n    if p['type'] == 'negative':\n        ax2.plot(p['x'], p['y'], 'rs', markersize=6, alpha=0.5)\nax2.set_title(r\"$Q^+(x, y, v_{\\leftarrow goal})$\" + \"\\n(Value of moving away from goal)\")\nax2.set_xlabel('$x$'); ax2.set_ylabel('$y$'); ax2.set_aspect('equal')\nplt.colorbar(c2, ax=ax2, label='$Q^+$')\n\nplt.tight_layout(); plt.show()\nprint(\"Left: High Q\u207a along the path when action points toward goal.\")\nprint(\"Right: Low/negative Q\u207a when action points away from goal.\")\n</pre> # Build the reinforcement field from particles  def rbf_kernel_4d(z, z_prime, lengthscale=0.8):     \"\"\"RBF kernel in 4D augmented space.\"\"\"     return np.exp(-np.sum((z - z_prime)**2) / (2 * lengthscale**2))  def compute_Q_plus(x, y, vx, vy, particles, lengthscale=0.8):     \"\"\"Compute Q\u207a(x, y, vx, vy) from particles.\"\"\"     z = np.array([x, y, vx, vy])     Q = 0.0     for p in particles:         z_i = np.array([p['x'], p['y'], p['vx'], p['vy']])         Q += p['w'] * rbf_kernel_4d(z, z_i, lengthscale)     return Q  def compute_Q_field_fixed_action(X, Y, vx_fixed, vy_fixed, particles, lengthscale=0.8):     \"\"\"Compute Q\u207a field for fixed action direction.\"\"\"     Q = np.zeros_like(X)     for i in range(X.shape[0]):         for j in range(X.shape[1]):             Q[i, j] = compute_Q_plus(X[i,j], Y[i,j], vx_fixed, vy_fixed, particles, lengthscale)     return Q  # Create grid for state space x = np.linspace(-0.5, 4.5, 40) y = np.linspace(-0.5, 4.5, 40) X, Y = np.meshgrid(x, y)  # Compute Q\u207a for action \"toward goal\" (diagonal direction) vx_goal, vy_goal = 0.707, 0.707  # Normalized (1,1) direction Q_toward_goal = compute_Q_field_fixed_action(X, Y, vx_goal, vy_goal, particles_simple)  # Compute Q\u207a for action \"away from goal\" (opposite direction) Q_away_goal = compute_Q_field_fixed_action(X, Y, -vx_goal, -vy_goal, particles_simple)  fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # Q\u207a for \"toward goal\" action ax1 = axes[0] c1 = ax1.contourf(X, Y, Q_toward_goal, levels=25, cmap='RdBu_r', alpha=0.8) ax1.plot(*domain_simple['goal'], 'g^', markersize=15, zorder=10) ax1.plot(*domain_simple['start'], 'ko', markersize=10, zorder=10) for p in particles_simple:     if p['type'] == 'positive':         ax1.plot(p['x'], p['y'], 'bo', markersize=6, alpha=0.5) ax1.set_title(r\"$Q^+(x, y, v_{\\rightarrow goal})$\" + \"\\n(Value of moving toward goal)\") ax1.set_xlabel('$x$'); ax1.set_ylabel('$y$'); ax1.set_aspect('equal') plt.colorbar(c1, ax=ax1, label='$Q^+$')  # Q\u207a for \"away from goal\" action ax2 = axes[1] c2 = ax2.contourf(X, Y, Q_away_goal, levels=25, cmap='RdBu_r', alpha=0.8) ax2.plot(*domain_simple['goal'], 'g^', markersize=15, zorder=10) ax2.plot(*domain_simple['start'], 'ko', markersize=10, zorder=10) for p in particles_simple:     if p['type'] == 'negative':         ax2.plot(p['x'], p['y'], 'rs', markersize=6, alpha=0.5) ax2.set_title(r\"$Q^+(x, y, v_{\\leftarrow goal})$\" + \"\\n(Value of moving away from goal)\") ax2.set_xlabel('$x$'); ax2.set_ylabel('$y$'); ax2.set_aspect('equal') plt.colorbar(c2, ax=ax2, label='$Q^+$')  plt.tight_layout(); plt.show() print(\"Left: High Q\u207a along the path when action points toward goal.\") print(\"Right: Low/negative Q\u207a when action points away from goal.\") <pre>Left: High Q\u207a along the path when action points toward goal.\nRight: Low/negative Q\u207a when action points away from goal.\n</pre> In\u00a0[5]: Copied! <pre># Example 4.1: Action Landscape at a Fixed State\n\ndef compute_action_landscape(x_fixed, y_fixed, particles, n_angles=36, lengthscale=0.8):\n    \"\"\"Compute Q\u207a for all action directions at a fixed state.\"\"\"\n    angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)\n    Q_values = []\n    for theta in angles:\n        vx, vy = np.cos(theta), np.sin(theta)\n        Q = compute_Q_plus(x_fixed, y_fixed, vx, vy, particles, lengthscale)\n        Q_values.append(Q)\n    return angles, np.array(Q_values)\n\n# Evaluate at different states\nstates_to_eval = [(0.5, 0.5), (2.0, 2.0), (1.0, 3.0)]\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4.5), subplot_kw={'projection': 'polar'})\n\nfor ax, (sx, sy) in zip(axes, states_to_eval):\n    angles, Q_vals = compute_action_landscape(sx, sy, particles_simple)\n    \n    # Normalize for visualization\n    Q_shifted = Q_vals - Q_vals.min() + 0.1\n    \n    ax.plot(angles, Q_shifted, 'b-', lw=2)\n    ax.fill(angles, Q_shifted, alpha=0.3)\n    \n    # Mark best action\n    best_idx = np.argmax(Q_vals)\n    ax.plot(angles[best_idx], Q_shifted[best_idx], 'r*', markersize=15)\n    \n    # Direction to goal\n    goal_angle = np.arctan2(4-sy, 4-sx)\n    ax.axvline(goal_angle, color='green', linestyle='--', alpha=0.7, label='To goal')\n    \n    ax.set_title(f'State ({sx}, {sy})')\n\nplt.suptitle(r'Action Landscape: $Q^+(s, \\theta)$ for different action angles', y=1.02)\nplt.tight_layout(); plt.show()\nprint(\"Red star = best action. Green dashed = direction to goal.\")\nprint(\"The field naturally guides the agent toward the goal!\")\n</pre> # Example 4.1: Action Landscape at a Fixed State  def compute_action_landscape(x_fixed, y_fixed, particles, n_angles=36, lengthscale=0.8):     \"\"\"Compute Q\u207a for all action directions at a fixed state.\"\"\"     angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)     Q_values = []     for theta in angles:         vx, vy = np.cos(theta), np.sin(theta)         Q = compute_Q_plus(x_fixed, y_fixed, vx, vy, particles, lengthscale)         Q_values.append(Q)     return angles, np.array(Q_values)  # Evaluate at different states states_to_eval = [(0.5, 0.5), (2.0, 2.0), (1.0, 3.0)]  fig, axes = plt.subplots(1, 3, figsize=(15, 4.5), subplot_kw={'projection': 'polar'})  for ax, (sx, sy) in zip(axes, states_to_eval):     angles, Q_vals = compute_action_landscape(sx, sy, particles_simple)          # Normalize for visualization     Q_shifted = Q_vals - Q_vals.min() + 0.1          ax.plot(angles, Q_shifted, 'b-', lw=2)     ax.fill(angles, Q_shifted, alpha=0.3)          # Mark best action     best_idx = np.argmax(Q_vals)     ax.plot(angles[best_idx], Q_shifted[best_idx], 'r*', markersize=15)          # Direction to goal     goal_angle = np.arctan2(4-sy, 4-sx)     ax.axvline(goal_angle, color='green', linestyle='--', alpha=0.7, label='To goal')          ax.set_title(f'State ({sx}, {sy})')  plt.suptitle(r'Action Landscape: $Q^+(s, \\theta)$ for different action angles', y=1.02) plt.tight_layout(); plt.show() print(\"Red star = best action. Green dashed = direction to goal.\") print(\"The field naturally guides the agent toward the goal!\") <pre>Red star = best action. Green dashed = direction to goal.\nThe field naturally guides the agent toward the goal!\n</pre> In\u00a0[6]: Copied! <pre># Example 4.2: Inferred Policy as Vector Field\n\ndef infer_policy(X, Y, particles, n_angles=16, lengthscale=0.8):\n    \"\"\"Infer greedy policy at each state.\"\"\"\n    U = np.zeros_like(X)\n    V = np.zeros_like(Y)\n    \n    angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)\n    \n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            x, y = X[i, j], Y[i, j]\n            best_Q = -np.inf\n            best_vx, best_vy = 0, 0\n            \n            for theta in angles:\n                vx, vy = np.cos(theta), np.sin(theta)\n                Q = compute_Q_plus(x, y, vx, vy, particles, lengthscale)\n                if Q &gt; best_Q:\n                    best_Q = Q\n                    best_vx, best_vy = vx, vy\n            \n            U[i, j] = best_vx\n            V[i, j] = best_vy\n    \n    return U, V\n\n# Coarser grid for policy visualization\nx_policy = np.linspace(0, 4, 12)\ny_policy = np.linspace(0, 4, 12)\nX_p, Y_p = np.meshgrid(x_policy, y_policy)\n\nU_policy, V_policy = infer_policy(X_p, Y_p, particles_simple)\n\nfig, ax = plt.subplots(figsize=(10, 10))\n\n# Background: Q\u207a for \"toward goal\" action\nc = ax.contourf(X, Y, Q_toward_goal, levels=20, cmap='RdBu_r', alpha=0.4)\n\n# Policy arrows\nax.quiver(X_p, Y_p, U_policy, V_policy, color='black', scale=20, width=0.005)\n\n# Domain elements\nax.plot(*domain_simple['goal'], 'g^', markersize=20, label='Goal', zorder=10)\nax.plot(*domain_simple['start'], 'ko', markersize=15, label='Start', zorder=10)\n\n# Particles\nfor p in particles_simple:\n    color = 'blue' if p['type'] == 'positive' else 'red'\n    ax.plot(p['x'], p['y'], 'o', color=color, markersize=8, alpha=0.6)\n\nax.set_xlabel('$x$'); ax.set_ylabel('$y$')\nax.set_title(r'Inferred Policy: $\\pi(s) = \\arg\\max_\\theta Q^+(s, \\theta)$')\nax.set_xlim(-0.5, 4.5); ax.set_ylim(-0.5, 4.5)\nax.set_aspect('equal'); ax.legend(loc='upper left')\nplt.colorbar(c, ax=ax, label='$Q^+$ (toward goal)', shrink=0.8)\nplt.show()\n\nprint(\"The policy emerges from reading the field \u2014 no explicit policy network needed!\")\n</pre> # Example 4.2: Inferred Policy as Vector Field  def infer_policy(X, Y, particles, n_angles=16, lengthscale=0.8):     \"\"\"Infer greedy policy at each state.\"\"\"     U = np.zeros_like(X)     V = np.zeros_like(Y)          angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)          for i in range(X.shape[0]):         for j in range(X.shape[1]):             x, y = X[i, j], Y[i, j]             best_Q = -np.inf             best_vx, best_vy = 0, 0                          for theta in angles:                 vx, vy = np.cos(theta), np.sin(theta)                 Q = compute_Q_plus(x, y, vx, vy, particles, lengthscale)                 if Q &gt; best_Q:                     best_Q = Q                     best_vx, best_vy = vx, vy                          U[i, j] = best_vx             V[i, j] = best_vy          return U, V  # Coarser grid for policy visualization x_policy = np.linspace(0, 4, 12) y_policy = np.linspace(0, 4, 12) X_p, Y_p = np.meshgrid(x_policy, y_policy)  U_policy, V_policy = infer_policy(X_p, Y_p, particles_simple)  fig, ax = plt.subplots(figsize=(10, 10))  # Background: Q\u207a for \"toward goal\" action c = ax.contourf(X, Y, Q_toward_goal, levels=20, cmap='RdBu_r', alpha=0.4)  # Policy arrows ax.quiver(X_p, Y_p, U_policy, V_policy, color='black', scale=20, width=0.005)  # Domain elements ax.plot(*domain_simple['goal'], 'g^', markersize=20, label='Goal', zorder=10) ax.plot(*domain_simple['start'], 'ko', markersize=15, label='Start', zorder=10)  # Particles for p in particles_simple:     color = 'blue' if p['type'] == 'positive' else 'red'     ax.plot(p['x'], p['y'], 'o', color=color, markersize=8, alpha=0.6)  ax.set_xlabel('$x$'); ax.set_ylabel('$y$') ax.set_title(r'Inferred Policy: $\\pi(s) = \\arg\\max_\\theta Q^+(s, \\theta)$') ax.set_xlim(-0.5, 4.5); ax.set_ylim(-0.5, 4.5) ax.set_aspect('equal'); ax.legend(loc='upper left') plt.colorbar(c, ax=ax, label='$Q^+$ (toward goal)', shrink=0.8) plt.show()  print(\"The policy emerges from reading the field \u2014 no explicit policy network needed!\") <pre>The policy emerges from reading the field \u2014 no explicit policy network needed!\n</pre> In\u00a0[7]: Copied! <pre># Example 5.1: Particles for Domain with Obstacle\n\ndef create_obstacle_particles(goal, obstacle):\n    \"\"\"Create particles that guide around an obstacle.\"\"\"\n    particles = []\n    \n    # Positive particles: path that goes AROUND the obstacle\n    # Path 1: Go above the obstacle\n    path_above = [\n        (0.5, 0.5), (1.0, 1.0), (1.2, 1.8), (1.5, 3.2), (2.0, 3.5),\n        (2.5, 3.5), (3.0, 3.5), (3.5, 3.8), (4.0, 4.0)\n    ]\n    for i, (px, py) in enumerate(path_above):\n        # Direction toward next point (or goal)\n        if i &lt; len(path_above) - 1:\n            dx = path_above[i+1][0] - px\n            dy = path_above[i+1][1] - py\n        else:\n            dx, dy = goal[0] - px, goal[1] - py\n        norm = np.sqrt(dx**2 + dy**2) + 1e-6\n        particles.append({\n            'x': px, 'y': py, 'vx': dx/norm, 'vy': dy/norm,\n            'w': 1.5, 'type': 'positive'\n        })\n    \n    # Negative particles: near obstacle edges (\"don't go into obstacle\")\n    ox, oy, ow, oh = obstacle['x'], obstacle['y'], obstacle['w'], obstacle['h']\n    obstacle_boundary = [\n        # Left edge: don't go right into obstacle\n        (ox - 0.2, oy + oh/2, 1.0, 0.0),\n        (ox - 0.2, oy + oh/4, 1.0, 0.0),\n        (ox - 0.2, oy + 3*oh/4, 1.0, 0.0),\n        # Bottom edge: don't go up into obstacle\n        (ox + ow/2, oy - 0.2, 0.0, 1.0),\n        (ox + ow/4, oy - 0.2, 0.0, 1.0),\n        # Corner: don't go diagonally into obstacle\n        (ox - 0.1, oy - 0.1, 0.7, 0.7),\n    ]\n    for px, py, vx, vy in obstacle_boundary:\n        particles.append({\n            'x': px, 'y': py, 'vx': vx, 'vy': vy,\n            'w': -1.5, 'type': 'negative'\n        })\n    \n    return particles\n\nobstacle = domain_obstacles['obstacles'][0]\nparticles_obstacle = create_obstacle_particles(domain_obstacles['goal'], obstacle)\n\n# Visualize\nfig, ax = plt.subplots(figsize=(10, 10))\nplot_domain(ax, domain_obstacles, 'Particles with Obstacle')\n\nfor p in particles_obstacle:\n    color = 'blue' if p['type'] == 'positive' else 'red'\n    ax.plot(p['x'], p['y'], 'o', color=color, markersize=10, mec='k', mew=1.5)\n    ax.arrow(p['x'], p['y'], p['vx']*0.25, p['vy']*0.25,\n             head_width=0.08, head_length=0.04, fc=color, ec=color, alpha=0.7)\n\nax.plot([], [], 'bo', markersize=10, label='Positive (go this way)')\nax.plot([], [], 'ro', markersize=10, label='Negative (avoid this)')\nax.legend(loc='upper left')\nplt.show()\n\nprint(\"Blue particles guide AROUND the obstacle.\")\nprint(\"Red particles warn: 'Don't go into the obstacle!'\")\n</pre> # Example 5.1: Particles for Domain with Obstacle  def create_obstacle_particles(goal, obstacle):     \"\"\"Create particles that guide around an obstacle.\"\"\"     particles = []          # Positive particles: path that goes AROUND the obstacle     # Path 1: Go above the obstacle     path_above = [         (0.5, 0.5), (1.0, 1.0), (1.2, 1.8), (1.5, 3.2), (2.0, 3.5),         (2.5, 3.5), (3.0, 3.5), (3.5, 3.8), (4.0, 4.0)     ]     for i, (px, py) in enumerate(path_above):         # Direction toward next point (or goal)         if i &lt; len(path_above) - 1:             dx = path_above[i+1][0] - px             dy = path_above[i+1][1] - py         else:             dx, dy = goal[0] - px, goal[1] - py         norm = np.sqrt(dx**2 + dy**2) + 1e-6         particles.append({             'x': px, 'y': py, 'vx': dx/norm, 'vy': dy/norm,             'w': 1.5, 'type': 'positive'         })          # Negative particles: near obstacle edges (\"don't go into obstacle\")     ox, oy, ow, oh = obstacle['x'], obstacle['y'], obstacle['w'], obstacle['h']     obstacle_boundary = [         # Left edge: don't go right into obstacle         (ox - 0.2, oy + oh/2, 1.0, 0.0),         (ox - 0.2, oy + oh/4, 1.0, 0.0),         (ox - 0.2, oy + 3*oh/4, 1.0, 0.0),         # Bottom edge: don't go up into obstacle         (ox + ow/2, oy - 0.2, 0.0, 1.0),         (ox + ow/4, oy - 0.2, 0.0, 1.0),         # Corner: don't go diagonally into obstacle         (ox - 0.1, oy - 0.1, 0.7, 0.7),     ]     for px, py, vx, vy in obstacle_boundary:         particles.append({             'x': px, 'y': py, 'vx': vx, 'vy': vy,             'w': -1.5, 'type': 'negative'         })          return particles  obstacle = domain_obstacles['obstacles'][0] particles_obstacle = create_obstacle_particles(domain_obstacles['goal'], obstacle)  # Visualize fig, ax = plt.subplots(figsize=(10, 10)) plot_domain(ax, domain_obstacles, 'Particles with Obstacle')  for p in particles_obstacle:     color = 'blue' if p['type'] == 'positive' else 'red'     ax.plot(p['x'], p['y'], 'o', color=color, markersize=10, mec='k', mew=1.5)     ax.arrow(p['x'], p['y'], p['vx']*0.25, p['vy']*0.25,              head_width=0.08, head_length=0.04, fc=color, ec=color, alpha=0.7)  ax.plot([], [], 'bo', markersize=10, label='Positive (go this way)') ax.plot([], [], 'ro', markersize=10, label='Negative (avoid this)') ax.legend(loc='upper left') plt.show()  print(\"Blue particles guide AROUND the obstacle.\") print(\"Red particles warn: 'Don't go into the obstacle!'\") <pre>Blue particles guide AROUND the obstacle.\nRed particles warn: 'Don't go into the obstacle!'\n</pre> In\u00a0[8]: Copied! <pre># Example 5.2: Compare Fields \u2014 With vs Without Obstacle\n\n# Compute Q\u207a fields\nQ_simple = compute_Q_field_fixed_action(X, Y, vx_goal, vy_goal, particles_simple)\nQ_obstacle = compute_Q_field_fixed_action(X, Y, vx_goal, vy_goal, particles_obstacle)\n\n# Infer policies\nU_simple, V_simple = infer_policy(X_p, Y_p, particles_simple)\nU_obstacle, V_obstacle = infer_policy(X_p, Y_p, particles_obstacle)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 6))\n\n# Simple domain\nax1 = axes[0]\nc1 = ax1.contourf(X, Y, Q_simple, levels=20, cmap='RdBu_r', alpha=0.5)\nax1.quiver(X_p, Y_p, U_simple, V_simple, color='black', scale=20, width=0.005)\nax1.plot(*domain_simple['goal'], 'g^', markersize=15, zorder=10)\nax1.plot(*domain_simple['start'], 'ko', markersize=10, zorder=10)\nax1.set_title('Simple Domain\\n(Direct path to goal)')\nax1.set_xlabel('$x$'); ax1.set_ylabel('$y$')\nax1.set_xlim(-0.5, 4.5); ax1.set_ylim(-0.5, 4.5); ax1.set_aspect('equal')\nplt.colorbar(c1, ax=ax1, shrink=0.8)\n\n# Domain with obstacle\nax2 = axes[1]\nc2 = ax2.contourf(X, Y, Q_obstacle, levels=20, cmap='RdBu_r', alpha=0.5)\nax2.quiver(X_p, Y_p, U_obstacle, V_obstacle, color='black', scale=20, width=0.005)\n# Draw obstacle\nrect = Rectangle((obstacle['x'], obstacle['y']), obstacle['w'], obstacle['h'],\n                  facecolor='gray', edgecolor='black', alpha=0.8, zorder=5)\nax2.add_patch(rect)\nax2.plot(*domain_obstacles['goal'], 'g^', markersize=15, zorder=10)\nax2.plot(*domain_obstacles['start'], 'ko', markersize=10, zorder=10)\nax2.set_title('Domain with Obstacle\\n(Path goes around)')\nax2.set_xlabel('$x$'); ax2.set_ylabel('$y$')\nax2.set_xlim(-0.5, 4.5); ax2.set_ylim(-0.5, 4.5); ax2.set_aspect('equal')\nplt.colorbar(c2, ax=ax2, shrink=0.8)\n\nplt.tight_layout(); plt.show()\n\nprint(\"Left: Direct diagonal path when no obstacles.\")\nprint(\"Right: Policy naturally avoids obstacle \u2014 encoded by negative particles!\")\n</pre> # Example 5.2: Compare Fields \u2014 With vs Without Obstacle  # Compute Q\u207a fields Q_simple = compute_Q_field_fixed_action(X, Y, vx_goal, vy_goal, particles_simple) Q_obstacle = compute_Q_field_fixed_action(X, Y, vx_goal, vy_goal, particles_obstacle)  # Infer policies U_simple, V_simple = infer_policy(X_p, Y_p, particles_simple) U_obstacle, V_obstacle = infer_policy(X_p, Y_p, particles_obstacle)  fig, axes = plt.subplots(1, 2, figsize=(14, 6))  # Simple domain ax1 = axes[0] c1 = ax1.contourf(X, Y, Q_simple, levels=20, cmap='RdBu_r', alpha=0.5) ax1.quiver(X_p, Y_p, U_simple, V_simple, color='black', scale=20, width=0.005) ax1.plot(*domain_simple['goal'], 'g^', markersize=15, zorder=10) ax1.plot(*domain_simple['start'], 'ko', markersize=10, zorder=10) ax1.set_title('Simple Domain\\n(Direct path to goal)') ax1.set_xlabel('$x$'); ax1.set_ylabel('$y$') ax1.set_xlim(-0.5, 4.5); ax1.set_ylim(-0.5, 4.5); ax1.set_aspect('equal') plt.colorbar(c1, ax=ax1, shrink=0.8)  # Domain with obstacle ax2 = axes[1] c2 = ax2.contourf(X, Y, Q_obstacle, levels=20, cmap='RdBu_r', alpha=0.5) ax2.quiver(X_p, Y_p, U_obstacle, V_obstacle, color='black', scale=20, width=0.005) # Draw obstacle rect = Rectangle((obstacle['x'], obstacle['y']), obstacle['w'], obstacle['h'],                   facecolor='gray', edgecolor='black', alpha=0.8, zorder=5) ax2.add_patch(rect) ax2.plot(*domain_obstacles['goal'], 'g^', markersize=15, zorder=10) ax2.plot(*domain_obstacles['start'], 'ko', markersize=10, zorder=10) ax2.set_title('Domain with Obstacle\\n(Path goes around)') ax2.set_xlabel('$x$'); ax2.set_ylabel('$y$') ax2.set_xlim(-0.5, 4.5); ax2.set_ylim(-0.5, 4.5); ax2.set_aspect('equal') plt.colorbar(c2, ax=ax2, shrink=0.8)  plt.tight_layout(); plt.show()  print(\"Left: Direct diagonal path when no obstacles.\") print(\"Right: Policy naturally avoids obstacle \u2014 encoded by negative particles!\") <pre>Left: Direct diagonal path when no obstacles.\nRight: Policy naturally avoids obstacle \u2014 encoded by negative particles!\n</pre>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#notebook-3-reinforcement-fields-in-grl","title":"Notebook 3: Reinforcement Fields in GRL\u00b6","text":"<p>Part 3 of the GRL Field Series</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#overview","title":"Overview\u00b6","text":"<p>Now we bring everything together! We'll apply the concepts from Notebooks 1-2 to a 2D navigation domain \u2014 the same domain from Figure 4 of the original GRL paper.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#learning-objectives","title":"Learning Objectives\u00b6","text":"<ol> <li>Augmented state-action space \u2014 Why $z = (s, \\theta)$?</li> <li>Particle memory \u2014 Experience as weighted points</li> <li>Field emergence \u2014 How $Q^+$ guides navigation</li> <li>Policy inference \u2014 Reading the field to choose actions</li> <li>With/without obstacles \u2014 How particles encode constraints</li> </ol>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#the-2d-navigation-domain","title":"The 2D Navigation Domain\u00b6","text":"<p>From the original paper (Figure 4):</p> <p>\"A 2D navigation domain where the agent seeks an optimal path to retrieve the flag following the shortest route. Positive particles are marked in blue that locally serve as the guiding signal, leading the agent to traverse along an optimal path, whereas negative particles are marked in red that serve as the constraints, informing the agent which actions (and hence directions) to avoid.\"</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#time","title":"Time\u00b6","text":"<p>~30 minutes</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#part-1-the-navigation-domain","title":"Part 1: The Navigation Domain\u00b6","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#setup","title":"Setup\u00b6","text":"<ul> <li>State space: 2D position $(x, y)$</li> <li>Action space: 2D velocity/direction $(v_x, v_y)$ or angle $\\theta$</li> <li>Goal: Reach the flag \ud83d\udea9</li> <li>Constraints: Avoid obstacles (in some scenarios)</li> </ul>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#augmented-space","title":"Augmented Space\u00b6","text":"<p>In GRL, we work in augmented state-action space:</p> <p>$$z = (s, \\theta) = (x, y, v_x, v_y)$$</p> <p>The reinforcement field $Q^+(z)$ is defined over this joint space.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#part-2-particles-as-experience","title":"Part 2: Particles as Experience\u00b6","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#what-are-particles","title":"What are Particles?\u00b6","text":"<p>In GRL, particles represent experience:</p> <p>$$\\text{Particle } i: \\quad (z_i, w_i) = ((s_i, \\theta_i), w_i)$$</p> <p>where:</p> <ul> <li>$z_i = (s_i, \\theta_i)$: Position in augmented space (state + action)</li> <li>$w_i$: Weight (positive = good, negative = bad)</li> </ul>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#particle-semantics","title":"Particle Semantics\u00b6","text":"Particle Type Weight Meaning Positive (blue) $w &gt; 0$ \"Taking this action at this state was good\" Negative (red) $w &lt; 0$ \"Taking this action at this state was bad\""},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#synthetic-particles","title":"Synthetic Particles\u00b6","text":"<p>For illustration, we'll place particles manually to show what a learned field might look like.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#part-3-the-reinforcement-field","title":"Part 3: The Reinforcement Field\u00b6","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#building-q-from-particles","title":"Building $Q^+$ from Particles\u00b6","text":"<p>The reinforcement field is a superposition of particle influences:</p> <p>$$Q^+(z) = \\sum_{i=1}^{N} w_i \\, k(z, z_i)$$</p> <p>where $z = (x, y, v_x, v_y)$ and $k$ is the RBF kernel:</p> <p>$$k(z, z') = \\exp\\left(-\\frac{\\|z - z'\\|^2}{2\\ell^2}\\right)$$</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#visualization-strategy","title":"Visualization Strategy\u00b6","text":"<p>Since $z$ is 4D, we'll visualize slices:</p> <ol> <li>Fixed action: Show $Q^+(x, y, v_x^*, v_y^*)$ as a 2D heatmap</li> <li>Fixed state: Show $Q^+(x^*, y^*, v_x, v_y)$ as action landscape</li> </ol>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#part-4-policy-inference-reading-the-field","title":"Part 4: Policy Inference \u2014 Reading the Field\u00b6","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#how-does-the-agent-choose-actions","title":"How Does the Agent Choose Actions?\u00b6","text":"<p>At state $s = (x, y)$, the agent evaluates $Q^+(s, \\theta)$ for different actions $\\theta$ and chooses:</p> <p>Greedy policy: $$\\theta^* = \\arg\\max_\\theta Q^+(s, \\theta)$$</p> <p>Boltzmann (soft) policy: $$\\pi(\\theta | s) \\propto \\exp(\\beta \\, Q^+(s, \\theta))$$</p> <p>where $\\beta$ is the inverse temperature (higher = more greedy).</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#why-do-the-policy-arrows-look-parallel","title":"\ud83d\udd0d Why Do the Policy Arrows Look Parallel?\u00b6","text":"<p>You may notice that the policy arrows appear mostly parallel (pointing toward the upper-right) rather than converging on the goal like a true \"point toward goal\" field. This is not a bug \u2014 it reveals an important property of GRL!</p> <p>Key insight: The $Q^+$ field is built from particle experiences, not from a direct \"distance to goal\" function.</p> Field Type How It's Built Arrow Behavior True gradient field $\\nabla(-\\|x - x_{goal}\\|^2)$ Arrows converge on goal from all directions Particle-based $Q^+$ $\\sum_i w_i k(z, z_i)$ Arrows follow gradient of particle influence <p>Why the difference?</p> <ol> <li><p>Sparse particle coverage: Our synthetic particles are clustered along the diagonal path. In regions far from particles, the gradient is approximately constant.</p> </li> <li><p>Local vs. global information: Each particle only knows \"at this state, this action was good.\" It doesn't encode global path information.</p> </li> <li><p>Discrete action search: We search over 16 angles \u2014 if the $Q^+$ gradient is nearly constant, many states pick the same best angle.</p> </li> </ol> <p>What would make arrows converge?</p> <ul> <li>More particles spread throughout the domain (recording trajectories from many starting points)</li> <li>Particles that encode \"turn toward goal\" from various angles</li> <li>Or using a goal-directed potential function instead of pure particle-based learning</li> </ul> <p>\ud83d\udcda For visual proof and detailed comparison, see:</p> <ul> <li><code>03a_particle_coverage_effects.ipynb</code> \u2014 Supplementary notebook with side-by-side comparison</li> <li><code>particle_vs_gradient_fields.md</code> \u2014 Detailed theory</li> </ul> <p>This is actually a feature, not a bug: GRL's policy quality depends on the richness of the particle memory. With more diverse experience, the field becomes more nuanced!</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#part-5-navigation-with-obstacles","title":"Part 5: Navigation with Obstacles\u00b6","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#how-obstacles-change-the-field","title":"How Obstacles Change the Field\u00b6","text":"<p>When there are obstacles:</p> <ul> <li>Negative particles are placed near obstacles (\"don't go here\")</li> <li>Positive particles guide around the obstacle</li> <li>The field naturally encodes the constraint!</li> </ul> <p>This is the key insight from Figure 4 of the original paper.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#summary","title":"Summary\u00b6","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#what-we-learned","title":"What We Learned\u00b6","text":"Concept Meaning Visualization Augmented space $z = (s, \\theta)$ 4D: position + action Particles $(z_i, w_i)$ Blue (good) / Red (bad) Reinforcement field $Q^+(z) = \\sum_i w_i k(z, z_i)$ Heatmap over space Policy $\\pi(s) = \\arg\\max_\\theta Q^+(s, \\theta)$ Arrow field Obstacles Negative particles Repel policy away"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#key-equations","title":"Key Equations\u00b6","text":"<p>Reinforcement Field: $$Q^+(z) = \\sum_{i=1}^{N} w_i \\, k(z, z_i), \\quad k(z, z') = \\exp\\left(-\\frac{\\|z-z'\\|^2}{2\\ell^2}\\right)$$</p> <p>Policy Inference: $$\\theta^* = \\arg\\max_\\theta Q^+(s, \\theta) \\quad \\text{or} \\quad \\pi(\\theta|s) \\propto \\exp(\\beta Q^+(s, \\theta))$$</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#the-grl-insight","title":"The GRL Insight\u00b6","text":"<p>Policy emerges from the field \u2014 no explicit policy network needed!</p> <ul> <li>Positive particles attract the policy toward good actions</li> <li>Negative particles repel the policy from bad actions</li> <li>The kernel spreads influence smoothly</li> <li>Learning = adding/updating particles</li> </ul>"},{"location":"notebooks/field_series/03_reinforcement_fields/03_reinforcement_fields/#next-steps","title":"Next Steps\u00b6","text":"<ul> <li>Tutorial chapters: Deep dive into MemoryUpdate and RF-SARSA algorithms</li> <li>Implementation: Build a real GRL agent</li> <li>Experiments: Compare with classical RL methods</li> </ul>"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/","title":"03a: Particle Coverage Effects","text":"In\u00a0[\u00a0]: Copied! <pre># Setup\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_theme(style='whitegrid', context='notebook')\nplt.rcParams['figure.figsize'] = (14, 10)\n%matplotlib inline\n\n# Goal location\nGOAL = np.array([4.0, 4.0])\nprint(\"Setup complete. Goal at (4, 4)\")\n</pre> # Setup import numpy as np import matplotlib.pyplot as plt import seaborn as sns  sns.set_theme(style='whitegrid', context='notebook') plt.rcParams['figure.figsize'] = (14, 10) %matplotlib inline  # Goal location GOAL = np.array([4.0, 4.0]) print(\"Setup complete. Goal at (4, 4)\") In\u00a0[\u00a0]: Copied! <pre>def rbf_kernel_4d(z, z_prime, lengthscale=0.8):\n    \"\"\"RBF kernel in 4D augmented space.\"\"\"\n    return np.exp(-np.sum((z - z_prime)**2) / (2 * lengthscale**2))\n\ndef compute_Q_plus(x, y, vx, vy, particles, lengthscale=0.8):\n    \"\"\"Compute Q+(x, y, vx, vy) from particles.\"\"\"\n    z = np.array([x, y, vx, vy])\n    Q = 0.0\n    for p in particles:\n        z_i = np.array([p['x'], p['y'], p['vx'], p['vy']])\n        Q += p['w'] * rbf_kernel_4d(z, z_i, lengthscale)\n    return Q\n\ndef infer_policy(X, Y, particles, n_angles=24, lengthscale=0.8):\n    \"\"\"Infer greedy policy at each state.\"\"\"\n    U = np.zeros_like(X)\n    V = np.zeros_like(Y)\n    \n    angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)\n    \n    for i in range(X.shape[0]):\n        for j in range(X.shape[1]):\n            x, y = X[i, j], Y[i, j]\n            best_Q = -np.inf\n            best_vx, best_vy = 0, 0\n            \n            for theta in angles:\n                vx, vy = np.cos(theta), np.sin(theta)\n                Q = compute_Q_plus(x, y, vx, vy, particles, lengthscale)\n                if Q &gt; best_Q:\n                    best_Q = Q\n                    best_vx, best_vy = vx, vy\n            \n            U[i, j] = best_vx\n            V[i, j] = best_vy\n    \n    return U, V\n\ndef true_gradient_policy(X, Y, goal):\n    \"\"\"Compute true gradient field pointing toward goal.\"\"\"\n    U = goal[0] - X\n    V = goal[1] - Y\n    # Normalize\n    norm = np.sqrt(U**2 + V**2) + 1e-6\n    return U / norm, V / norm\n\nprint(\"Core functions defined.\")\n</pre> def rbf_kernel_4d(z, z_prime, lengthscale=0.8):     \"\"\"RBF kernel in 4D augmented space.\"\"\"     return np.exp(-np.sum((z - z_prime)**2) / (2 * lengthscale**2))  def compute_Q_plus(x, y, vx, vy, particles, lengthscale=0.8):     \"\"\"Compute Q+(x, y, vx, vy) from particles.\"\"\"     z = np.array([x, y, vx, vy])     Q = 0.0     for p in particles:         z_i = np.array([p['x'], p['y'], p['vx'], p['vy']])         Q += p['w'] * rbf_kernel_4d(z, z_i, lengthscale)     return Q  def infer_policy(X, Y, particles, n_angles=24, lengthscale=0.8):     \"\"\"Infer greedy policy at each state.\"\"\"     U = np.zeros_like(X)     V = np.zeros_like(Y)          angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)          for i in range(X.shape[0]):         for j in range(X.shape[1]):             x, y = X[i, j], Y[i, j]             best_Q = -np.inf             best_vx, best_vy = 0, 0                          for theta in angles:                 vx, vy = np.cos(theta), np.sin(theta)                 Q = compute_Q_plus(x, y, vx, vy, particles, lengthscale)                 if Q &gt; best_Q:                     best_Q = Q                     best_vx, best_vy = vx, vy                          U[i, j] = best_vx             V[i, j] = best_vy          return U, V  def true_gradient_policy(X, Y, goal):     \"\"\"Compute true gradient field pointing toward goal.\"\"\"     U = goal[0] - X     V = goal[1] - Y     # Normalize     norm = np.sqrt(U**2 + V**2) + 1e-6     return U / norm, V / norm  print(\"Core functions defined.\") In\u00a0[\u00a0]: Copied! <pre>def create_sparse_diagonal_particles(goal, n_points=8):\n    \"\"\"\n    Sparse particles along diagonal path.\n    All particles have similar action direction (toward upper-right).\n    \"\"\"\n    particles = []\n    for i in range(n_points):\n        t = i / (n_points - 1)\n        px, py = t * goal[0], t * goal[1]\n        # Action: always toward goal (upper-right)\n        dx, dy = goal[0] - px, goal[1] - py\n        norm = np.sqrt(dx**2 + dy**2) + 1e-6\n        particles.append({\n            'x': px, 'y': py,\n            'vx': dx/norm, 'vy': dy/norm,\n            'w': 1.5\n        })\n    return particles\n\ndef create_rich_radial_particles(goal, n_rings=3, points_per_ring=8):\n    \"\"\"\n    Rich particles from multiple directions.\n    Particles at various positions, all pointing toward goal.\n    \"\"\"\n    particles = []\n    radii = [1.0, 2.0, 3.0][:n_rings]\n    \n    for r in radii:\n        for i in range(points_per_ring):\n            angle = 2 * np.pi * i / points_per_ring\n            # Position: around the goal at radius r\n            px = goal[0] + r * np.cos(angle)\n            py = goal[1] + r * np.sin(angle)\n            # Skip if outside domain\n            if px &lt; -0.5 or px &gt; 4.5 or py &lt; -0.5 or py &gt; 4.5:\n                continue\n            # Action: toward goal\n            dx, dy = goal[0] - px, goal[1] - py\n            norm = np.sqrt(dx**2 + dy**2) + 1e-6\n            particles.append({\n                'x': px, 'y': py,\n                'vx': dx/norm, 'vy': dy/norm,\n                'w': 1.5\n            })\n    \n    # Also add particles along paths from corners\n    corners = [(0, 0), (0, 4), (4, 0)]\n    for cx, cy in corners:\n        for t in [0.25, 0.5, 0.75]:\n            px = cx + t * (goal[0] - cx)\n            py = cy + t * (goal[1] - cy)\n            dx, dy = goal[0] - px, goal[1] - py\n            norm = np.sqrt(dx**2 + dy**2) + 1e-6\n            particles.append({\n                'x': px, 'y': py,\n                'vx': dx/norm, 'vy': dy/norm,\n                'w': 1.5\n            })\n    \n    return particles\n\n# Create particle sets\nparticles_sparse = create_sparse_diagonal_particles(GOAL)\nparticles_rich = create_rich_radial_particles(GOAL)\n\nprint(f\"Sparse diagonal: {len(particles_sparse)} particles\")\nprint(f\"Rich radial: {len(particles_rich)} particles\")\n</pre> def create_sparse_diagonal_particles(goal, n_points=8):     \"\"\"     Sparse particles along diagonal path.     All particles have similar action direction (toward upper-right).     \"\"\"     particles = []     for i in range(n_points):         t = i / (n_points - 1)         px, py = t * goal[0], t * goal[1]         # Action: always toward goal (upper-right)         dx, dy = goal[0] - px, goal[1] - py         norm = np.sqrt(dx**2 + dy**2) + 1e-6         particles.append({             'x': px, 'y': py,             'vx': dx/norm, 'vy': dy/norm,             'w': 1.5         })     return particles  def create_rich_radial_particles(goal, n_rings=3, points_per_ring=8):     \"\"\"     Rich particles from multiple directions.     Particles at various positions, all pointing toward goal.     \"\"\"     particles = []     radii = [1.0, 2.0, 3.0][:n_rings]          for r in radii:         for i in range(points_per_ring):             angle = 2 * np.pi * i / points_per_ring             # Position: around the goal at radius r             px = goal[0] + r * np.cos(angle)             py = goal[1] + r * np.sin(angle)             # Skip if outside domain             if px &lt; -0.5 or px &gt; 4.5 or py &lt; -0.5 or py &gt; 4.5:                 continue             # Action: toward goal             dx, dy = goal[0] - px, goal[1] - py             norm = np.sqrt(dx**2 + dy**2) + 1e-6             particles.append({                 'x': px, 'y': py,                 'vx': dx/norm, 'vy': dy/norm,                 'w': 1.5             })          # Also add particles along paths from corners     corners = [(0, 0), (0, 4), (4, 0)]     for cx, cy in corners:         for t in [0.25, 0.5, 0.75]:             px = cx + t * (goal[0] - cx)             py = cy + t * (goal[1] - cy)             dx, dy = goal[0] - px, goal[1] - py             norm = np.sqrt(dx**2 + dy**2) + 1e-6             particles.append({                 'x': px, 'y': py,                 'vx': dx/norm, 'vy': dy/norm,                 'w': 1.5             })          return particles  # Create particle sets particles_sparse = create_sparse_diagonal_particles(GOAL) particles_rich = create_rich_radial_particles(GOAL)  print(f\"Sparse diagonal: {len(particles_sparse)} particles\") print(f\"Rich radial: {len(particles_rich)} particles\") In\u00a0[\u00a0]: Copied! <pre># Create grids\nx_policy = np.linspace(0, 4, 12)\ny_policy = np.linspace(0, 4, 12)\nX_p, Y_p = np.meshgrid(x_policy, y_policy)\n\n# Compute policies\nprint(\"Computing sparse diagonal policy...\")\nU_sparse, V_sparse = infer_policy(X_p, Y_p, particles_sparse)\n\nprint(\"Computing rich radial policy...\")\nU_rich, V_rich = infer_policy(X_p, Y_p, particles_rich)\n\nprint(\"Computing true gradient policy...\")\nU_true, V_true = true_gradient_policy(X_p, Y_p, GOAL)\n\nprint(\"Done!\")\n</pre> # Create grids x_policy = np.linspace(0, 4, 12) y_policy = np.linspace(0, 4, 12) X_p, Y_p = np.meshgrid(x_policy, y_policy)  # Compute policies print(\"Computing sparse diagonal policy...\") U_sparse, V_sparse = infer_policy(X_p, Y_p, particles_sparse)  print(\"Computing rich radial policy...\") U_rich, V_rich = infer_policy(X_p, Y_p, particles_rich)  print(\"Computing true gradient policy...\") U_true, V_true = true_gradient_policy(X_p, Y_p, GOAL)  print(\"Done!\") In\u00a0[\u00a0]: Copied! <pre># Visualization\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\nconfigs = [\n    (axes[0], U_sparse, V_sparse, particles_sparse, \n     'Sparse Diagonal Particles', 'Arrows mostly parallel'),\n    (axes[1], U_rich, V_rich, particles_rich,\n     'Rich Radial Particles', 'Arrows converge on goal'),\n    (axes[2], U_true, V_true, None,\n     'True Gradient Field', 'Perfect convergence (no particles)')\n]\n\nfor ax, U, V, particles, title, subtitle in configs:\n    # Policy arrows\n    ax.quiver(X_p, Y_p, U, V, color='black', scale=18, width=0.006, zorder=5)\n    \n    # Particles (if any)\n    if particles:\n        for p in particles:\n            ax.plot(p['x'], p['y'], 'bo', markersize=8, alpha=0.6, mec='darkblue', mew=1)\n            ax.arrow(p['x'], p['y'], p['vx']*0.2, p['vy']*0.2,\n                     head_width=0.08, head_length=0.04, fc='blue', ec='blue', alpha=0.5)\n    \n    # Goal\n    ax.plot(*GOAL, 'g^', markersize=20, label='Goal', zorder=10)\n    ax.plot(0, 0, 'ko', markersize=12, label='Start', zorder=10)\n    \n    ax.set_xlim(-0.5, 4.5)\n    ax.set_ylim(-0.5, 4.5)\n    ax.set_xlabel('$x$')\n    ax.set_ylabel('$y$')\n    ax.set_title(f'{title}\\n({subtitle})', fontsize=12)\n    ax.set_aspect('equal')\n    ax.legend(loc='upper left')\n    ax.grid(True, alpha=0.3)\n\nplt.suptitle('Effect of Particle Coverage on Inferred Policy', fontsize=14, y=1.02)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"KEY OBSERVATION:\")\nprint(\"=\"*70)\nprint(\"Left:   Sparse particles \u2192 parallel arrows (limited directional info)\")\nprint(\"Middle: Rich particles \u2192 converging arrows (diverse directional info)\")\nprint(\"Right:  True gradient \u2192 perfect convergence (geometric, not learned)\")\nprint(\"=\"*70)\n</pre> # Visualization fig, axes = plt.subplots(1, 3, figsize=(18, 6))  configs = [     (axes[0], U_sparse, V_sparse, particles_sparse,       'Sparse Diagonal Particles', 'Arrows mostly parallel'),     (axes[1], U_rich, V_rich, particles_rich,      'Rich Radial Particles', 'Arrows converge on goal'),     (axes[2], U_true, V_true, None,      'True Gradient Field', 'Perfect convergence (no particles)') ]  for ax, U, V, particles, title, subtitle in configs:     # Policy arrows     ax.quiver(X_p, Y_p, U, V, color='black', scale=18, width=0.006, zorder=5)          # Particles (if any)     if particles:         for p in particles:             ax.plot(p['x'], p['y'], 'bo', markersize=8, alpha=0.6, mec='darkblue', mew=1)             ax.arrow(p['x'], p['y'], p['vx']*0.2, p['vy']*0.2,                      head_width=0.08, head_length=0.04, fc='blue', ec='blue', alpha=0.5)          # Goal     ax.plot(*GOAL, 'g^', markersize=20, label='Goal', zorder=10)     ax.plot(0, 0, 'ko', markersize=12, label='Start', zorder=10)          ax.set_xlim(-0.5, 4.5)     ax.set_ylim(-0.5, 4.5)     ax.set_xlabel('$x$')     ax.set_ylabel('$y$')     ax.set_title(f'{title}\\n({subtitle})', fontsize=12)     ax.set_aspect('equal')     ax.legend(loc='upper left')     ax.grid(True, alpha=0.3)  plt.suptitle('Effect of Particle Coverage on Inferred Policy', fontsize=14, y=1.02) plt.tight_layout() plt.show()  print(\"\\n\" + \"=\"*70) print(\"KEY OBSERVATION:\") print(\"=\"*70) print(\"Left:   Sparse particles \u2192 parallel arrows (limited directional info)\") print(\"Middle: Rich particles \u2192 converging arrows (diverse directional info)\") print(\"Right:  True gradient \u2192 perfect convergence (geometric, not learned)\") print(\"=\"*70) In\u00a0[\u00a0]: Copied! <pre>def compute_alignment(U_policy, V_policy, U_true, V_true):\n    \"\"\"\n    Compute average cosine similarity between policy and true gradient.\n    1.0 = perfect alignment, 0.0 = orthogonal, -1.0 = opposite\n    \"\"\"\n    dot_product = U_policy * U_true + V_policy * V_true\n    # Both are already normalized, so dot product = cosine similarity\n    return np.mean(dot_product)\n\n# Normalize sparse and rich policies for fair comparison\nnorm_sparse = np.sqrt(U_sparse**2 + V_sparse**2) + 1e-6\nU_sparse_norm = U_sparse / norm_sparse\nV_sparse_norm = V_sparse / norm_sparse\n\nnorm_rich = np.sqrt(U_rich**2 + V_rich**2) + 1e-6\nU_rich_norm = U_rich / norm_rich\nV_rich_norm = V_rich / norm_rich\n\nalign_sparse = compute_alignment(U_sparse_norm, V_sparse_norm, U_true, V_true)\nalign_rich = compute_alignment(U_rich_norm, V_rich_norm, U_true, V_true)\nalign_true = compute_alignment(U_true, V_true, U_true, V_true)\n\nprint(\"Alignment with True Gradient (cosine similarity):\")\nprint(f\"  Sparse diagonal: {align_sparse:.3f}\")\nprint(f\"  Rich radial:     {align_rich:.3f}\")\nprint(f\"  True gradient:   {align_true:.3f} (baseline)\")\nprint()\nprint(f\"Improvement from sparse to rich: {(align_rich - align_sparse):.3f}\")\nprint(f\"  ({(align_rich - align_sparse) / (1 - align_sparse) * 100:.1f}% of remaining gap closed)\")\n</pre> def compute_alignment(U_policy, V_policy, U_true, V_true):     \"\"\"     Compute average cosine similarity between policy and true gradient.     1.0 = perfect alignment, 0.0 = orthogonal, -1.0 = opposite     \"\"\"     dot_product = U_policy * U_true + V_policy * V_true     # Both are already normalized, so dot product = cosine similarity     return np.mean(dot_product)  # Normalize sparse and rich policies for fair comparison norm_sparse = np.sqrt(U_sparse**2 + V_sparse**2) + 1e-6 U_sparse_norm = U_sparse / norm_sparse V_sparse_norm = V_sparse / norm_sparse  norm_rich = np.sqrt(U_rich**2 + V_rich**2) + 1e-6 U_rich_norm = U_rich / norm_rich V_rich_norm = V_rich / norm_rich  align_sparse = compute_alignment(U_sparse_norm, V_sparse_norm, U_true, V_true) align_rich = compute_alignment(U_rich_norm, V_rich_norm, U_true, V_true) align_true = compute_alignment(U_true, V_true, U_true, V_true)  print(\"Alignment with True Gradient (cosine similarity):\") print(f\"  Sparse diagonal: {align_sparse:.3f}\") print(f\"  Rich radial:     {align_rich:.3f}\") print(f\"  True gradient:   {align_true:.3f} (baseline)\") print() print(f\"Improvement from sparse to rich: {(align_rich - align_sparse):.3f}\") print(f\"  ({(align_rich - align_sparse) / (1 - align_sparse) * 100:.1f}% of remaining gap closed)\") In\u00a0[\u00a0]: Copied! <pre># Visualize alignment heatmap\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Compute local alignment\nalign_sparse_local = U_sparse_norm * U_true + V_sparse_norm * V_true\nalign_rich_local = U_rich_norm * U_true + V_rich_norm * V_true\n\n# Plot\nfor ax, align_local, title in [\n    (axes[0], align_sparse_local, 'Sparse Diagonal'),\n    (axes[1], align_rich_local, 'Rich Radial')\n]:\n    c = ax.contourf(X_p, Y_p, align_local, levels=np.linspace(-1, 1, 21), cmap='RdYlGn')\n    ax.plot(*GOAL, 'k^', markersize=15, zorder=10)\n    ax.set_xlabel('$x$'); ax.set_ylabel('$y$')\n    ax.set_title(f'{title}\\nLocal Alignment with True Gradient')\n    ax.set_aspect('equal')\n    plt.colorbar(c, ax=ax, label='Cosine Similarity')\n\nplt.suptitle('Where Does Each Policy Align with the True Gradient?', fontsize=13)\nplt.tight_layout()\nplt.show()\n\nprint(\"Green = good alignment, Red = poor alignment\")\nprint(\"Rich particles provide better coverage, especially away from the diagonal.\")\n</pre> # Visualize alignment heatmap fig, axes = plt.subplots(1, 2, figsize=(14, 5))  # Compute local alignment align_sparse_local = U_sparse_norm * U_true + V_sparse_norm * V_true align_rich_local = U_rich_norm * U_true + V_rich_norm * V_true  # Plot for ax, align_local, title in [     (axes[0], align_sparse_local, 'Sparse Diagonal'),     (axes[1], align_rich_local, 'Rich Radial') ]:     c = ax.contourf(X_p, Y_p, align_local, levels=np.linspace(-1, 1, 21), cmap='RdYlGn')     ax.plot(*GOAL, 'k^', markersize=15, zorder=10)     ax.set_xlabel('$x$'); ax.set_ylabel('$y$')     ax.set_title(f'{title}\\nLocal Alignment with True Gradient')     ax.set_aspect('equal')     plt.colorbar(c, ax=ax, label='Cosine Similarity')  plt.suptitle('Where Does Each Policy Align with the True Gradient?', fontsize=13) plt.tight_layout() plt.show()  print(\"Green = good alignment, Red = poor alignment\") print(\"Rich particles provide better coverage, especially away from the diagonal.\")"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#notebook-3a-particle-coverage-effects-on-policy-fields","title":"Notebook 3a: Particle Coverage Effects on Policy Fields\u00b6","text":"<p>Supplementary to Notebook 3 \u2014 Reinforcement Fields in GRL</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#motivation","title":"Motivation\u00b6","text":"<p>In Notebook 3, we observed that the inferred policy arrows appear mostly parallel rather than converging on the goal. This notebook provides visual proof that this behavior is due to sparse particle coverage and demonstrates how richer particle distributions produce more intuitive policy fields.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#what-well-compare","title":"What We'll Compare\u00b6","text":"Scenario Particle Distribution Expected Arrow Behavior Sparse (diagonal) Particles only along diagonal path Parallel arrows Rich (radial) Particles from many directions Converging arrows True gradient No particles \u2014 direct goal potential Perfect convergence"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#time","title":"Time\u00b6","text":"<p>~15 minutes</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#part-1-core-functions","title":"Part 1: Core Functions\u00b6","text":"<p>We'll reuse the $Q^+$ computation from Notebook 3.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#part-2-three-particle-distributions","title":"Part 2: Three Particle Distributions\u00b6","text":"<p>We'll create three different particle configurations to compare.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#part-3-visual-comparison","title":"Part 3: Visual Comparison\u00b6","text":"<p>Now let's see the difference in policy fields!</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#part-4-quantitative-analysis","title":"Part 4: Quantitative Analysis\u00b6","text":"<p>Let's measure how well each policy field aligns with the true gradient.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#summary","title":"Summary\u00b6","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#visual-proof","title":"Visual Proof\u00b6","text":"<p>We demonstrated that:</p> <ol> <li><p>Sparse diagonal particles \u2192 Policy arrows are mostly parallel because all particles encode similar action directions.</p> </li> <li><p>Rich radial particles \u2192 Policy arrows converge on the goal because particles from different directions provide diverse directional information.</p> </li> <li><p>True gradient field \u2192 Perfect convergence (but requires explicit goal knowledge, cannot encode obstacles).</p> </li> </ol>"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#implications-for-grl","title":"Implications for GRL\u00b6","text":"Aspect Sparse Coverage Rich Coverage Exploration Limited trajectories Diverse trajectories Policy quality Approximate Near-optimal Generalization Poor in unseen regions Good everywhere"},{"location":"notebooks/field_series/03_reinforcement_fields/03a_particle_coverage_effects/#key-takeaway","title":"Key Takeaway\u00b6","text":"<p>Policy quality in GRL is directly proportional to the richness of the particle memory.</p> <p>This is why exploration and diverse experience collection are crucial in GRL \u2014 the more varied the particles, the more the learned field resembles the true optimal policy.</p> <p>See also:</p> <ul> <li><code>03_reinforcement_fields.ipynb</code> \u2014 Main notebook</li> <li><code>notes/particle_vs_gradient_fields.md</code> \u2014 Detailed theory</li> </ul>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/","title":"Particle-Based Q\u207a Fields vs. True Gradient Fields","text":"<p>A detailed comparison for understanding GRL policy behavior</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#overview","title":"Overview","text":"<p>When visualizing GRL's inferred policy as a vector field, you may notice that arrows appear mostly parallel rather than converging on the goal. This document explains why this happens and what it reveals about the nature of particle-based reinforcement fields.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#two-types-of-fields","title":"Two Types of Fields","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#1-true-gradient-field-goal-directed-potential","title":"1. True Gradient Field (Goal-Directed Potential)","text":"<p>A classical approach to navigation uses a potential function that directly encodes distance to the goal:</p> \\[\\phi(x, y) = -\\|(x, y) - (x_{goal}, y_{goal})\\|^2\\] <p>The gradient of this potential:</p> \\[\\nabla \\phi = -2 \\begin{pmatrix} x - x_{goal} \\\\ y - y_{goal} \\end{pmatrix}\\] <p>This gradient field has a key property: arrows at every point converge toward the goal.</p> <pre><code>        \u2198   \u2193   \u2199\n         \u2198  \u2193  \u2199\n    \u2192  \u2192  [GOAL]  \u2190  \u2190\n         \u2197  \u2191  \u2196\n        \u2197   \u2191   \u2196\n</code></pre> <p>Pros: - Globally optimal direction at every point - Arrows naturally \"rotate\" to point at goal</p> <p>Cons: - Requires explicit knowledge of goal location - Cannot encode constraints (obstacles) naturally - No learning \u2014 purely geometric</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#2-particle-based-q-field-grl","title":"2. Particle-Based Q\u207a Field (GRL)","text":"<p>GRL builds the reinforcement field from experience particles:</p> \\[Q^+(z) = \\sum_{i=1}^{N} w_i \\, k(z, z_i)\\] <p>where \\(z = (x, y, v_x, v_y)\\) is the augmented state-action and \\(k\\) is the RBF kernel.</p> <p>The policy is inferred by finding the best action at each state:</p> \\[\\pi(s) = \\arg\\max_\\theta Q^+(s, \\theta)\\] <p>Key difference: The field gradient depends on where particles are located, not on the goal position directly.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#why-arrows-appear-parallel","title":"Why Arrows Appear Parallel","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#the-sparse-particle-problem","title":"The Sparse Particle Problem","text":"<p>Consider synthetic particles placed along a diagonal path to the goal:</p> <pre><code>Particles:  (0.5, 0.5) \u2192 (1, 1) \u2192 (1.5, 1.5) \u2192 ... \u2192 (4, 4) [GOAL]\n            All with action direction \u2248 (0.7, 0.7)\n</code></pre> <p>At any query point \\((x, y)\\), the \\(Q^+\\) gradient is dominated by the nearest particles. Since all particles encode similar action directions (toward upper-right), the inferred policy is approximately constant across the domain.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#mathematical-explanation","title":"Mathematical Explanation","text":"<p>The gradient of \\(Q^+\\) with respect to action direction:</p> \\[\\nabla_\\theta Q^+(s, \\theta) = \\sum_{i=1}^{N} w_i \\nabla_\\theta k((s, \\theta), z_i)\\] <p>If particles are clustered with similar action components, this gradient points in roughly the same direction everywhere \u2014 hence parallel arrows.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#discrete-action-search-limitation","title":"Discrete Action Search Limitation","text":"<p>In the notebook, we search over 16 discrete angles:</p> <pre><code>angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)\n</code></pre> <p>When the \\(Q^+\\) landscape is relatively flat (due to sparse particles), many states select the same \"best\" angle, reinforcing the parallel appearance.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#what-would-make-arrows-converge","title":"What Would Make Arrows Converge?","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#1-richer-particle-coverage","title":"1. Richer Particle Coverage","text":"<p>If particles recorded trajectories from many starting points approaching the goal from different angles:</p> <pre><code>From (0, 4):  particles with action \u2248 (0.7, -0.7)  [down-right]\nFrom (4, 0):  particles with action \u2248 (-0.7, 0.7) [up-left... wait, away from goal!]\nFrom (0, 0):  particles with action \u2248 (0.7, 0.7)  [up-right]\n</code></pre> <p>With diverse coverage, the field would encode local directional information that varies across the domain.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#2-goal-conditioned-particles","title":"2. Goal-Conditioned Particles","text":"<p>Particles could encode \"turn toward goal\" rather than absolute directions:</p> \\[\\theta_i = \\text{angle toward goal from } s_i\\] <p>This would make the particle-based field behave more like a gradient field.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#3-hybrid-approach","title":"3. Hybrid Approach","text":"<p>Combine particle-based learning with a goal-directed potential:</p> \\[Q^+_{hybrid}(z) = Q^+_{particles}(z) + \\lambda \\cdot \\phi_{goal}(s)\\]"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#the-grl-perspective-feature-not-bug","title":"The GRL Perspective: Feature, Not Bug","text":"<p>The parallel-arrow behavior reveals a fundamental property of GRL:</p> <p>Policy quality depends on the richness of the particle memory.</p> <p>This is actually desirable because:</p> <ol> <li> <p>No goal knowledge required: The agent learns from experience, not from knowing where the goal is.</p> </li> <li> <p>Constraint encoding: Negative particles naturally encode obstacles \u2014 something a pure gradient field cannot do.</p> </li> <li> <p>Generalization: With enough diverse experience, the field generalizes to unseen states.</p> </li> <li> <p>Adaptability: If the goal moves, the agent can learn new particles without redesigning the potential function.</p> </li> </ol>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#visualization-comparison","title":"Visualization Comparison","text":""},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#gradient-field-ideal","title":"Gradient Field (Ideal)","text":"<pre><code>    \u2198  \u2193  \u2199\n     \u2198 \u2193 \u2199\n  \u2192  \u2192 \u2605 \u2190  \u2190    \u2605 = Goal\n     \u2197 \u2191 \u2196\n    \u2197  \u2191  \u2196\n</code></pre>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#particle-based-field-sparse-diagonal-particles","title":"Particle-Based Field (Sparse Diagonal Particles)","text":"<pre><code>    \u2197  \u2197  \u2197\n    \u2197  \u2197  \u2197\n    \u2197  \u2197  \u2605       \u2605 = Goal\n    \u2197  \u2197  \u2197\n    \u2197  \u2197  \u2197\n</code></pre>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#particle-based-field-rich-coverage","title":"Particle-Based Field (Rich Coverage)","text":"<pre><code>    \u2197  \u2197  \u2192\n    \u2197  \u2197  \u2197\n    \u2197  \u2197  \u2605       \u2605 = Goal\n    \u2191  \u2197  \u2197\n    \u2191  \u2191  \u2197\n</code></pre>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#implications-for-grl-implementation","title":"Implications for GRL Implementation","text":"<ol> <li> <p>Exploration matters: Diverse trajectories lead to richer particle coverage.</p> </li> <li> <p>Particle placement: Strategic particle placement (or learning algorithms like RF-SARSA) is crucial.</p> </li> <li> <p>Lengthscale tuning: The kernel lengthscale \\(\\ell\\) controls how far each particle's influence spreads.</p> </li> <li> <p>Action discretization: Finer action discretization can reveal more nuanced policy structure.</p> </li> </ol>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#summary","title":"Summary","text":"Aspect Gradient Field Particle-Based Q\u207a Source Goal position Experience particles Arrows Converge on goal Follow particle gradient Obstacles Cannot encode Natural via negative particles Learning None (geometric) From experience Coverage Global Depends on particle density <p>Bottom line: The parallel arrows in sparse particle scenarios are expected behavior. With richer experience, the particle-based field approaches the behavior of a goal-directed gradient field while retaining the ability to encode constraints and learn from experience.</p>"},{"location":"notebooks/field_series/03_reinforcement_fields/particle_vs_gradient_fields/#references","title":"References","text":"<ul> <li>GRL Paper: Figure 4 (2D Navigation Domain)</li> <li>Notebook: <code>notebooks/field_series/03_reinforcement_fields.ipynb</code></li> <li>Related: <code>notebooks/field_series/01_classical_vector_fields.ipynb</code> (gradient fields)</li> </ul>"},{"location":"runpods/project_setup_on_new_pod/","title":"GRL Setup on RunPods","text":"<p>Guide for setting up the GRL environment on a RunPods GPU pod for heavy training experiments.</p> <p>When to use RunPods: - Training with large operator networks - PDE control experiments requiring significant compute - Baseline comparisons with many episodes - Experiments that exceed M1's 16GB memory</p> <p>GPU Recommendation: A40 (48GB VRAM) for most GRL experiments</p>"},{"location":"runpods/project_setup_on_new_pod/#quick-setup","title":"Quick Setup","text":""},{"location":"runpods/project_setup_on_new_pod/#step-1-install-miniforge","title":"Step 1: Install Miniforge","text":"<pre><code>cd /tmp\nwget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"\nbash Miniforge3-$(uname)-$(uname -m).sh -b -p ~/miniforge3\nrm Miniforge3-$(uname)-$(uname -m).sh\n\n# Initialize shell\n~/miniforge3/bin/conda init bash\nsource ~/.bashrc\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#step-2-clone-and-setup-grl","title":"Step 2: Clone and Setup GRL","text":"<pre><code>cd /workspace\ngit clone https://github.com/YOUR_USERNAME/GRL.git\ncd GRL\n\n# Create environment\nmamba env create -f environment.yml\nmamba activate grl\n\n# Add CUDA support\nmamba install pytorch-cuda=12.1 -c pytorch -c nvidia\n\n# Install GRL\npip install -e .\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#step-3-verify-setup","title":"Step 3: Verify Setup","text":"<pre><code># Check GPU\nnvidia-smi\n\n# Verify PyTorch CUDA\npython -c \"import torch; print(f'CUDA: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0)}')\"\n\n# Test GRL\npython -c \"import grl; print(f'GRL version: {grl.__version__}')\"\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#running-grl-training-on-gpu","title":"Running GRL Training on GPU","text":"<pre><code># Quick training test\npython -m grl.workflows.train --episodes 100 --save-dir /workspace/checkpoints\n\n# Full training run\npython -m grl.workflows.train \\\n    --env field_navigation \\\n    --episodes 10000 \\\n    --save-dir /workspace/checkpoints/field_nav_run1\n\n# Evaluate checkpoint\npython -m grl.workflows.evaluate /workspace/checkpoints/final_checkpoint.pt\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#syncing-results-back-to-local","title":"Syncing Results Back to Local","text":"<p>From your local machine:</p> <pre><code># Get results from pod\nrsync -avz --progress root@POD_IP:/workspace/GRL/checkpoints/ ./checkpoints/ -e \"ssh -p PORT\"\nrsync -avz --progress root@POD_IP:/workspace/GRL/results/ ./results/ -e \"ssh -p PORT\"\n</code></pre>"},{"location":"runpods/project_setup_on_new_pod/#reference","title":"Reference","text":"<p>For detailed RunPods setup including SSH configuration, see: <code>genai-lab/docs/runpods/project_setup_on_new_pod.md</code></p>"},{"location":"theory/","title":"GRL Theory: Actions as Operators","text":""},{"location":"theory/#overview","title":"Overview","text":"<p>Generalized Reinforcement Learning (GRL) reconceptualizes the notion of \"action\" in reinforcement learning. Instead of treating actions as discrete indices or fixed-dimensional vectors, GRL models actions as functional operators on the state space.</p>"},{"location":"theory/#the-core-insight","title":"The Core Insight","text":"<p>In classical RL, an agent selects an action \\(a \\in \\mathcal{A}\\) which influences the next state through the transition dynamics \\(T(s' | s, a)\\). The action is external to the state transformation.</p> <p>In GRL, the action is the transformation:</p> \\[ \\hat{O}: \\mathcal{S} \\to \\mathcal{S} \\] <p>The agent's policy generates an operator \\(\\hat{O}\\) that directly maps the current state to the next state.</p>"},{"location":"theory/#the-grl-tuple","title":"The GRL Tuple","text":"<p>Classical MDP: \\((\\mathcal{S}, \\mathcal{A}, T, R, \\gamma)\\)</p> <p>GRL: \\((\\mathcal{S}, \\mathcal{O}, T, R, \\gamma)\\)</p> <p>Where: - \\(\\mathcal{S}\\): State space - \\(\\mathcal{O}\\): Space of operators on \\(\\mathcal{S}\\) - \\(T\\): Transition function (often deterministic: \\(s' = \\hat{O}(s)\\)) - \\(R\\): Reward function - \\(\\gamma\\): Discount factor</p>"},{"location":"theory/#operator-generator-policy","title":"Operator Generator (Policy)","text":"<p>The policy in GRL is an operator generator:</p> \\[ \\pi_\\theta: \\mathcal{S} \\to \\mathcal{O} \\] <p>Given state \\(s\\), the policy produces an operator \\(\\hat{O} = \\pi_\\theta(s)\\) which is then applied:</p> \\[ s' = \\hat{O}(s) \\] <p>This is fundamentally different from classical policies which output action vectors.</p>"},{"location":"theory/#least-action-principle","title":"Least-Action Principle","text":"<p>Inspired by physics, GRL incorporates the principle of least action as a regularizer:</p> \\[ \\mathcal{L} = \\mathcal{L}_{\\text{RL}} + \\lambda \\mathcal{E}(\\hat{O}) \\] <p>Where \\(\\mathcal{E}(\\hat{O})\\) is the \"energy\" of the operator, encouraging: - Smooth transformations - Minimal perturbations - Physically plausible behavior</p>"},{"location":"theory/#classical-rl-as-a-special-case","title":"Classical RL as a Special Case","text":"<p>Standard continuous-action RL is recovered when operators are restricted to displacements:</p> \\[ \\hat{O}_a(s) = s + a \\] <p>Here \\(a\\) is the classical action vector, and the operator simply translates the state.</p>"},{"location":"theory/#next-steps","title":"Next Steps","text":"<ul> <li>Operator Families: Explore different operator architectures</li> <li>Generalized Bellman: Value iteration with operators</li> <li>Least-Action: Physics-inspired regularization</li> </ul>"},{"location":"theory/particle_vs_gradient_fields/","title":"Particle-Based Q\u207a Fields vs. True Gradient Fields","text":"<p>A detailed comparison for understanding GRL policy behavior</p>"},{"location":"theory/particle_vs_gradient_fields/#overview","title":"Overview","text":"<p>When visualizing GRL's inferred policy as a vector field, you may notice that arrows appear mostly parallel rather than converging on the goal. This document explains why this happens and what it reveals about the nature of particle-based reinforcement fields.</p>"},{"location":"theory/particle_vs_gradient_fields/#two-types-of-fields","title":"Two Types of Fields","text":""},{"location":"theory/particle_vs_gradient_fields/#1-true-gradient-field-goal-directed-potential","title":"1. True Gradient Field (Goal-Directed Potential)","text":"<p>A classical approach to navigation uses a potential function that directly encodes distance to the goal:</p> \\[\\phi(x, y) = -\\|(x, y) - (x_{goal}, y_{goal})\\|^2\\] <p>The gradient of this potential:</p> \\[\\nabla \\phi = -2 \\begin{pmatrix} x - x_{goal} \\\\ y - y_{goal} \\end{pmatrix}\\] <p>This gradient field has a key property: arrows at every point converge toward the goal.</p> <pre><code>        \u2198   \u2193   \u2199\n         \u2198  \u2193  \u2199\n    \u2192  \u2192  [GOAL]  \u2190  \u2190\n         \u2197  \u2191  \u2196\n        \u2197   \u2191   \u2196\n</code></pre> <p>Pros: - Globally optimal direction at every point - Arrows naturally \"rotate\" to point at goal</p> <p>Cons: - Requires explicit knowledge of goal location - Cannot encode constraints (obstacles) naturally - No learning \u2014 purely geometric</p>"},{"location":"theory/particle_vs_gradient_fields/#2-particle-based-q-field-grl","title":"2. Particle-Based Q\u207a Field (GRL)","text":"<p>GRL builds the reinforcement field from experience particles:</p> \\[Q^+(z) = \\sum_{i=1}^{N} w_i \\, k(z, z_i)\\] <p>where \\(z = (x, y, v_x, v_y)\\) is the augmented state-action and \\(k\\) is the RBF kernel.</p> <p>The policy is inferred by finding the best action at each state:</p> \\[\\pi(s) = \\arg\\max_\\theta Q^+(s, \\theta)\\] <p>Key difference: The field gradient depends on where particles are located, not on the goal position directly.</p>"},{"location":"theory/particle_vs_gradient_fields/#why-arrows-appear-parallel","title":"Why Arrows Appear Parallel","text":""},{"location":"theory/particle_vs_gradient_fields/#the-sparse-particle-problem","title":"The Sparse Particle Problem","text":"<p>Consider synthetic particles placed along a diagonal path to the goal:</p> <pre><code>Particles:  (0.5, 0.5) \u2192 (1, 1) \u2192 (1.5, 1.5) \u2192 ... \u2192 (4, 4) [GOAL]\n            All with action direction \u2248 (0.7, 0.7)\n</code></pre> <p>At any query point \\((x, y)\\), the \\(Q^+\\) gradient is dominated by the nearest particles. Since all particles encode similar action directions (toward upper-right), the inferred policy is approximately constant across the domain.</p>"},{"location":"theory/particle_vs_gradient_fields/#mathematical-explanation","title":"Mathematical Explanation","text":"<p>The gradient of \\(Q^+\\) with respect to action direction:</p> \\[\\nabla_\\theta Q^+(s, \\theta) = \\sum_{i=1}^{N} w_i \\nabla_\\theta k((s, \\theta), z_i)\\] <p>If particles are clustered with similar action components, this gradient points in roughly the same direction everywhere \u2014 hence parallel arrows.</p>"},{"location":"theory/particle_vs_gradient_fields/#discrete-action-search-limitation","title":"Discrete Action Search Limitation","text":"<p>In the notebook, we search over 16 discrete angles:</p> <pre><code>angles = np.linspace(0, 2*np.pi, n_angles, endpoint=False)\n</code></pre> <p>When the \\(Q^+\\) landscape is relatively flat (due to sparse particles), many states select the same \"best\" angle, reinforcing the parallel appearance.</p>"},{"location":"theory/particle_vs_gradient_fields/#what-would-make-arrows-converge","title":"What Would Make Arrows Converge?","text":""},{"location":"theory/particle_vs_gradient_fields/#1-richer-particle-coverage","title":"1. Richer Particle Coverage","text":"<p>If particles recorded trajectories from many starting points approaching the goal from different angles:</p> <pre><code>From (0, 4):  particles with action \u2248 (0.7, -0.7)  [down-right]\nFrom (4, 0):  particles with action \u2248 (-0.7, 0.7) [up-left... wait, away from goal!]\nFrom (0, 0):  particles with action \u2248 (0.7, 0.7)  [up-right]\n</code></pre> <p>With diverse coverage, the field would encode local directional information that varies across the domain.</p>"},{"location":"theory/particle_vs_gradient_fields/#2-goal-conditioned-particles","title":"2. Goal-Conditioned Particles","text":"<p>Particles could encode \"turn toward goal\" rather than absolute directions:</p> \\[\\theta_i = \\text{angle toward goal from } s_i\\] <p>This would make the particle-based field behave more like a gradient field.</p>"},{"location":"theory/particle_vs_gradient_fields/#3-hybrid-approach","title":"3. Hybrid Approach","text":"<p>Combine particle-based learning with a goal-directed potential:</p> \\[Q^+_{hybrid}(z) = Q^+_{particles}(z) + \\lambda \\cdot \\phi_{goal}(s)\\]"},{"location":"theory/particle_vs_gradient_fields/#the-grl-perspective-feature-not-bug","title":"The GRL Perspective: Feature, Not Bug","text":"<p>The parallel-arrow behavior reveals a fundamental property of GRL:</p> <p>Policy quality depends on the richness of the particle memory.</p> <p>This is actually desirable because:</p> <ol> <li> <p>No goal knowledge required: The agent learns from experience, not from knowing where the goal is.</p> </li> <li> <p>Constraint encoding: Negative particles naturally encode obstacles \u2014 something a pure gradient field cannot do.</p> </li> <li> <p>Generalization: With enough diverse experience, the field generalizes to unseen states.</p> </li> <li> <p>Adaptability: If the goal moves, the agent can learn new particles without redesigning the potential function.</p> </li> </ol>"},{"location":"theory/particle_vs_gradient_fields/#visualization-comparison","title":"Visualization Comparison","text":""},{"location":"theory/particle_vs_gradient_fields/#gradient-field-ideal","title":"Gradient Field (Ideal)","text":"<pre><code>    \u2198  \u2193  \u2199\n     \u2198 \u2193 \u2199\n  \u2192  \u2192 \u2605 \u2190  \u2190    \u2605 = Goal\n     \u2197 \u2191 \u2196\n    \u2197  \u2191  \u2196\n</code></pre>"},{"location":"theory/particle_vs_gradient_fields/#particle-based-field-sparse-diagonal-particles","title":"Particle-Based Field (Sparse Diagonal Particles)","text":"<pre><code>    \u2197  \u2197  \u2197\n    \u2197  \u2197  \u2197\n    \u2197  \u2197  \u2605       \u2605 = Goal\n    \u2197  \u2197  \u2197\n    \u2197  \u2197  \u2197\n</code></pre>"},{"location":"theory/particle_vs_gradient_fields/#particle-based-field-rich-coverage","title":"Particle-Based Field (Rich Coverage)","text":"<pre><code>    \u2197  \u2197  \u2192\n    \u2197  \u2197  \u2197\n    \u2197  \u2197  \u2605       \u2605 = Goal\n    \u2191  \u2197  \u2197\n    \u2191  \u2191  \u2197\n</code></pre>"},{"location":"theory/particle_vs_gradient_fields/#implications-for-grl-implementation","title":"Implications for GRL Implementation","text":"<ol> <li> <p>Exploration matters: Diverse trajectories lead to richer particle coverage.</p> </li> <li> <p>Particle placement: Strategic particle placement (or learning algorithms like RF-SARSA) is crucial.</p> </li> <li> <p>Lengthscale tuning: The kernel lengthscale \\(\\ell\\) controls how far each particle's influence spreads.</p> </li> <li> <p>Action discretization: Finer action discretization can reveal more nuanced policy structure.</p> </li> </ol>"},{"location":"theory/particle_vs_gradient_fields/#summary","title":"Summary","text":"Aspect Gradient Field Particle-Based Q\u207a Source Goal position Experience particles Arrows Converge on goal Follow particle gradient Obstacles Cannot encode Natural via negative particles Learning None (geometric) From experience Coverage Global Depends on particle density <p>Bottom line: The parallel arrows in sparse particle scenarios are expected behavior. With richer experience, the particle-based field approaches the behavior of a goal-directed gradient field while retaining the ability to encode constraints and learn from experience.</p>"},{"location":"theory/particle_vs_gradient_fields/#references","title":"References","text":"<ul> <li>GRL Paper: Figure 4 (2D Navigation Domain)</li> <li>Notebook: <code>notebooks/field_series/03_reinforcement_fields.ipynb</code></li> <li>Related: <code>notebooks/field_series/01_classical_vector_fields.ipynb</code> (gradient fields)</li> </ul>"},{"location":"tutorials/quickstart/","title":"Quick Start: Your First GRL Agent","text":"<p>This tutorial walks through creating and training a GRL agent on the Field Navigation environment.</p>"},{"location":"tutorials/quickstart/#installation","title":"Installation","text":"<pre><code># Clone and setup\ngit clone https://github.com/your-username/GRL.git\ncd GRL\n\n# Create environment\nmamba env create -f environment.yml\nmamba activate grl\n\n# Install package\npip install -e .\n</code></pre>"},{"location":"tutorials/quickstart/#basic-concepts","title":"Basic Concepts","text":"<p>In GRL, agents don't output action vectors\u2014they output operators that transform states:</p> <pre><code>import torch\nfrom grl.operators import FieldOperator\n\n# Create a field operator\noperator = FieldOperator(state_dim=4, hidden_dims=[64, 64])\n\n# Apply operator to transform state\nstate = torch.randn(1, 4)\nnext_state = operator(state)\n\n# Check operator energy (for least-action regularization)\nenergy = operator.energy()\nprint(f\"Operator energy: {energy.item():.4f}\")\n</code></pre>"},{"location":"tutorials/quickstart/#creating-an-agent","title":"Creating an Agent","text":"<pre><code>from grl.policies import OperatorPolicy\nfrom grl.algorithms import OperatorActorCritic\n\n# Create an operator policy\npolicy = OperatorPolicy(\n    state_dim=4,\n    operator_class=FieldOperator,\n    hidden_dims=[256, 256],\n    exploration_noise=0.1,\n    least_action_weight=0.01,\n)\n\n# Wrap in Operator-Actor-Critic\nagent = OperatorActorCritic(\n    policy=policy,\n    state_dim=4,\n    gamma=0.99,\n    least_action_weight=0.01,\n)\n</code></pre>"},{"location":"tutorials/quickstart/#training-on-field-navigation","title":"Training on Field Navigation","text":"<pre><code>from grl.envs import FieldNavigationEnv\n\n# Create environment\nenv = FieldNavigationEnv(arena_size=10.0, max_steps=200)\n\n# Training loop\nfor episode in range(1000):\n    state, _ = env.reset()\n    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n\n    episode_reward = 0\n\n    for step in range(200):\n        # Agent selects next state via operator\n        next_state = agent.select_action(state)\n\n        # Extract velocity for environment\n        action = (next_state - state).squeeze(0).numpy()[:2]\n\n        # Step environment\n        obs, reward, terminated, truncated, info = env.step(action)\n        episode_reward += reward\n\n        # Update agent\n        obs_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0)\n        agent.update(\n            state,\n            torch.tensor([reward]),\n            obs_tensor,\n            torch.tensor([terminated or truncated]),\n        )\n\n        state = obs_tensor\n\n        if terminated or truncated:\n            break\n\n    if (episode + 1) % 100 == 0:\n        print(f\"Episode {episode + 1}: Return = {episode_reward:.2f}\")\n</code></pre>"},{"location":"tutorials/quickstart/#visualizing-the-learned-field","title":"Visualizing the Learned Field","text":"<pre><code>from grl.eval.visualization import plot_operator_field\nimport matplotlib.pyplot as plt\n\n# Get the learned operator\noperator = policy.get_operator()\n\n# Plot the field\nfig, ax = plot_operator_field(\n    operator,\n    xlim=(-10, 10),\n    ylim=(-10, 10),\n    resolution=20,\n    title=\"Learned Navigation Field\",\n)\n\nplt.savefig(\"learned_field.png\")\n</code></pre>"},{"location":"tutorials/quickstart/#using-the-cli","title":"Using the CLI","text":"<pre><code># Train an agent\ngrl-train --env field_navigation --episodes 1000 --save-dir checkpoints/\n\n# Evaluate\ngrl-evaluate checkpoints/final_checkpoint.pt --episodes 100\n\n# Visualize\ngrl-visualize --checkpoint checkpoints/final_checkpoint.pt\n</code></pre>"},{"location":"tutorials/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Field Navigation Tutorial: Deep dive into the navigation environment</li> <li>Custom Operators: Define your own operator types</li> <li>Comparing with Baselines: SAC/PPO comparison</li> </ul>"}]}